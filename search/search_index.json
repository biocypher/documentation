{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The BioCypher Ecosystem","text":"<p>Warning</p> <p>This unified documentation is currently under construction. Please check the docs of the individual projects for now.</p> <p>Welcome to the unified documentation for the BioCypher ecosystem.  Our mission is to facilitate research, particularly in biology and medicine, by streamlining the scientific workflow.  We do this by providing a suite of tools that help manage knowledge in various ways.</p> <ul> <li> <p> Bespoke libraries</p> <p>Automated and tested workflows for common and tedious tasks.</p> </li> <li> <p> Reducing redundancy</p> <p>Sharing resource adapters for data sources, AI models, etc.</p> </li> <li> <p> Robust assistance</p> <p>AI agents that can help with low-level tasks, letting you focus on the big picture.</p> </li> <li> <p> Community support</p> <p>Share your workflows, ask questions, and get help from the community.</p> </li> </ul> <p>This documentation covers:</p> <ul> <li> <p> BioCypher</p> <p>A framework for the creation of knowledge graphs in biology and medicine.</p> <p> To the BioCypher documentation</p> </li> </ul> <ul> <li> <p> BioChatter</p> <p>A framework for building biomedical AI agents.</p> <p> To the BioChatter documentation</p> </li> </ul> <ul> <li> <p> BioGather</p> <p>A framework for extracting biological data from text and other modalities. Coming soon.</p> </li> </ul> <ul> <li> <p> Biotope</p> <p>A unified interface for accessing BioCypher ecosystem resources. Coming soon.</p> </li> </ul>"},{"location":"BioChatter/","title":"Home","text":"<p>Generative AI models have shown tremendous usefulness in increasing accessibility and automation of a wide range of tasks. Yet, their application to the biomedical domain is still limited, in part due to the lack of a common framework for deploying, testing, and evaluating the diverse models and auxiliary technologies that are needed. <code>biochatter</code> is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this preprint; for a more hands-on experience, check out our two web app implementations:</p> <ul> <li> <p> BioChatter Light</p> <p>Agile framework in pure Python built with Streamlit, for fast prototyping and iteration.</p> <p> Go To BioChatter Light</p> </li> <li> <p> BioChatter Next</p> <p>Advanced client-server architecture based on Flask and Next.js.</p> <p> Go To BioChatter Next</p> </li> </ul> <p>BioChatter is part of the BioCypher ecosystem, connecting natively to BioCypher knowledge graphs. The BioChatter paper is being written here and the current version can be read here.</p> <p></p> <p>Hot Topics</p> <p>BioChatter natively extends BioCypher knowledge graphs. Check there for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models.\" You can read it here.</p>"},{"location":"BioChatter/#installation","title":"Installation","text":"<p>To use the package, install it from PyPI, for instance using pip (<code>pip install biochatter</code>) or Poetry (<code>poetry add biochatter</code>).</p>"},{"location":"BioChatter/#extras","title":"Extras","text":"<p>The package has some optional dependencies that can be installed using the following extras (e.g. <code>pip install biochatter[xinference]</code>):</p> <ul> <li> <p><code>xinference</code>: support for querying open-source LLMs through Xorbits Inference</p> </li> <li> <p><code>ollama</code>: support for querying open-source LLMs through Ollama</p> </li> <li> <p><code>podcast</code>: support for podcast text-to-speech (for the free Google TTS; the paid OpenAI TTS can be used without this extra)</p> </li> <li> <p><code>streamlit</code>: support for streamlit UI functions (used in BioChatter Light)</p> </li> </ul> <p> </p>"},{"location":"BioChatter/#documentation-and-tutorials","title":"Documentation and Tutorials","text":"<p>For a description of the features of the framework, see the <code>Features</code> option in the main menu. For examples of usage and customisation, check out the <code>Vignettes</code> section. For a more detailed reference of functions and classes, see the <code>API Reference</code>.</p>"},{"location":"BioChatter/api-docs/","title":"BioChatter API Reference Documentation","text":"<p>Here we collect documentation of BioChatter module APIs. For detailed information on each module, please refer to the navigation side bar.</p>"},{"location":"BioChatter/api-docs/api-calling-base/","title":"API Calling: Base Classes","text":"<p>We provide base classes for the API Agent implementation to align individual modules.</p>"},{"location":"BioChatter/api-docs/api-calling-base/#abstract-base-classes","title":"Abstract base classes","text":"<p>Abstract base classes for API interaction components.</p> <p>Provides base classes for query builders, fetchers, and interpreters used in API interactions and result processing.</p>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseAPIModel","title":"<code>BaseAPIModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base class for all API models.</p> <p>Includes default fields <code>uuid</code> and <code>method_name</code>.</p> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseAPIModel(BaseModel):\n    \"\"\"A base class for all API models.\n\n    Includes default fields `uuid` and `method_name`.\n    \"\"\"\n\n    uuid: str | None = Field(\n        None,\n        description=\"Unique identifier for the model instance\",\n    )\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseFetcher","title":"<code>BaseFetcher</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for fetchers.</p> <p>A fetcher is responsible for submitting queries (in systems where submission and fetching are separate) and fetching and saving results of queries. It has to implement a <code>fetch_results()</code> method, which can wrap a multi-step procedure to submit and retrieve. Should implement retry method to account for connectivity issues or processing times.</p> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseFetcher(ABC):\n    \"\"\"Abstract base class for fetchers.\n\n    A fetcher is responsible for submitting queries (in systems where\n    submission and fetching are separate) and fetching and saving results of\n    queries. It has to implement a `fetch_results()` method, which can wrap a\n    multi-step procedure to submit and retrieve. Should implement retry method to\n    account for connectivity issues or processing times.\n    \"\"\"\n\n    @abstractmethod\n    def fetch_results(\n        self,\n        query_models: list[BaseModel],\n        retries: int | None = 3,\n    ):\n        \"\"\"Fetch results by submitting a query.\n\n        Can implement a multi-step procedure if submitting and fetching are\n        distinct processes (e.g., in the case of long processing times as in the\n        case of BLAST).\n\n        Args:\n        ----\n            query_models: list of Pydantic models describing the parameterised\n                queries\n\n        \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseFetcher.fetch_results","title":"<code>fetch_results(query_models, retries=3)</code>  <code>abstractmethod</code>","text":"<p>Fetch results by submitting a query.</p> <p>Can implement a multi-step procedure if submitting and fetching are distinct processes (e.g., in the case of long processing times as in the case of BLAST).</p> <pre><code>query_models: list of Pydantic models describing the parameterised\n    queries\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef fetch_results(\n    self,\n    query_models: list[BaseModel],\n    retries: int | None = 3,\n):\n    \"\"\"Fetch results by submitting a query.\n\n    Can implement a multi-step procedure if submitting and fetching are\n    distinct processes (e.g., in the case of long processing times as in the\n    case of BLAST).\n\n    Args:\n    ----\n        query_models: list of Pydantic models describing the parameterised\n            queries\n\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseInterpreter","title":"<code>BaseInterpreter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for result interpreters.</p> <p>The interpreter is aware of the nature and structure of the results and can extract and summarise information from them.</p> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseInterpreter(ABC):\n    \"\"\"Abstract base class for result interpreters.\n\n    The interpreter is aware of the nature and structure of the results and can\n    extract and summarise information from them.\n    \"\"\"\n\n    @abstractmethod\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Summarise an answer based on the given parameters.\n\n        Args:\n        ----\n            question (str): The question that was asked.\n\n            conversation_factory (Callable): A function that creates a\n                BioChatter conversation.\n\n            response_text (str): The response.text returned from the request.\n\n        Returns:\n        -------\n            A summary of the answer.\n\n        Todo:\n        ----\n            Genericise (remove file path and n_lines parameters, and use a\n            generic way to get the results). The child classes should manage the\n            specifics of the results.\n\n        \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>  <code>abstractmethod</code>","text":"<p>Summarise an answer based on the given parameters.</p> <pre><code>question (str): The question that was asked.\n\nconversation_factory (Callable): A function that creates a\n    BioChatter conversation.\n\nresponse_text (str): The response.text returned from the request.\n</code></pre> <pre><code>A summary of the answer.\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseInterpreter.summarise_results--todo","title":"Todo:","text":"<pre><code>Genericise (remove file path and n_lines parameters, and use a\ngeneric way to get the results). The child classes should manage the\nspecifics of the results.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Summarise an answer based on the given parameters.\n\n    Args:\n    ----\n        question (str): The question that was asked.\n\n        conversation_factory (Callable): A function that creates a\n            BioChatter conversation.\n\n        response_text (str): The response.text returned from the request.\n\n    Returns:\n    -------\n        A summary of the answer.\n\n    Todo:\n    ----\n        Genericise (remove file path and n_lines parameters, and use a\n        generic way to get the results). The child classes should manage the\n        specifics of the results.\n\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder","title":"<code>BaseQueryBuilder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for query builders.</p> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseQueryBuilder(ABC):\n    \"\"\"An abstract base class for query builders.\"\"\"\n\n    @property\n    def structured_output_prompt(self) -&gt; ChatPromptTemplate:\n        \"\"\"Define a structured output prompt template.\n\n        This provides a default implementation for an API agent that can be\n        overridden by subclasses to return a ChatPromptTemplate-compatible\n        object.\n        \"\"\"\n        return ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class algorithm for extracting information in structured formats.\",\n                ),\n                (\n                    \"human\",\n                    \"Use the given format to extract information from the following input: {input}\",\n                ),\n                (\"human\", \"Tip: Make sure to answer in the correct format\"),\n            ],\n        )\n\n    @abstractmethod\n    def create_runnable(\n        self,\n        query_parameters: \"BaseModel\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Must be implemented by subclasses. Should use the LangChain\n        `create_structured_output_runnable` method to generate the Callable.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n\n    @abstractmethod\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[BaseModel]:\n        \"\"\"Parameterise a query object.\n\n        Parameterises a Pydantic model with the fields of the API based on the\n        given question using a BioChatter conversation instance. Must be\n        implemented by subclasses.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The BioChatter conversation object containing the LLM\n                that should parameterise the query.\n\n        Returns:\n        -------\n            A list containing one or more parameterised instance(s) of the query\n            object (Pydantic BaseModel).\n\n        \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder.structured_output_prompt","title":"<code>structured_output_prompt</code>  <code>property</code>","text":"<p>Define a structured output prompt template.</p> <p>This provides a default implementation for an API agent that can be overridden by subclasses to return a ChatPromptTemplate-compatible object.</p>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>  <code>abstractmethod</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Must be implemented by subclasses. Should use the LangChain <code>create_structured_output_runnable</code> method to generate the Callable.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef create_runnable(\n    self,\n    query_parameters: \"BaseModel\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Must be implemented by subclasses. Should use the LangChain\n    `create_structured_output_runnable` method to generate the Callable.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>  <code>abstractmethod</code>","text":"<p>Parameterise a query object.</p> <p>Parameterises a Pydantic model with the fields of the API based on the given question using a BioChatter conversation instance. Must be implemented by subclasses.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The BioChatter conversation object containing the LLM\n    that should parameterise the query.\n</code></pre> <pre><code>A list containing one or more parameterised instance(s) of the query\nobject (Pydantic BaseModel).\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[BaseModel]:\n    \"\"\"Parameterise a query object.\n\n    Parameterises a Pydantic model with the fields of the API based on the\n    given question using a BioChatter conversation instance. Must be\n    implemented by subclasses.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The BioChatter conversation object containing the LLM\n            that should parameterise the query.\n\n    Returns:\n    -------\n        A list containing one or more parameterised instance(s) of the query\n        object (Pydantic BaseModel).\n\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseTools","title":"<code>BaseTools</code>","text":"<p>Abstract base class for tools.</p> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseTools:\n    \"\"\"Abstract base class for tools.\"\"\"\n\n    def make_pydantic_tools(self) -&gt; list[BaseAPIModel]:\n        \"\"\"Uses pydantics create_model to create a list of pydantic tools from a dictionary of parameters\"\"\"\n        tools = []\n        for func_name, tool_params in self.tools_params.items():\n            tools.append(create_model(func_name, **tool_params, __base__=BaseAPIModel))\n        return tools\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseTools.make_pydantic_tools","title":"<code>make_pydantic_tools()</code>","text":"<p>Uses pydantics create_model to create a list of pydantic tools from a dictionary of parameters</p> Source code in <code>biochatter/biochatter/api_agent/base/agent_abc.py</code> <pre><code>def make_pydantic_tools(self) -&gt; list[BaseAPIModel]:\n    \"\"\"Uses pydantics create_model to create a list of pydantic tools from a dictionary of parameters\"\"\"\n    tools = []\n    for func_name, tool_params in self.tools_params.items():\n        tools.append(create_model(func_name, **tool_params, __base__=BaseAPIModel))\n    return tools\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#the-api-agent","title":"The API Agent","text":"<p>Base API agent module.</p>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent","title":"<code>APIAgent</code>","text":"Source code in <code>biochatter/biochatter/api_agent/base/api_agent.py</code> <pre><code>class APIAgent:\n    def __init__(\n        self,\n        conversation_factory: Callable,\n        query_builder: \"BaseQueryBuilder\",\n        fetcher: \"BaseFetcher\",\n        interpreter: \"BaseInterpreter\",\n    ):\n        \"\"\"API agent class to interact with a tool's API for querying and fetching\n        results.  The query fields have to be defined in a Pydantic model\n        (`BaseModel`) and used (i.e., parameterised by the LLM) in the query\n        builder. Specific API agents are defined in submodules of this directory\n        (`api_agent`). The agent's logic is implemented in the `execute` method.\n\n        Attributes\n        ----------\n            conversation_factory (Callable): A function used to create a\n                BioChatter conversation, providing LLM access.\n\n            query_builder (BaseQueryBuilder): An instance of a child of the\n                BaseQueryBuilder class.\n\n            result_fetcher (BaseFetcher): An instance of a child of the\n                BaseFetcher class.\n\n            result_interpreter (BaseInterpreter): An instance of a child of the\n                BaseInterpreter class.\n\n        \"\"\"\n        self.conversation_factory = conversation_factory\n        self.query_builder = query_builder\n        self.fetcher = fetcher\n        self.interpreter = interpreter\n        self.final_answer = None\n\n    def parameterise_query(self, question: str) -&gt; list[BaseModel] | None:\n        \"\"\"Use LLM to parameterise a query (a list of Pydantic models) based on the given\n        question using a BioChatter conversation instance.\n        \"\"\"\n        try:\n            conversation = self.conversation_factory()\n            return self.query_builder.parameterise_query(question, conversation)\n        except Exception as e:\n            print(f\"Error generating query: {e}\")\n            return None\n\n    def fetch_results(self, query_models: list[BaseModel]) -&gt; str | None:\n        \"\"\"Fetch the results of the query using the individual API's implementation\n        (either single-step or submit-retrieve).\n\n        Args:\n        ----\n            query_models: list of parameterised query Pydantic models\n\n        \"\"\"\n        try:\n            return self.fetcher.fetch_results(query_models, 100)\n        except Exception as e:\n            print(f\"Error fetching results: {e}\")\n            return None\n\n    def summarise_results(\n        self,\n        question: str,\n        response_text: str,\n    ) -&gt; str | None:\n        \"\"\"Summarise the retrieved results to extract the answer to the question.\"\"\"\n        try:\n            return self.interpreter.summarise_results(\n                question=question,\n                conversation_factory=self.conversation_factory,\n                response_text=response_text,\n            )\n        except Exception as e:\n            print(f\"Error extracting answer: {e}\")\n            return None\n\n    def execute(self, question: str) -&gt; str | None:\n        \"\"\"Wrapper that uses class methods to execute the API agent logic. Consists\n        of 1) query generation, 2) query submission, 3) results fetching, and\n        4) answer extraction. The final answer is stored in the final_answer\n        attribute.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n        \"\"\"\n        # Generate query\n        try:\n            query_models = self.parameterise_query(question)\n            if not query_models:\n                raise ValueError(\"Failed to generate query.\")\n        except ValueError as e:\n            print(e)\n\n        # Fetch results\n        try:\n            response_text = self.fetch_results(\n                query_models=query_models,\n            )\n            if not response_text:\n                raise ValueError(\"Failed to fetch results.\")\n        except ValueError as e:\n            print(e)\n\n        # Extract answer from results\n        try:\n            final_answer = self.summarise_results(question, response_text)\n            if not final_answer:\n                raise ValueError(\"Failed to extract answer from results.\")\n        except ValueError as e:\n            print(e)\n\n        self.final_answer = final_answer\n        return final_answer\n\n    def get_description(self, tool_name: str, tool_desc: str):\n        return f\"This API agent interacts with {tool_name}'s API for querying and fetching results. {tool_desc}\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.__init__","title":"<code>__init__(conversation_factory, query_builder, fetcher, interpreter)</code>","text":"<p>API agent class to interact with a tool's API for querying and fetching results.  The query fields have to be defined in a Pydantic model (<code>BaseModel</code>) and used (i.e., parameterised by the LLM) in the query builder. Specific API agents are defined in submodules of this directory (<code>api_agent</code>). The agent's logic is implemented in the <code>execute</code> method.</p>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.__init__--attributes","title":"Attributes","text":"<pre><code>conversation_factory (Callable): A function used to create a\n    BioChatter conversation, providing LLM access.\n\nquery_builder (BaseQueryBuilder): An instance of a child of the\n    BaseQueryBuilder class.\n\nresult_fetcher (BaseFetcher): An instance of a child of the\n    BaseFetcher class.\n\nresult_interpreter (BaseInterpreter): An instance of a child of the\n    BaseInterpreter class.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/api_agent.py</code> <pre><code>def __init__(\n    self,\n    conversation_factory: Callable,\n    query_builder: \"BaseQueryBuilder\",\n    fetcher: \"BaseFetcher\",\n    interpreter: \"BaseInterpreter\",\n):\n    \"\"\"API agent class to interact with a tool's API for querying and fetching\n    results.  The query fields have to be defined in a Pydantic model\n    (`BaseModel`) and used (i.e., parameterised by the LLM) in the query\n    builder. Specific API agents are defined in submodules of this directory\n    (`api_agent`). The agent's logic is implemented in the `execute` method.\n\n    Attributes\n    ----------\n        conversation_factory (Callable): A function used to create a\n            BioChatter conversation, providing LLM access.\n\n        query_builder (BaseQueryBuilder): An instance of a child of the\n            BaseQueryBuilder class.\n\n        result_fetcher (BaseFetcher): An instance of a child of the\n            BaseFetcher class.\n\n        result_interpreter (BaseInterpreter): An instance of a child of the\n            BaseInterpreter class.\n\n    \"\"\"\n    self.conversation_factory = conversation_factory\n    self.query_builder = query_builder\n    self.fetcher = fetcher\n    self.interpreter = interpreter\n    self.final_answer = None\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.execute","title":"<code>execute(question)</code>","text":"<p>Wrapper that uses class methods to execute the API agent logic. Consists of 1) query generation, 2) query submission, 3) results fetching, and 4) answer extraction. The final answer is stored in the final_answer attribute.</p> <pre><code>question (str): The question to be answered.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/api_agent.py</code> <pre><code>def execute(self, question: str) -&gt; str | None:\n    \"\"\"Wrapper that uses class methods to execute the API agent logic. Consists\n    of 1) query generation, 2) query submission, 3) results fetching, and\n    4) answer extraction. The final answer is stored in the final_answer\n    attribute.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n    \"\"\"\n    # Generate query\n    try:\n        query_models = self.parameterise_query(question)\n        if not query_models:\n            raise ValueError(\"Failed to generate query.\")\n    except ValueError as e:\n        print(e)\n\n    # Fetch results\n    try:\n        response_text = self.fetch_results(\n            query_models=query_models,\n        )\n        if not response_text:\n            raise ValueError(\"Failed to fetch results.\")\n    except ValueError as e:\n        print(e)\n\n    # Extract answer from results\n    try:\n        final_answer = self.summarise_results(question, response_text)\n        if not final_answer:\n            raise ValueError(\"Failed to extract answer from results.\")\n    except ValueError as e:\n        print(e)\n\n    self.final_answer = final_answer\n    return final_answer\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.fetch_results","title":"<code>fetch_results(query_models)</code>","text":"<p>Fetch the results of the query using the individual API's implementation (either single-step or submit-retrieve).</p> <pre><code>query_models: list of parameterised query Pydantic models\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/api_agent.py</code> <pre><code>def fetch_results(self, query_models: list[BaseModel]) -&gt; str | None:\n    \"\"\"Fetch the results of the query using the individual API's implementation\n    (either single-step or submit-retrieve).\n\n    Args:\n    ----\n        query_models: list of parameterised query Pydantic models\n\n    \"\"\"\n    try:\n        return self.fetcher.fetch_results(query_models, 100)\n    except Exception as e:\n        print(f\"Error fetching results: {e}\")\n        return None\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.parameterise_query","title":"<code>parameterise_query(question)</code>","text":"<p>Use LLM to parameterise a query (a list of Pydantic models) based on the given question using a BioChatter conversation instance.</p> Source code in <code>biochatter/biochatter/api_agent/base/api_agent.py</code> <pre><code>def parameterise_query(self, question: str) -&gt; list[BaseModel] | None:\n    \"\"\"Use LLM to parameterise a query (a list of Pydantic models) based on the given\n    question using a BioChatter conversation instance.\n    \"\"\"\n    try:\n        conversation = self.conversation_factory()\n        return self.query_builder.parameterise_query(question, conversation)\n    except Exception as e:\n        print(f\"Error generating query: {e}\")\n        return None\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.summarise_results","title":"<code>summarise_results(question, response_text)</code>","text":"<p>Summarise the retrieved results to extract the answer to the question.</p> Source code in <code>biochatter/biochatter/api_agent/base/api_agent.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    response_text: str,\n) -&gt; str | None:\n    \"\"\"Summarise the retrieved results to extract the answer to the question.\"\"\"\n    try:\n        return self.interpreter.summarise_results(\n            question=question,\n            conversation_factory=self.conversation_factory,\n            response_text=response_text,\n        )\n    except Exception as e:\n        print(f\"Error extracting answer: {e}\")\n        return None\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/","title":"API Calling: Python APIs","text":"<p>API calling for large Python APIs is currently experimental. In particular, we observe a decrease in stability with increasing number of total parameters offered to the LLM. Due to this limitation, we recommend benchmarking the stability of the calls using our benchmarking framework. If you're interested in the performance of a specific API / LLM combination, don't hesitate to get in touch.</p>"},{"location":"BioChatter/api-docs/api-calling-python/#generic-python-api-ingestion","title":"Generic Python API ingestion","text":"<p>Using Pydantic parsing, we autogenerate API descriptions for tool bindings. While this allows better scaling (given suitable structure of the ingested code, particularly with respect to the docstrings), it offers less control than the manual implementation of API descriptions. For instance, it is much harder to reduce the set of parameters to the essentials.</p> <p>Module for ingesting any Python module and generating a query builder.</p> <p>AutoGenerate Pydantic classes for each callable.</p> <p>This module provides a function to generate Pydantic classes for each callable (function/method) in a given module. It extracts parameters from docstrings using docstring-parser and creates Pydantic models with fields corresponding to the parameters. If a parameter name conflicts with BaseModel attributes, it is aliased.</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.generic_agent.GenericQueryBuilder","title":"<code>GenericQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a generic query using LLM tools.</p> <p>The query builder works by ingesting a Python module and generating a list of Pydantic classes for each callable in the module. It then uses these classes to parameterise a query using LLM tool binding.</p> Source code in <code>biochatter/biochatter/api_agent/python/generic_agent.py</code> <pre><code>class GenericQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a generic query using LLM tools.\n\n    The query builder works by ingesting a Python module and generating a list\n    of Pydantic classes for each callable in the module. It then uses these\n    classes to parameterise a query using LLM tool binding.\n    \"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: Conversation,\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for the query builder.\n\n        Args:\n        ----\n            query_parameters: The list of Pydantic classes to be used for the\n                query.\n\n            conversation: The conversation object used for parameterising the\n                query.\n\n        Returns:\n        -------\n            The runnable object for the query builder.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        prompt: str,\n        conversation: \"Conversation\",\n        module: ModuleType,\n        generated_classes: list[BaseAPIModel] | None = None,\n    ) -&gt; list[BaseAPIModel]:\n        \"\"\"Parameterise tool calls for any Python module.\n\n        Generate a list of parameterised BaseModel instances based on the given\n        question, prompt, and BioChatter conversation. Uses a Pydantic model\n        to define the API fields.\n\n        Using langchain's `bind_tools` method to allow the LLM to parameterise\n        the function call, based on the functions available in the module.\n\n        Relies on defined structure and annotation of the passed module.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            prompt (str): The prompt to be used for the query, instructing the\n                LLM of its task and the module context.\n\n            conversation: The conversation object used for parameterising the\n                query.\n\n            module: The Python module to be used for the query.\n\n            generated_classes: The list of Pydantic classes to be used for the\n                query. If not provided, the classes will be generated from the\n                module. Allows for external injection of classes for testing\n                purposes.\n\n        Returns:\n        -------\n            list[BaseAPIModel]: the parameterised query object (Pydantic\n                model)\n\n        \"\"\"\n        if generated_classes is None:\n            tools = generate_pydantic_classes(module)\n\n        runnable = self.create_runnable(\n            conversation=conversation,\n            query_parameters=tools,\n        )\n\n        query = [\n            (\"system\", prompt),\n            (\"human\", question),\n        ]\n\n        return runnable.invoke(query)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.generic_agent.GenericQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for the query builder.</p> <pre><code>query_parameters: The list of Pydantic classes to be used for the\n    query.\n\nconversation: The conversation object used for parameterising the\n    query.\n</code></pre> <pre><code>The runnable object for the query builder.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/generic_agent.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: Conversation,\n) -&gt; Callable:\n    \"\"\"Create a runnable object for the query builder.\n\n    Args:\n    ----\n        query_parameters: The list of Pydantic classes to be used for the\n            query.\n\n        conversation: The conversation object used for parameterising the\n            query.\n\n    Returns:\n    -------\n        The runnable object for the query builder.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.generic_agent.GenericQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, prompt, conversation, module, generated_classes=None)</code>","text":"<p>Parameterise tool calls for any Python module.</p> <p>Generate a list of parameterised BaseModel instances based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields.</p> <p>Using langchain's <code>bind_tools</code> method to allow the LLM to parameterise the function call, based on the functions available in the module.</p> <p>Relies on defined structure and annotation of the passed module.</p> <pre><code>question (str): The question to be answered.\n\nprompt (str): The prompt to be used for the query, instructing the\n    LLM of its task and the module context.\n\nconversation: The conversation object used for parameterising the\n    query.\n\nmodule: The Python module to be used for the query.\n\ngenerated_classes: The list of Pydantic classes to be used for the\n    query. If not provided, the classes will be generated from the\n    module. Allows for external injection of classes for testing\n    purposes.\n</code></pre> <pre><code>list[BaseAPIModel]: the parameterised query object (Pydantic\n    model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/generic_agent.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    prompt: str,\n    conversation: \"Conversation\",\n    module: ModuleType,\n    generated_classes: list[BaseAPIModel] | None = None,\n) -&gt; list[BaseAPIModel]:\n    \"\"\"Parameterise tool calls for any Python module.\n\n    Generate a list of parameterised BaseModel instances based on the given\n    question, prompt, and BioChatter conversation. Uses a Pydantic model\n    to define the API fields.\n\n    Using langchain's `bind_tools` method to allow the LLM to parameterise\n    the function call, based on the functions available in the module.\n\n    Relies on defined structure and annotation of the passed module.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        prompt (str): The prompt to be used for the query, instructing the\n            LLM of its task and the module context.\n\n        conversation: The conversation object used for parameterising the\n            query.\n\n        module: The Python module to be used for the query.\n\n        generated_classes: The list of Pydantic classes to be used for the\n            query. If not provided, the classes will be generated from the\n            module. Allows for external injection of classes for testing\n            purposes.\n\n    Returns:\n    -------\n        list[BaseAPIModel]: the parameterised query object (Pydantic\n            model)\n\n    \"\"\"\n    if generated_classes is None:\n        tools = generate_pydantic_classes(module)\n\n    runnable = self.create_runnable(\n        conversation=conversation,\n        query_parameters=tools,\n    )\n\n    query = [\n        (\"system\", prompt),\n        (\"human\", question),\n    ]\n\n    return runnable.invoke(query)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model--examples","title":"Examples","text":"<p>import scanpy as sc generated_classes = generate_pydantic_classes(sc.tl) for model in generated_classes: ...     print(model.schema())</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes","title":"<code>generate_pydantic_classes(module)</code>","text":"<p>Generate Pydantic classes for each callable.</p> <p>For each callable (function/method) in a given module. Extracts parameters from docstrings using docstring-parser. Each generated class has fields corresponding to the parameters of the function. If a parameter name conflicts with BaseModel attributes, it is aliased.</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes--params","title":"Params:","text":"<p>module : ModuleType     The Python module from which to extract functions and generate models.</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes--returns","title":"Returns","text":"<p>list[Type[BaseModel]]     A list of Pydantic model classes corresponding to each function found in         <code>module</code>.</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes--notes","title":"Notes","text":"<ul> <li>For now, all parameter types are set to <code>Any</code> to avoid complications with   complex or external classes that are not easily JSON-serializable.</li> <li>Optional parameters (those with a None default) are represented as   <code>Optional[Any]</code>.</li> <li>Required parameters (no default) use <code>...</code> to indicate that the field is   required.</li> </ul> Source code in <code>biochatter/biochatter/api_agent/python/autogenerate_model.py</code> <pre><code>def generate_pydantic_classes(module: ModuleType) -&gt; list[type[BaseAPIModel]]:\n    \"\"\"Generate Pydantic classes for each callable.\n\n    For each callable (function/method) in a given module. Extracts parameters\n    from docstrings using docstring-parser. Each generated class has fields\n    corresponding to the parameters of the function. If a parameter name\n    conflicts with BaseModel attributes, it is aliased.\n\n    Params:\n    -------\n    module : ModuleType\n        The Python module from which to extract functions and generate models.\n\n    Returns\n    -------\n    list[Type[BaseModel]]\n        A list of Pydantic model classes corresponding to each function found in\n            `module`.\n\n    Notes\n    -----\n    - For now, all parameter types are set to `Any` to avoid complications with\n      complex or external classes that are not easily JSON-serializable.\n    - Optional parameters (those with a None default) are represented as\n      `Optional[Any]`.\n    - Required parameters (no default) use `...` to indicate that the field is\n      required.\n\n    \"\"\"\n    base_attributes = set(dir(BaseAPIModel))\n    classes_list = []\n\n    for name, func in inspect.getmembers(module, inspect.isfunction):\n        # Skip private/internal functions (e.g., _something)\n        if name.startswith(\"_\"):\n            continue\n\n        # Parse docstring for parameter descriptions\n        doc = inspect.getdoc(func) or \"\"\n        parsed_doc = parse(doc)\n        doc_params = {p.arg_name: p.description or \"No description available.\" for p in parsed_doc.params}\n\n        sig = inspect.signature(func)\n        fields = {}\n\n        for param_name, param in sig.parameters.items():\n            # Skip *args and **kwargs for now\n            if param_name in (\"args\", \"kwargs\"):\n                continue\n\n            # Fetch docstring description or fallback\n            description = doc_params.get(param_name, \"No description available.\")\n\n            # Determine default value\n            # If no default, we use `...` indicating a required field\n            if param.default is not inspect.Parameter.empty:\n                default_value = param.default\n\n                # Convert MappingProxyType to a dict for JSON compatibility\n                if isinstance(default_value, MappingProxyType):\n                    default_value = dict(default_value)\n\n                # Handle non-JSON-compliant float values by converting to string\n                if default_value in [float(\"inf\"), float(\"-inf\"), float(\"nan\"), float(\"-nan\")]:\n                    default_value = str(default_value)\n            else:\n                default_value = ...  # No default means required\n\n            # For now, all parameter types are Any\n            annotation = Any\n\n            # Append the original annotation as a note in the description if\n            # available\n            if param.annotation is not inspect.Parameter.empty:\n                description += f\"\\nOriginal type annotation: {param.annotation}\"\n\n            # If default_value is None, parameter can be Optional\n            # If not required, mark as Optional[Any]\n            if default_value is None:\n                annotation = Any | None\n\n            # Prepare field kwargs\n            field_kwargs = {\"description\": description, \"default\": default_value}\n\n            # If field name conflicts with BaseModel attributes, alias it\n            field_name = param_name\n            if param_name in base_attributes:\n                alias_name = param_name + \"_param\"\n                field_kwargs[\"alias\"] = param_name\n                field_name = alias_name\n\n            fields[field_name] = (annotation, Field(**field_kwargs))\n\n        # Create the Pydantic model\n\n        tl_parameters_model = create_model(\n            name,\n            **fields,\n            __base__=BaseAPIModel,\n        )\n        classes_list.append(tl_parameters_model)\n    return classes_list\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#scanpy-modules","title":"Scanpy modules","text":"<p>We manually define the API descriptions for select Scanpy modules.</p> <p>Module for generating anndata queries using LLM tools.</p> <p>Module for interacting with the <code>scanpy</code> API for plotting (<code>pl</code>).</p> <p>Module for interacting with the <code>scanpy</code> API for plotting (<code>pl</code>).</p> <p>Scanpy Preprocessing (scanpy.pp) Query Builder.</p> <p>TODO: not sure if the approach below is functional yet.</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.AnnDataIOQueryBuilder","title":"<code>AnnDataIOQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a AnndataIO query object.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class AnnDataIOQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a AnndataIO query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a AnnDataIOQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                AnnDataIOQuery.\n\n        Returns:\n        -------\n            AnnDataIOQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tools = [\n            ReadCSV,\n            ReadExcel,\n            ReadH5AD,\n            ReadHDF,\n            ReadLoom,\n            ReadMTX,\n            ReadText,\n            ReadZarr,\n            ConcatenateAnnData,\n            MapAnnData,\n        ]\n        runnable = self.create_runnable(\n            conversation=conversation,\n            query_parameters=tools,\n        )\n        query = [\n            (\"system\", ANNDATA_IO_QUERY_PROMPT),\n            (\"human\", f\"{question}\"),\n        ]\n        return runnable.invoke(\n            query,\n        )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.AnnDataIOQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.AnnDataIOQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a AnnDataIOQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    AnnDataIOQuery.\n</code></pre> <pre><code>AnnDataIOQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a AnnDataIOQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            AnnDataIOQuery.\n\n    Returns:\n    -------\n        AnnDataIOQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tools = [\n        ReadCSV,\n        ReadExcel,\n        ReadH5AD,\n        ReadHDF,\n        ReadLoom,\n        ReadMTX,\n        ReadText,\n        ReadZarr,\n        ConcatenateAnnData,\n        MapAnnData,\n    ]\n    runnable = self.create_runnable(\n        conversation=conversation,\n        query_parameters=tools,\n    )\n    query = [\n        (\"system\", ANNDATA_IO_QUERY_PROMPT),\n        (\"human\", f\"{question}\"),\n    ]\n    return runnable.invoke(\n        query,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ConcatenateAnnData","title":"<code>ConcatenateAnnData</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Concatenate AnnData objects along an axis.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ConcatenateAnnData(BaseAPIModel):\n    \"\"\"Concatenate AnnData objects along an axis.\"\"\"\n\n    method_name: str = Field(default=\"anndata.concat\", description=\"NEVER CHANGE\")\n    adatas: list | dict = Field(\n        ...,\n        description=(\n            \"The objects to be concatenated. \"\n            \"Either a list of AnnData objects or a mapping of keys to AnnData objects.\"\n        ),\n    )\n    axis: str = Field(\n        default=\"obs\",\n        description=\"Axis to concatenate along. Can be 'obs' (0) or 'var' (1). Default is 'obs'.\",\n    )\n    join: str = Field(\n        default=\"inner\",\n        description=\"How to align values when concatenating. Options: 'inner' or 'outer'. Default is 'inner'.\",\n    )\n    merge: str | Callable | None = Field(\n        default=None,\n        description=(\n            \"How to merge elements not aligned to the concatenated axis. \"\n            \"Strategies include 'same', 'unique', 'first', 'only', or a callable function.\"\n        ),\n    )\n    uns_merge: str | Callable | None = Field(\n        default=None,\n        description=\"How to merge the .uns elements. Uses the same strategies as 'merge'.\",\n    )\n    label: str | None = Field(\n        default=None,\n        description=\"Column in axis annotation (.obs or .var) to place batch information. Default is None.\",\n    )\n    keys: list | None = Field(\n        default=None,\n        description=(\n            \"Names for each object being concatenated. \"\n            \"Used for column values or appended to the index if 'index_unique' is not None. \"\n            \"Default is None.\"\n        ),\n    )\n    index_unique: str | None = Field(\n        default=None,\n        description=\"Delimiter for making the index unique. When None, original indices are kept.\",\n    )\n    fill_value: Any | None = Field(\n        default=None,\n        description=\"Value used to fill missing indices when join='outer'. Default behavior depends on array type.\",\n    )\n    pairwise: bool = Field(\n        default=False,\n        description=\"Include pairwise elements along the concatenated dimension. Default is False.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.MapAnnData","title":"<code>MapAnnData</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Apply mapping functions to elements of AnnData.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class MapAnnData(BaseAPIModel):\n    \"\"\"Apply mapping functions to elements of AnnData.\"\"\"\n\n    method_name: str = Field(\n        default=\"anndata.obs|var['annotation_name'].map\",\n        description=(\n            \"ALWAYS ALWAYS ALWAYS REPLACE THE anndata BY THE ONE GIVEN BY THE INPUT\"\n            \"Specifies the AnnData attribute and operation being performed. \"\n            \"For example, 'obs.map' applies a mapping function or dictionary to the specified column in `adata.obs`. \"\n            \"This must always include the AnnData component and the `.map` operation. \"\n            \"Adapt the component (e.g., 'obs', 'var', etc.) to the specific use case.\"\n        ),\n    )\n    dics: dict | None = Field(default=None, description=\"Dictionary to map over.\")\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadCSV","title":"<code>ReadCSV</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .csv file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadCSV(BaseAPIModel):\n    \"\"\"Read .csv file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_csv\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.csv\",\n        description=\"Path to the .csv file\",\n    )\n    delimiter: str | None = Field(\n        None,\n        description=\"Delimiter used in the .csv file\",\n    )\n    first_column_names: bool | None = Field(\n        None,\n        description=\"Whether the first column contains names\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadExcel","title":"<code>ReadExcel</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .xlsx (Excel) file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadExcel(BaseAPIModel):\n    \"\"\"Read .xlsx (Excel) file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_excel\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.xlsx\",\n        description=\"Path to the .xlsx file\",\n    )\n    sheet: str | None = Field(None, description=\"Sheet name or index to read from\")\n    dtype: str | None = Field(\n        None,\n        description=\"Data type for the resulting dataframe\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadH5AD","title":"<code>ReadH5AD</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .h5ad-formatted hdf5 file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadH5AD(BaseAPIModel):\n    \"\"\"Read .h5ad-formatted hdf5 file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_h5ad\", description=\"NEVER CHANGE\")\n    filename: str = Field(default=\"dummy.h5ad\", description=\"Path to the .h5ad file\")\n    backed: str | None = Field(\n        default=None,\n        description=\"Mode to access file: None, 'r' for read-only\",\n    )\n    as_sparse: str | None = Field(\n        default=None,\n        description=\"Convert to sparse format: 'csr', 'csc', or None\",\n    )\n    as_sparse_fmt: str | None = Field(\n        default=None,\n        description=\"Sparse format if converting, e.g., 'csr'\",\n    )\n    index_unique: str | None = Field(\n        default=None,\n        description=\"Make index unique by appending suffix if needed\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadHDF","title":"<code>ReadHDF</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .h5 (hdf5) file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadHDF(BaseAPIModel):\n    \"\"\"Read .h5 (hdf5) file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_hdf\", description=\"NEVER CHANGE\")\n    filename: str = Field(default=\"placeholder.h5\", description=\"Path to the .h5 file\")\n    key: str | None = Field(None, description=\"Group key within the .h5 file\")\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadLoom","title":"<code>ReadLoom</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .loom-formatted hdf5 file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadLoom(BaseAPIModel):\n    \"\"\"Read .loom-formatted hdf5 file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_loom\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.loom\",\n        description=\"Path to the .loom file\",\n    )\n    sparse: bool | None = Field(None, description=\"Whether to read data as sparse\")\n    cleanup: bool | None = Field(None, description=\"Clean up invalid entries\")\n    X_name: str | None = Field(None, description=\"Name to use for X matrix\")\n    obs_names: str | None = Field(\n        None,\n        description=\"Column to use for observation names\",\n    )\n    var_names: str | None = Field(\n        None,\n        description=\"Column to use for variable names\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadMTX","title":"<code>ReadMTX</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .mtx file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadMTX(BaseAPIModel):\n    \"\"\"Read .mtx file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_mtx\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.mtx\",\n        description=\"Path to the .mtx file\",\n    )\n    dtype: str | None = Field(None, description=\"Data type for the matrix\")\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadText","title":"<code>ReadText</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .txt, .tab, .data (text) file.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadText(BaseAPIModel):\n    \"\"\"Read .txt, .tab, .data (text) file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_text\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.txt\",\n        description=\"Path to the text file\",\n    )\n    delimiter: str | None = Field(None, description=\"Delimiter used in the file\")\n    first_column_names: bool | None = Field(\n        None,\n        description=\"Whether the first column contains names\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadZarr","title":"<code>ReadZarr</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read from a hierarchical Zarr array store.</p> Source code in <code>biochatter/biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadZarr(BaseAPIModel):\n    \"\"\"Read from a hierarchical Zarr array store.\"\"\"\n\n    method_name: str = Field(default=\"io.read_zarr\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.zarr\",\n        description=\"Path or URL to the Zarr store\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlDrawGraphQueryParameters","title":"<code>ScanpyPlDrawGraphQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.draw_graph</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlDrawGraphQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.draw_graph` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.draw_graph\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=(\n            \"For continuous annotations used as color parameter, \"\n            \"plot data points with higher values on top of others.\"\n        ),\n    )\n    edges: bool = Field(\n        default=False,\n        description=\"Show edges.\",\n    )\n    edges_width: float = Field(\n        default=0.1,\n        description=\"Width of edges.\",\n    )\n    edges_color: str | list[float] | list[str] = Field(\n        default=\"grey\",\n        description=\"Color of edges.\",\n    )\n    neighbors_key: str | None = Field(\n        default=None,\n        description=\"Where to look for neighbors connectivities.\",\n    )\n    arrows: bool = Field(\n        default=False,\n        description=\"Show arrows (deprecated in favor of `scvelo.pl.velocity_embedding`).\",\n    )\n    arrows_kwds: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Arguments passed to `quiver()`.\",\n    )\n    groups: str | list[str] | None = Field(\n        default=None,\n        description=\"Restrict to a few categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"For instance, ['1,2', '2,3']. To plot all available components use components='all'.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Numeric size in pt or string describing the size.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Legend font weight.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    colorbar_loc: str | None = Field(\n        default=\"right\",\n        description=\"Where to place the colorbar for continuous variables.\",\n    )\n    size: float | list[float] | None = Field(\n        default=None,\n        description=\"Point size. If None, is automatically computed as 120000 / n_cells.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] = Field(\n        default=\"lightgray\",\n        description=\"Color to use for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"If there are missing values, whether they get an entry in the legend.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    layout: str | None = Field(\n        default=None,\n        description=\"One of the `draw_graph()` layouts.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlPcaQueryParameters","title":"<code>ScanpyPlPcaQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.pca</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlPcaQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.pca` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.pca\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    components: str | list[str] = Field(\n        default=\"1,2\",\n        description=\"For example, ['1,2', '2,3']. To plot all available components use 'all'.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Font size for legend.\",\n    )\n    legend_fontweight: int | str | None = Field(\n        default=None,\n        description=\"Font weight for legend.\",\n    )\n    color_map: str | None = Field(\n        default=None,\n        description=\"String denoting matplotlib color map.\",\n    )\n    palette: str | list[str] | dict | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    size: int | float | None = Field(\n        default=None,\n        description=\"Point size. If `None`, is automatically computed as 120000 / n_cells.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot, do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: str | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    return_fig: bool = Field(\n        default=False,\n        description=\"Return the matplotlib figure object.\",\n    )\n    marker: str | None = Field(\n        default=\".\",\n        description=\"Marker symbol.\",\n    )\n    annotate_var_explained: bool = Field(\n        default=False,\n        description=\"Annotate the percentage of explained variance.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlQueryBuilder","title":"<code>ScanpyPlQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a AnndataIO query object.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a AnndataIO query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters)\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a AnnDataIOQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                ScanpyPlQuery.\n\n        Returns:\n        -------\n            ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tools = [\n            ScanpyPlScatterQueryParameters,\n            ScanpyPlPcaQueryParameters,\n            ScanpyPlTsneQueryParameters,\n            ScanpyPlUmapQueryParameters,\n            ScanpyPlDrawGraphQueryParameters,\n            ScanpyPlSpatialQueryParameters,\n        ]\n        runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n        return runnable.invoke(question)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters)\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a AnnDataIOQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    ScanpyPlQuery.\n</code></pre> <pre><code>ScanpyPlQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a AnnDataIOQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            ScanpyPlQuery.\n\n    Returns:\n    -------\n        ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tools = [\n        ScanpyPlScatterQueryParameters,\n        ScanpyPlPcaQueryParameters,\n        ScanpyPlTsneQueryParameters,\n        ScanpyPlUmapQueryParameters,\n        ScanpyPlDrawGraphQueryParameters,\n        ScanpyPlSpatialQueryParameters,\n    ]\n    runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n    return runnable.invoke(question)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlScatterQueryParameters","title":"<code>ScanpyPlScatterQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.scatter</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlScatterQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.scatter` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.scatter\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(description=\"Annotated data matrix.\")\n    x: str | None = Field(default=None, description=\"x coordinate.\")\n    y: str | None = Field(default=None, description=\"y coordinate.\")\n    color: str | tuple[float, ...] | list[str | tuple[float, ...]] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes, or a hex color specification.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Whether to use raw attribute of adata. Defaults to True if .raw is present.\",\n    )\n    layers: str | list[str] | None = Field(\n        default=None,\n        description=\"Layer(s) to use from adata's layers attribute.\",\n    )\n    basis: str | None = Field(\n        default=None,\n        description=\"String that denotes a plotting tool that computed coordinates (e.g., 'pca', 'tsne', 'umap').\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=\"For continuous annotations used as color parameter, plot data points with higher values on top.\",\n    )\n    groups: str | list[str] | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot ('2d' or '3d').\",\n    )\n    legend_loc: str | None = Field(\n        default=\"right margin\",\n        description=\"Location of legend ('none', 'right margin', 'on data', etc.).\",\n    )\n    size: int | float | None = Field(\n        default=None,\n        description=\"Point size. If None, automatically computed as 120000 / n_cells.\",\n    )\n    color_map: str | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables (e.g., 'magma', 'viridis').\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot, do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If True or a str, save the figure. String is appended to default filename.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlSpatialQueryParameters","title":"<code>ScanpyPlSpatialQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.spatial</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlSpatialQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.spatial` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.spatial\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    library_id: str | None = Field(\n        default=None,\n        description=\"Library ID for Visium data, e.g., key in `adata.uns['spatial']`.\",\n    )\n    img_key: str | None = Field(\n        default=None,\n        description=(\n            \"Key for image data, used to get `img` and `scale_factor` from \"\n            \"'images' and 'scalefactors' entries for this library.\"\n        ),\n    )\n    img: Any | None = Field(\n        default=None,\n        description=\"Image data to plot, overrides `img_key`.\",\n    )\n    scale_factor: float | None = Field(\n        default=None,\n        description=\"Scaling factor used to map from coordinate space to pixel space.\",\n    )\n    spot_size: float | None = Field(\n        default=None,\n        description=\"Diameter of spot (in coordinate space) for each point.\",\n    )\n    crop_coord: tuple[int, ...] | None = Field(\n        default=None,\n        description=\"Coordinates to use for cropping the image (left, right, top, bottom).\",\n    )\n    alpha_img: float = Field(\n        default=1.0,\n        description=\"Alpha value for image.\",\n    )\n    bw: bool = Field(\n        default=False,\n        description=\"Plot image data in grayscale.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=(\n            \"For continuous annotations used as color parameter, plot data points \"\n            \"with higher values on top of others.\"\n        ),\n    )\n    groups: str | list[str] | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"For example, ['1,2', '2,3']. To plot all available components, use 'all'.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Numeric size in pt or string describing the size.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Legend font weight.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    colorbar_loc: str | None = Field(\n        default=\"right\",\n        description=\"Where to place the colorbar for continuous variables.\",\n    )\n    size: float = Field(\n        default=1.0,\n        description=\"Point size. If None, automatically computed as 120000 / n_cells.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] | None = Field(\n        default=None,\n        description=\"Color to use for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"If there are missing values, whether they get an entry in the legend.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlTsneQueryParameters","title":"<code>ScanpyPlTsneQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.tsne</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlTsneQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.tsne` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.tsne\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=\"Plot data points with higher values on top for continuous annotations.\",\n    )\n    edges: bool = Field(\n        default=False,\n        description=\"Show edges.\",\n    )\n    edges_width: float = Field(\n        default=0.1,\n        description=\"Width of edges.\",\n    )\n    edges_color: str | list[float] | list[str] = Field(\n        default=\"grey\",\n        description=\"Color of edges.\",\n    )\n    neighbors_key: str | None = Field(\n        default=None,\n        description=\"Key for neighbors connectivities.\",\n    )\n    arrows: bool = Field(\n        default=False,\n        description=\"Show arrows (deprecated in favor of `scvelo.pl.velocity_embedding`).\",\n    )\n    arrows_kwds: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Arguments passed to `quiver()`.\",\n    )\n    groups: str | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"Components to plot, e.g., ['1,2', '2,3']. Use 'all' to plot all available components.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot ('2d' or '3d').\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Font size for legend.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Font weight for legend.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    size: float | list[float] | None = Field(\n        default=None,\n        description=\"Point size. If `None`, computed as 120000 / n_cells.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] = Field(\n        default=\"lightgray\",\n        description=\"Color for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"Include missing values in the legend.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlUmapQueryParameters","title":"<code>ScanpyPlUmapQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.umap</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlUmapQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.umap` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.umap\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    mask_obs: str | None = Field(\n        default=None,\n        description=\"Mask for observations.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=\"Plot data points with higher values on top for continuous annotations.\",\n    )\n    edges: bool = Field(\n        default=False,\n        description=\"Show edges.\",\n    )\n    edges_width: float = Field(\n        default=0.1,\n        description=\"Width of edges.\",\n    )\n    edges_color: str | list[float] | list[str] = Field(\n        default=\"grey\",\n        description=\"Color of edges.\",\n    )\n    neighbors_key: str | None = Field(\n        default=None,\n        description=\"Key for neighbors connectivities.\",\n    )\n    arrows: bool = Field(\n        default=False,\n        description=\"Show arrows (deprecated in favor of `scvelo.pl.velocity_embedding`).\",\n    )\n    arrows_kwds: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Arguments passed to `quiver()`.\",\n    )\n    groups: str | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"Components to plot, e.g., ['1,2', '2,3']. Use 'all' to plot all available components.\",\n    )\n    dimensions: int | None = Field(\n        default=None,\n        description=\"Number of dimensions to plot.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot ('2d' or '3d').\",\n    )\n    scale_factor: float | None = Field(\n        default=None,\n        description=\"Scale factor for the plot.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map for continuous variables.\",\n    )\n    cmap: str | Any | None = Field(\n        default=None,\n        description=\"Alias for `color_map`.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] = Field(\n        default=\"lightgray\",\n        description=\"Color for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"Include missing values in the legend.\",\n    )\n    size: float | list[float] | None = Field(\n        default=None,\n        description=\"Point size. If `None`, computed as 120000 / n_cells.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Font size for legend.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Font weight for legend.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    colorbar_loc: str = Field(\n        default=\"right\",\n        description=\"Location of the colorbar.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    marker: str = Field(\n        default=\".\",\n        description=\"Marker symbol.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlDrawGraphQueryParameters","title":"<code>ScanpyPlDrawGraphQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.draw_graph</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlDrawGraphQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.draw_graph` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.draw_graph\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlPcaQueryParameters","title":"<code>ScanpyPlPcaQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.pca</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlPcaQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.pca` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.pca\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    color_map: str | None = Field(\n        default=None,\n        description=\"String denoting matplotlib color map.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlQueryBuilder","title":"<code>ScanpyPlQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a AnndataIO query object.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a AnndataIO query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a AnnDataIOQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                ScanpyPlQuery.\n\n        Returns:\n        -------\n            ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tools = [\n            ScanpyPlScatterQueryParameters,\n            ScanpyPlPcaQueryParameters,\n            ScanpyPlTsneQueryParameters,\n            ScanpyPlUmapQueryParameters,\n            ScanpyPlDrawGraphQueryParameters,\n            ScanpyPlSpatialQueryParameters,\n        ]\n        runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n        return runnable.invoke(question)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a AnnDataIOQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    ScanpyPlQuery.\n</code></pre> <pre><code>ScanpyPlQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a AnnDataIOQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            ScanpyPlQuery.\n\n    Returns:\n    -------\n        ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tools = [\n        ScanpyPlScatterQueryParameters,\n        ScanpyPlPcaQueryParameters,\n        ScanpyPlTsneQueryParameters,\n        ScanpyPlUmapQueryParameters,\n        ScanpyPlDrawGraphQueryParameters,\n        ScanpyPlSpatialQueryParameters,\n    ]\n    runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n    return runnable.invoke(question)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlScatterQueryParameters","title":"<code>ScanpyPlScatterQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.scatter</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlScatterQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.scatter` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.scatter\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(description=\"Annotated data matrix.\")\n    x: str | None = Field(default=None, description=\"x coordinate.\")\n    y: str | None = Field(default=None, description=\"y coordinate.\")\n    color: str | tuple[float, ...] | list[str | tuple[float, ...]] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes, or a hex color specification.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Whether to use raw attribute of adata. Defaults to True if .raw is present.\",\n    )\n    layers: str | list[str] | None = Field(\n        default=None,\n        description=\"Layer(s) to use from adata's layers attribute.\",\n    )\n    basis: str | None = Field(\n        default=None,\n        description=\"String that denotes a plotting tool that computed coordinates (e.g., 'pca', 'tsne', 'umap').\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlSpatialQueryParameters","title":"<code>ScanpyPlSpatialQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.spatial</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlSpatialQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.spatial` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.spatial\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    library_id: str | None = Field(\n        default=None,\n        description=\"Library ID for Visium data, e.g., key in `adata.uns['spatial']`.\",\n    )\n    img_key: str | None = Field(\n        default=None,\n        description=(\n            \"Key for image data, used to get `img` and `scale_factor` from \"\n            \"'images' and 'scalefactors' entries for this library.\"\n        ),\n    )\n    img: Any | None = Field(\n        default=None,\n        description=\"Image data to plot, overrides `img_key`.\",\n    )\n    scale_factor: float | None = Field(\n        default=None,\n        description=\"Scaling factor used to map from coordinate space to pixel space.\",\n    )\n    spot_size: float | None = Field(\n        default=None,\n        description=\"Diameter of spot (in coordinate space) for each point.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlTsneQueryParameters","title":"<code>ScanpyPlTsneQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.tsne</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlTsneQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.tsne` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.tsne\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    groups: str | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlUmapQueryParameters","title":"<code>ScanpyPlUmapQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.umap</code> API.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlUmapQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.umap` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.umap\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpFuncs","title":"<code>ScanpyPpFuncs</code>","text":"<p>               Bases: <code>BaseTools</code></p> <p>Scanpy Preprocessing (scanpy.pp) Query Builder.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>class ScanpyPpFuncs(BaseTools):\n    \"\"\"Scanpy Preprocessing (scanpy.pp) Query Builder.\"\"\"\n\n    tools_params = {}\n\n    tools_params[\"filter_cells\"] = {\n        \"data\": (str, Field(..., description=\"The (annotated) data matrix.\")),\n        \"min_counts\": (Optional[int], Field(None, description=\"Minimum counts per cell.\")),\n        \"min_genes\": (Optional[int], Field(None, description=\"Minimum genes expressed in a cell.\")),\n        \"max_counts\": (Optional[int], Field(None, description=\"Maximum counts per cell.\")),\n        \"max_genes\": (Optional[int], Field(None, description=\"Maximum genes expressed in a cell.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to modify the data in place.\")),\n    }\n\n    tools_params[\"filter_genes\"] = {\n        \"data\": (str, Field(..., description=\"The (annotated) data matrix.\")),\n        \"min_counts\": (Optional[int], Field(None, description=\"Minimum counts per gene.\")),\n        \"min_cells\": (Optional[int], Field(None, description=\"Minimum number of cells expressing the gene.\")),\n        \"max_counts\": (Optional[int], Field(None, description=\"Maximum counts per gene.\")),\n        \"max_cells\": (Optional[int], Field(None, description=\"Maximum number of cells expressing the gene.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to modify the data in place.\")),\n    }\n\n    tools_params[\"highly_variable_genes\"] = {\n        \"adata\": (str, Field(..., description=\"Annotated data matrix.\")),\n        \"n_top_genes\": (Optional[int], Field(None, description=\"Number of highly-variable genes to keep.\")),\n        \"min_mean\": (float, Field(0.0125, description=\"Minimum mean expression for highly-variable genes.\")),\n        \"max_mean\": (float, Field(3, description=\"Maximum mean expression for highly-variable genes.\")),\n        \"flavor\": (str, Field(\"seurat\", description=\"Method for identifying highly-variable genes.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to place metrics in .var or return them.\")),\n    }\n\n    tools_params[\"log1p\"] = {\n        \"data\": (str, Field(..., description=\"The data matrix.\")),\n        \"base\": (Optional[float], Field(None, description=\"Base of the logarithm.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n        \"chunked\": (Optional[bool], Field(None, description=\"Process data in chunks.\")),\n    }\n\n    tools_params[\"pca\"] = {\n        \"data\": (str, Field(..., description=\"The (annotated) data matrix.\")),\n        \"n_comps\": (Optional[int], Field(None, description=\"Number of principal components to compute.\")),\n        \"layer\": (Optional[str], Field(None, description=\"Element of layers to use for PCA.\")),\n        \"zero_center\": (bool, Field(True, description=\"Whether to zero-center the data.\")),\n        \"svd_solver\": (Optional[str], Field(None, description=\"SVD solver to use.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"normalize_total\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"target_sum\": (Optional[float], Field(None, description=\"Target sum after normalization.\")),\n        \"exclude_highly_expressed\": (bool, Field(False, description=\"Whether to exclude highly expressed genes.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to update adata or return normalized data.\")),\n    }\n\n    tools_params[\"regress_out\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"keys\": (Union[str, Collection[str]], Field(..., description=\"Keys for regression.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"scale\"] = {\n        \"data\": (str, Field(..., description=\"The data matrix.\")),\n        \"zero_center\": (bool, Field(True, description=\"Whether to zero-center the data.\")),\n        \"copy\": (bool, Field(False, description=\"Whether to perform operation inplace.\")),\n    }\n\n    tools_params[\"subsample\"] = {\n        \"data\": (str, Field(..., description=\"The data matrix.\")),\n        \"fraction\": (Optional[float], Field(None, description=\"Fraction of observations to subsample.\")),\n        \"n_obs\": (Optional[int], Field(None, description=\"Number of observations to subsample.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"downsample_counts\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"counts_per_cell\": (Optional[int | str], Field(None, description=\"Target total counts per cell.\")),\n        \"replace\": (bool, Field(False, description=\"Whether to sample with replacement.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"combat\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"key\": (str, Field(\"batch\", description=\"Key for batch effect removal.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to replace the data inplace.\")),\n    }\n\n    tools_params[\"scrublet\"] = {\n        \"adata\": (str, Field(..., description=\"Annotated data matrix.\")),\n        \"sim_doublet_ratio\": (float, Field(2.0, description=\"Number of doublets to simulate.\")),\n        \"threshold\": (Optional[float], Field(None, description=\"Doublet score threshold.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"scrublet_simulate_doublets\"] = {\n        \"adata\": (str, Field(..., description=\"Annotated data matrix.\")),\n        \"sim_doublet_ratio\": (float, Field(2.0, description=\"Number of doublets to simulate.\")),\n        \"random_seed\": (int, Field(0, description=\"Random seed for reproducibility.\")),\n    }\n    tools_params[\"calculate_qc_metrics\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"expr_type\": (str, Field(\"counts\", description=\"Name of kind of values in X.\")),\n        \"var_type\": (str, Field(\"genes\", description=\"The kind of thing the variables are.\")),\n        \"qc_vars\": (\n            Collection[str],\n            Field(\n                (),\n                description=\"Keys for boolean columns of .var which identify variables you could want to control for (e.g., \u201cERCC\u201d or \u201cmito\u201d).\",\n            ),\n        ),\n        \"percent_top\": (\n            Collection[int],\n            Field(\n                (50, 100, 200, 500),\n                description=\"List of ranks at which cumulative proportion of expression will be reported as a percentage.\",\n            ),\n        ),\n        \"layer\": (\n            Optional[str],\n            Field(None, description=\"If provided, use adata.layers[layer] for expression values instead of adata.X.\"),\n        ),\n        \"use_raw\": (\n            bool,\n            Field(False, description=\"If True, use adata.raw.X for expression values instead of adata.X.\"),\n        ),\n        \"inplace\": (bool, Field(False, description=\"Whether to place calculated metrics in adata\u2019s .obs and .var.\")),\n        \"log1p\": (bool, Field(True, description=\"Set to False to skip computing log1p transformed annotations.\")),\n    }\n\n    tools_params[\"recipe_zheng17\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"n_top_genes\": (int, Field(1000, description=\"Number of genes to keep.\")),\n        \"log\": (bool, Field(True, description=\"Take logarithm of the data.\")),\n        \"plot\": (bool, Field(False, description=\"Show a plot of the gene dispersion vs. mean relation.\")),\n        \"copy\": (bool, Field(False, description=\"Return a copy of adata instead of updating it.\")),\n    }\n\n    tools_params[\"recipe_weinreb17\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"log\": (bool, Field(True, description=\"Logarithmize the data?\")),\n        \"mean_threshold\": (float, Field(0.01, description=\"Mean expression threshold for gene selection.\")),\n        \"cv_threshold\": (float, Field(2, description=\"Coefficient of variation threshold for gene selection.\")),\n        \"n_pcs\": (int, Field(50, description=\"Number of principal components to compute.\")),\n        \"svd_solver\": (str, Field(\"randomized\", description=\"SVD solver to use for PCA.\")),\n        \"random_state\": (int, Field(0, description=\"Random seed for reproducibility.\")),\n        \"copy\": (\n            bool,\n            Field(False, description=\"Return a copy if true, otherwise modifies the original adata object.\"),\n        ),\n    }\n\n    tools_params[\"recipe_seurat\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"log\": (bool, Field(True, description=\"Logarithmize the data?\")),\n        \"plot\": (bool, Field(False, description=\"Show a plot of the gene dispersion vs. mean relation.\")),\n        \"copy\": (\n            bool,\n            Field(False, description=\"Return a copy if true, otherwise modifies the original adata object.\"),\n        ),\n    }\n\n    def __init__(self, tools_params: dict = tools_params) -&gt; None:\n        \"\"\"Initialise the ScanpyPpFuncs class.\"\"\"\n        super().__init__()\n        self.tools_params = tools_params\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpFuncs.__init__","title":"<code>__init__(tools_params=tools_params)</code>","text":"<p>Initialise the ScanpyPpFuncs class.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>def __init__(self, tools_params: dict = tools_params) -&gt; None:\n    \"\"\"Initialise the ScanpyPpFuncs class.\"\"\"\n    super().__init__()\n    self.tools_params = tools_params\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpQueryBuilder","title":"<code>ScanpyPpQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a ScanpyPp query object.</p> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>class ScanpyPpQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a ScanpyPp query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters)\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a ScanpyPp query object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                ScanpyPpQuery.\n\n        Returns:\n        -------\n            ScanpyPpQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tool_maker = ScanpyPpFuncs()\n        tools = tool_maker.make_pydantic_tools()\n        runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n        return runnable.invoke(\n            question,\n        )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters)\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a ScanpyPp query object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    ScanpyPpQuery.\n</code></pre> <pre><code>ScanpyPpQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a ScanpyPp query object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            ScanpyPpQuery.\n\n    Returns:\n    -------\n        ScanpyPpQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tool_maker = ScanpyPpFuncs()\n    tools = tool_maker.make_pydantic_tools()\n    runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n    return runnable.invoke(\n        question,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#api-calling-utility-functions","title":"API Calling: Utility functions","text":""},{"location":"BioChatter/api-docs/api-calling-python/#formatters-to-parse-the-calls","title":"Formatters to parse the calls","text":"<p>Formatters for API calls (Pydantic models to strings).</p>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.base.formatters.format_as_python_call","title":"<code>format_as_python_call(model)</code>","text":"<p>Convert a parameter model into a Python method call string.</p> <pre><code>model: Pydantic model containing method parameters\n</code></pre> <pre><code>String representation of the Python method call\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/formatters.py</code> <pre><code>def format_as_python_call(model: BaseAPIModel) -&gt; str:\n    \"\"\"Convert a parameter model into a Python method call string.\n\n    Args:\n    ----\n        model: Pydantic model containing method parameters\n\n    Returns:\n    -------\n        String representation of the Python method call\n\n    \"\"\"\n    params = model.dict(exclude_none=True)\n    method_name = params.pop(\"method_name\", None)\n    params.pop(\"question_uuid\", None)\n    if isinstance(model, MapAnnData):\n        param_str = params.pop(\"dics\", {})\n    else:\n        param_str = \", \".join(f\"{k}={v!r}\" for k, v in params.items())\n\n    return f\"{method_name}({param_str})\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-python/#biochatter.api_agent.base.formatters.format_as_rest_call","title":"<code>format_as_rest_call(model)</code>","text":"<p>Convert a parameter model (BaseModel) into a REST API call string.</p> <pre><code>model: Pydantic model containing API call parameters\n</code></pre> <pre><code>String representation of the REST API call\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/base/formatters.py</code> <pre><code>def format_as_rest_call(model: BaseModel) -&gt; str:\n    \"\"\"Convert a parameter model (BaseModel) into a REST API call string.\n\n    Args:\n    ----\n        model: Pydantic model containing API call parameters\n\n    Returns:\n    -------\n        String representation of the REST API call\n\n    \"\"\"\n    params = model.dict(exclude_none=True)\n    endpoint = params.pop(\"endpoint\")\n    base_url = params.pop(\"base_url\")\n    params.pop(\"question_uuid\", None)\n\n    full_url = f\"{base_url.rstrip('/')}/{endpoint.strip('/')}\"\n    return f\"{full_url}?{urlencode(params)}\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/","title":"API Calling: Web APIs","text":""},{"location":"BioChatter/api-docs/api-calling-web/#blast","title":"BLAST","text":"<p>Module for handling BLAST API interactions.</p> <p>Provides functionality for building queries, fetching results, and interpreting BLAST (Basic Local Alignment Search Tool) sequence alignment data.</p>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastFetcher","title":"<code>BlastFetcher</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>A class for retrieving API results from BLAST.</p> <p>Retrieves results from BLAST given a parameterised BlastQuery.</p> <p>TODO add a limit of characters to be returned from the response.text?</p> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>class BlastFetcher(BaseFetcher):\n    \"\"\"A class for retrieving API results from BLAST.\n\n    Retrieves results from BLAST given a parameterised BlastQuery.\n\n    TODO add a limit of characters to be returned from the response.text?\n    \"\"\"\n\n    def _submit_query(self, request_data: BlastQueryParameters) -&gt; str:\n        \"\"\"POST the BLAST query and retrieve the RID.\n\n        The method submits the structured BlastQuery object and returns the RID.\n\n        Args:\n        ----\n            request_data: BlastQuery object containing the BLAST query\n                parameters.\n\n        Returns:\n        -------\n            str: The Request ID (RID) for the submitted BLAST query.\n\n        \"\"\"\n        data = {\n            \"CMD\": request_data.cmd,\n            \"PROGRAM\": request_data.program,\n            \"DATABASE\": request_data.database,\n            \"QUERY\": request_data.query,\n            \"FORMAT_TYPE\": request_data.format_type,\n            \"MEGABLAST\": request_data.megablast,\n            \"HITLIST_SIZE\": request_data.max_hits,\n        }\n        # Include any other_params if provided\n        if request_data.other_params:\n            data.update(request_data.other_params)\n        # Make the API call\n        query_string = urlencode(data)\n        # Combine base URL with the query string\n        full_url = f\"{request_data.url}?{query_string}\"\n        # Print the full URL\n        request_data.full_url = full_url\n        print(\"Full URL built by retriever:\\n\", request_data.full_url)\n        response = requests.post(request_data.url, data=data, timeout=10)\n        response.raise_for_status()\n        # Extract RID from response\n        print(response)\n        match = re.search(r\"RID = (\\w+)\", response.text)\n        if match:\n            return match.group(1)\n\n        msg = \"RID not found in BLAST submission response.\"\n        raise ValueError(msg)\n\n    def _fetch_results(\n        self,\n        rid: str,\n        question_uuid: str,\n        retries: int = 10000,\n    ) -&gt; str:\n        \"\"\"Fetch BLAST query data given RID.\n\n        The second function to be called for a BLAST query.\n        \"\"\"\n        base_url = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n        check_status_params = {\n            \"CMD\": \"Get\",\n            \"FORMAT_OBJECT\": \"SearchInfo\",\n            \"RID\": rid,\n        }\n        get_results_params = {\n            \"CMD\": \"Get\",\n            \"FORMAT_TYPE\": \"XML\",\n            \"RID\": rid,\n        }\n\n        # Check the status of the BLAST job\n        for attempt in range(retries):\n            status_response = requests.get(base_url, params=check_status_params, timeout=10)\n            status_response.raise_for_status()\n            status_text = status_response.text\n            print(\"evaluating status\")\n            if \"Status=WAITING\" in status_text:\n                print(f\"{question_uuid} results not ready, waiting...\")\n                time.sleep(15)\n            elif \"Status=FAILED\" in status_text:\n                msg = \"BLAST query FAILED.\"\n                raise RuntimeError(msg)\n            elif \"Status=UNKNOWN\" in status_text:\n                msg = \"BLAST query expired or does not exist.\"\n                raise RuntimeError(msg)\n            elif \"Status=READY\" in status_text:\n                if \"ThereAreHits=yes\" in status_text:\n                    print(f\"{question_uuid} results are ready, retrieving.\")\n                    results_response = requests.get(\n                        base_url,\n                        params=get_results_params,\n                        timeout=10,\n                    )\n                    results_response.raise_for_status()\n                    return results_response.text\n                return \"No hits found\"\n            if attempt == retries - 1:\n                msg = \"Maximum attempts reached. Results may not be ready.\"\n                raise TimeoutError(msg)\n        return None\n\n    def fetch_results(\n        self,\n        query_models: list[BlastQueryParameters],\n        retries: int = 20,\n    ) -&gt; str:\n        \"\"\"Submit request and fetch results from BLAST API.\n\n        Wraps individual submission and retrieval of results.\n\n        Args:\n        ----\n            query_models: list of Pydantic models of the queries\n            retries: the number of maximum retries\n\n        Returns:\n        -------\n            str: the result from the BLAST API\n\n        \"\"\"\n        # For now, we only use the first query in the list\n        query = query_models[0]\n        rid = self._submit_query(request_data=query)\n        return self._fetch_results(\n            rid=rid,\n            question_uuid=query.question_uuid,\n            retries=retries,\n        )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastFetcher._fetch_results","title":"<code>_fetch_results(rid, question_uuid, retries=10000)</code>","text":"<p>Fetch BLAST query data given RID.</p> <p>The second function to be called for a BLAST query.</p> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>def _fetch_results(\n    self,\n    rid: str,\n    question_uuid: str,\n    retries: int = 10000,\n) -&gt; str:\n    \"\"\"Fetch BLAST query data given RID.\n\n    The second function to be called for a BLAST query.\n    \"\"\"\n    base_url = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n    check_status_params = {\n        \"CMD\": \"Get\",\n        \"FORMAT_OBJECT\": \"SearchInfo\",\n        \"RID\": rid,\n    }\n    get_results_params = {\n        \"CMD\": \"Get\",\n        \"FORMAT_TYPE\": \"XML\",\n        \"RID\": rid,\n    }\n\n    # Check the status of the BLAST job\n    for attempt in range(retries):\n        status_response = requests.get(base_url, params=check_status_params, timeout=10)\n        status_response.raise_for_status()\n        status_text = status_response.text\n        print(\"evaluating status\")\n        if \"Status=WAITING\" in status_text:\n            print(f\"{question_uuid} results not ready, waiting...\")\n            time.sleep(15)\n        elif \"Status=FAILED\" in status_text:\n            msg = \"BLAST query FAILED.\"\n            raise RuntimeError(msg)\n        elif \"Status=UNKNOWN\" in status_text:\n            msg = \"BLAST query expired or does not exist.\"\n            raise RuntimeError(msg)\n        elif \"Status=READY\" in status_text:\n            if \"ThereAreHits=yes\" in status_text:\n                print(f\"{question_uuid} results are ready, retrieving.\")\n                results_response = requests.get(\n                    base_url,\n                    params=get_results_params,\n                    timeout=10,\n                )\n                results_response.raise_for_status()\n                return results_response.text\n            return \"No hits found\"\n        if attempt == retries - 1:\n            msg = \"Maximum attempts reached. Results may not be ready.\"\n            raise TimeoutError(msg)\n    return None\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastFetcher._submit_query","title":"<code>_submit_query(request_data)</code>","text":"<p>POST the BLAST query and retrieve the RID.</p> <p>The method submits the structured BlastQuery object and returns the RID.</p> <pre><code>request_data: BlastQuery object containing the BLAST query\n    parameters.\n</code></pre> <pre><code>str: The Request ID (RID) for the submitted BLAST query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>def _submit_query(self, request_data: BlastQueryParameters) -&gt; str:\n    \"\"\"POST the BLAST query and retrieve the RID.\n\n    The method submits the structured BlastQuery object and returns the RID.\n\n    Args:\n    ----\n        request_data: BlastQuery object containing the BLAST query\n            parameters.\n\n    Returns:\n    -------\n        str: The Request ID (RID) for the submitted BLAST query.\n\n    \"\"\"\n    data = {\n        \"CMD\": request_data.cmd,\n        \"PROGRAM\": request_data.program,\n        \"DATABASE\": request_data.database,\n        \"QUERY\": request_data.query,\n        \"FORMAT_TYPE\": request_data.format_type,\n        \"MEGABLAST\": request_data.megablast,\n        \"HITLIST_SIZE\": request_data.max_hits,\n    }\n    # Include any other_params if provided\n    if request_data.other_params:\n        data.update(request_data.other_params)\n    # Make the API call\n    query_string = urlencode(data)\n    # Combine base URL with the query string\n    full_url = f\"{request_data.url}?{query_string}\"\n    # Print the full URL\n    request_data.full_url = full_url\n    print(\"Full URL built by retriever:\\n\", request_data.full_url)\n    response = requests.post(request_data.url, data=data, timeout=10)\n    response.raise_for_status()\n    # Extract RID from response\n    print(response)\n    match = re.search(r\"RID = (\\w+)\", response.text)\n    if match:\n        return match.group(1)\n\n    msg = \"RID not found in BLAST submission response.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastFetcher.fetch_results","title":"<code>fetch_results(query_models, retries=20)</code>","text":"<p>Submit request and fetch results from BLAST API.</p> <p>Wraps individual submission and retrieval of results.</p> <pre><code>query_models: list of Pydantic models of the queries\nretries: the number of maximum retries\n</code></pre> <pre><code>str: the result from the BLAST API\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>def fetch_results(\n    self,\n    query_models: list[BlastQueryParameters],\n    retries: int = 20,\n) -&gt; str:\n    \"\"\"Submit request and fetch results from BLAST API.\n\n    Wraps individual submission and retrieval of results.\n\n    Args:\n    ----\n        query_models: list of Pydantic models of the queries\n        retries: the number of maximum retries\n\n    Returns:\n    -------\n        str: the result from the BLAST API\n\n    \"\"\"\n    # For now, we only use the first query in the list\n    query = query_models[0]\n    rid = self._submit_query(request_data=query)\n    return self._fetch_results(\n        rid=rid,\n        question_uuid=query.question_uuid,\n        retries=retries,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastInterpreter","title":"<code>BlastInterpreter</code>","text":"<p>               Bases: <code>BaseInterpreter</code></p> <p>A class for interpreting BLAST results.</p> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>class BlastInterpreter(BaseInterpreter):\n    \"\"\"A class for interpreting BLAST results.\"\"\"\n\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Extract the answer from the BLAST results.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n            conversation_factory: A BioChatter conversation object.\n            response_text (str): The response.text returned by NCBI.\n\n        Returns:\n        -------\n            str: The extracted answer from the BLAST results.\n\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class molecular biologist who knows everything about NCBI and BLAST results.\",\n                ),\n                (\"user\", \"{input}\"),\n            ],\n        )\n        summary_prompt = BLAST_SUMMARY_PROMPT.format(\n            question=question,\n            context=response_text,\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>","text":"<p>Extract the answer from the BLAST results.</p> <pre><code>question (str): The question to be answered.\nconversation_factory: A BioChatter conversation object.\nresponse_text (str): The response.text returned by NCBI.\n</code></pre> <pre><code>str: The extracted answer from the BLAST results.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Extract the answer from the BLAST results.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n        conversation_factory: A BioChatter conversation object.\n        response_text (str): The response.text returned by NCBI.\n\n    Returns:\n    -------\n        str: The extracted answer from the BLAST results.\n\n    \"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a world class molecular biologist who knows everything about NCBI and BLAST results.\",\n            ),\n            (\"user\", \"{input}\"),\n        ],\n    )\n    summary_prompt = BLAST_SUMMARY_PROMPT.format(\n        question=question,\n        context=response_text,\n    )\n    output_parser = StrOutputParser()\n    conversation = conversation_factory()\n    chain = prompt | conversation.chat | output_parser\n    return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryBuilder","title":"<code>BlastQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a BlastQuery object.</p> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>class BlastQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a BlastQuery object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: \"BlastQueryParameters\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Creates a runnable using the LangChain\n        `create_structured_output_runnable` method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[BlastQueryParameters]:\n        \"\"\"Generate a BlastQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                BlastQuery.\n\n        Returns:\n        -------\n            BlastQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        runnable = self.create_runnable(\n            query_parameters=BlastQueryParameters,\n            conversation=conversation,\n        )\n        blast_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {BLAST_QUERY_PROMPT}\"},\n        )\n        blast_call_obj.question_uuid = str(uuid.uuid4())\n        return [blast_call_obj]\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Creates a runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: \"BlastQueryParameters\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Creates a runnable using the LangChain\n    `create_structured_output_runnable` method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    return create_structured_output_runnable(\n        output_schema=query_parameters,\n        llm=conversation.chat,\n        prompt=self.structured_output_prompt,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a BlastQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    BlastQuery.\n</code></pre> <pre><code>BlastQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[BlastQueryParameters]:\n    \"\"\"Generate a BlastQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            BlastQuery.\n\n    Returns:\n    -------\n        BlastQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    runnable = self.create_runnable(\n        query_parameters=BlastQueryParameters,\n        conversation=conversation,\n    )\n    blast_call_obj = runnable.invoke(\n        {\"input\": f\"Answer:\\n{question} based on:\\n {BLAST_QUERY_PROMPT}\"},\n    )\n    blast_call_obj.question_uuid = str(uuid.uuid4())\n    return [blast_call_obj]\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryParameters","title":"<code>BlastQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for the parameters of a BLAST query request.</p> <p>The class is used for configuring and sending a request to the NCBI BLAST query API. The fields are dynamically configured by the LLM based on the user's question.</p> Source code in <code>biochatter/biochatter/api_agent/web/blast.py</code> <pre><code>class BlastQueryParameters(BaseModel):\n    \"\"\"Pydantic model for the parameters of a BLAST query request.\n\n    The class is used for configuring and sending a request to the NCBI BLAST\n    query API. The fields are dynamically configured by the LLM based on the\n    user's question.\n\n    \"\"\"\n\n    url: str | None = Field(\n        default=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?\",\n        description=\"ALWAYS USE DEFAULT, DO NOT CHANGE\",\n    )\n    cmd: str | None = Field(\n        default=\"Put\",\n        description=\"Command to execute, 'Put' for submitting query, 'Get' for retrieving results.\",\n    )\n    program: str | None = Field(\n        default=\"blastn\",\n        description=(\n            \"BLAST program to use, e.g., 'blastn' for nucleotide-nucleotide BLAST, \"\n            \"'blastp' for protein-protein BLAST.\"\n        ),\n    )\n    database: str | None = Field(\n        default=\"nt\",\n        description=(\n            \"Database to search, e.g., 'nt' for nucleotide database, 'nr' for \"\n            \"non redundant protein database, 'pdb' the Protein Data Bank \"\n            \"database, which is used specifically for protein structures, \"\n            \"'refseq_rna' and 'refseq_genomic': specialized databases for \"\n            \"RNA sequences and genomic sequences\"\n        ),\n    )\n    query: str | None = Field(\n        None,\n        description=(\n            \"Nucleotide or protein sequence for the BLAST or blat query, \"\n            \"make sure to always keep the entire sequence given.\"\n        ),\n    )\n    format_type: str | None = Field(\n        default=\"Text\",\n        description=\"Format of the BLAST results, e.g., 'Text', 'XML'.\",\n    )\n    rid: str | None = Field(\n        None,\n        description=\"Request ID for retrieving BLAST results.\",\n    )\n    other_params: dict | None = Field(\n        default={\"email\": \"user@example.com\"},\n        description=\"Other optional BLAST parameters, including user email.\",\n    )\n    max_hits: int | None = Field(\n        default=15,\n        description=\"Maximum number of hits to return in the BLAST results.\",\n    )\n    sort_by: str | None = Field(\n        default=\"score\",\n        description=\"Criterion to sort BLAST results by, e.g., 'score', 'evalue'.\",\n    )\n    megablast: str | None = Field(\n        default=\"on\",\n        description=\"Set to 'on' for human genome alignemnts\",\n    )\n    question_uuid: str | None = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n    full_url: str | None = Field(\n        default=\"TBF\",\n        description=\"Full URL to be used to submit the BLAST query\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#oncokb","title":"OncoKB","text":"<p>OncoKB API agent.</p>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBFetcher","title":"<code>OncoKBFetcher</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>A class for retrieving API results.</p> <p>Retrieve from OncoKB given a parameterized OncoKBQuery.</p> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>class OncoKBFetcher(BaseFetcher):\n    \"\"\"A class for retrieving API results.\n\n    Retrieve from OncoKB given a parameterized OncoKBQuery.\n    \"\"\"\n\n    def __init__(self, api_token=\"demo\"):\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        self.base_url = \"https://demo.oncokb.org/api/v1\"\n\n    def fetch_results(\n        self,\n        request_data: list[OncoKBQueryParameters],\n        retries: int | None = 3,\n    ) -&gt; str:\n        \"\"\"Submit the OncoKB query and fetch the results directly.\n\n        No multi-step procedure, thus no wrapping of submission and retrieval in\n        this case.\n\n        Args:\n        ----\n            request_data: List of OncoKBQuery objects (Pydantic models)\n                containing the OncoKB query parameters.\n\n            retries: The number of retries to fetch the results.\n\n        Returns:\n        -------\n            str: The results of the OncoKB query.\n\n        \"\"\"\n        # For now, we only use the first query in the list\n        query = request_data[0]\n\n        # Submit the query and get the URL\n        params = query.dict(exclude_unset=True)\n        endpoint = params.pop(\"endpoint\")\n        params.pop(\"question_uuid\")\n        full_url = f\"{self.base_url}/{endpoint}\"\n        response = requests.get(full_url, headers=self.headers, params=params)\n        response.raise_for_status()\n\n        # Fetch the results from the URL\n        results_response = requests.get(response.url, headers=self.headers)\n        results_response.raise_for_status()\n\n        return results_response.text\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBFetcher.fetch_results","title":"<code>fetch_results(request_data, retries=3)</code>","text":"<p>Submit the OncoKB query and fetch the results directly.</p> <p>No multi-step procedure, thus no wrapping of submission and retrieval in this case.</p> <pre><code>request_data: List of OncoKBQuery objects (Pydantic models)\n    containing the OncoKB query parameters.\n\nretries: The number of retries to fetch the results.\n</code></pre> <pre><code>str: The results of the OncoKB query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>def fetch_results(\n    self,\n    request_data: list[OncoKBQueryParameters],\n    retries: int | None = 3,\n) -&gt; str:\n    \"\"\"Submit the OncoKB query and fetch the results directly.\n\n    No multi-step procedure, thus no wrapping of submission and retrieval in\n    this case.\n\n    Args:\n    ----\n        request_data: List of OncoKBQuery objects (Pydantic models)\n            containing the OncoKB query parameters.\n\n        retries: The number of retries to fetch the results.\n\n    Returns:\n    -------\n        str: The results of the OncoKB query.\n\n    \"\"\"\n    # For now, we only use the first query in the list\n    query = request_data[0]\n\n    # Submit the query and get the URL\n    params = query.dict(exclude_unset=True)\n    endpoint = params.pop(\"endpoint\")\n    params.pop(\"question_uuid\")\n    full_url = f\"{self.base_url}/{endpoint}\"\n    response = requests.get(full_url, headers=self.headers, params=params)\n    response.raise_for_status()\n\n    # Fetch the results from the URL\n    results_response = requests.get(response.url, headers=self.headers)\n    results_response.raise_for_status()\n\n    return results_response.text\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBInterpreter","title":"<code>OncoKBInterpreter</code>","text":"<p>               Bases: <code>BaseInterpreter</code></p> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>class OncoKBInterpreter(BaseInterpreter):\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Extract the answer from the BLAST results.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n            conversation_factory: A BioChatter conversation object.\n            response_text (str): The response.text returned by OncoKB.\n\n        Returns:\n        -------\n            str: The extracted answer from the BLAST results.\n\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class molecular biologist who knows \"\n                    \"everything about OncoKB and cancer genomics. Your task is \"\n                    \"to interpret results from OncoKB API calls and summarise \"\n                    \"them for the user.\",\n                ),\n                (\"user\", \"{input}\"),\n            ],\n        )\n        summary_prompt = ONCOKB_SUMMARY_PROMPT.format(\n            question=question,\n            context=response_text,\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        answer = chain.invoke({\"input\": {summary_prompt}})\n        return answer\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>","text":"<p>Extract the answer from the BLAST results.</p> <pre><code>question (str): The question to be answered.\nconversation_factory: A BioChatter conversation object.\nresponse_text (str): The response.text returned by OncoKB.\n</code></pre> <pre><code>str: The extracted answer from the BLAST results.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Extract the answer from the BLAST results.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n        conversation_factory: A BioChatter conversation object.\n        response_text (str): The response.text returned by OncoKB.\n\n    Returns:\n    -------\n        str: The extracted answer from the BLAST results.\n\n    \"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a world class molecular biologist who knows \"\n                \"everything about OncoKB and cancer genomics. Your task is \"\n                \"to interpret results from OncoKB API calls and summarise \"\n                \"them for the user.\",\n            ),\n            (\"user\", \"{input}\"),\n        ],\n    )\n    summary_prompt = ONCOKB_SUMMARY_PROMPT.format(\n        question=question,\n        context=response_text,\n    )\n    output_parser = StrOutputParser()\n    conversation = conversation_factory()\n    chain = prompt | conversation.chat | output_parser\n    answer = chain.invoke({\"input\": {summary_prompt}})\n    return answer\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBQueryBuilder","title":"<code>OncoKBQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building an OncoKBQuery object.</p> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>class OncoKBQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building an OncoKBQuery object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: \"OncoKBQueryParameters\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Creates a runnable object for executing queries using the LangChain\n        `create_structured_output_runnable` method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[OncoKBQueryParameters]:\n        \"\"\"Generate an OncoKBQuery object.\n\n        Generate based on the given question, prompt, and BioChatter\n        conversation. Uses a Pydantic model to define the API fields. Creates a\n        runnable that can be invoked on LLMs that are qualified to parameterise\n        functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                OncoKBQuery.\n\n        Returns:\n        -------\n            OncoKBQueryParameters: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        runnable = self.create_runnable(\n            query_parameters=OncoKBQueryParameters,\n            conversation=conversation,\n        )\n        oncokb_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {ONCOKB_QUERY_PROMPT}\"},\n        )\n        oncokb_call_obj.question_uuid = str(uuid.uuid4())\n        return [oncokb_call_obj]\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Creates a runnable object for executing queries using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: \"OncoKBQueryParameters\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Creates a runnable object for executing queries using the LangChain\n    `create_structured_output_runnable` method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    return create_structured_output_runnable(\n        output_schema=query_parameters,\n        llm=conversation.chat,\n        prompt=self.structured_output_prompt,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate an OncoKBQuery object.</p> <p>Generate based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    OncoKBQuery.\n</code></pre> <pre><code>OncoKBQueryParameters: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/oncokb.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[OncoKBQueryParameters]:\n    \"\"\"Generate an OncoKBQuery object.\n\n    Generate based on the given question, prompt, and BioChatter\n    conversation. Uses a Pydantic model to define the API fields. Creates a\n    runnable that can be invoked on LLMs that are qualified to parameterise\n    functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            OncoKBQuery.\n\n    Returns:\n    -------\n        OncoKBQueryParameters: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    runnable = self.create_runnable(\n        query_parameters=OncoKBQueryParameters,\n        conversation=conversation,\n    )\n    oncokb_call_obj = runnable.invoke(\n        {\"input\": f\"Answer:\\n{question} based on:\\n {ONCOKB_QUERY_PROMPT}\"},\n    )\n    oncokb_call_obj.question_uuid = str(uuid.uuid4())\n    return [oncokb_call_obj]\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biotools","title":"bio.tools","text":"<p>Module for interacting with the bio.tools API.</p>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsFetcher","title":"<code>BioToolsFetcher</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>A class for retrieving API results from BioTools.</p> <p>Retrieves API results given a parameterized BioToolsQuery.</p> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsFetcher(BaseFetcher):\n    \"\"\"A class for retrieving API results from BioTools.\n\n    Retrieves API results given a parameterized BioToolsQuery.\n    \"\"\"\n\n    def __init__(self, api_token: str = \"demo\") -&gt; None:  # noqa: S107\n        \"\"\"Initialise the BioToolsFetcher.\n\n        Args:\n        ----\n            api_token: The API token for the BioTools API.\n\n        \"\"\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        self.base_url = \"https://bio.tools/api\"\n\n    def fetch_results(\n        self,\n        request_data: list[BioToolsQueryParameters],\n        retries: int | None = 3,  # noqa: ARG002\n    ) -&gt; str:\n        \"\"\"Submit the BioTools query and fetch the results directly.\n\n        No multi-step procedure, thus no wrapping of submission and retrieval in\n        this case.\n\n        Args:\n        ----\n            request_data: List of BioToolsQuery objects (Pydantic models)\n                containing the BioTools query parameters.\n\n            retries: The number of retries to fetch the results.\n\n        Returns:\n        -------\n            str: The results of the BioTools query.\n\n        \"\"\"\n        # For now, we only use the first query in the list\n        query = request_data[0]\n\n        # Submit the query and get the URL\n        params = query.dict(exclude_unset=True)\n        endpoint = params.pop(\"endpoint\")\n        params.pop(\"question_uuid\")\n        full_url = f\"{self.base_url}/{endpoint}\"\n        response = requests.get(full_url, headers=self.headers, params=params, timeout=30)\n        response.raise_for_status()\n\n        # Fetch the results from the URL\n        results_response = requests.get(response.url, headers=self.headers, timeout=30)\n        results_response.raise_for_status()\n\n        return results_response.text\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsFetcher.__init__","title":"<code>__init__(api_token='demo')</code>","text":"<p>Initialise the BioToolsFetcher.</p> <pre><code>api_token: The API token for the BioTools API.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>def __init__(self, api_token: str = \"demo\") -&gt; None:  # noqa: S107\n    \"\"\"Initialise the BioToolsFetcher.\n\n    Args:\n    ----\n        api_token: The API token for the BioTools API.\n\n    \"\"\"\n    self.headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Accept\": \"application/json\",\n    }\n    self.base_url = \"https://bio.tools/api\"\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsFetcher.fetch_results","title":"<code>fetch_results(request_data, retries=3)</code>","text":"<p>Submit the BioTools query and fetch the results directly.</p> <p>No multi-step procedure, thus no wrapping of submission and retrieval in this case.</p> <pre><code>request_data: List of BioToolsQuery objects (Pydantic models)\n    containing the BioTools query parameters.\n\nretries: The number of retries to fetch the results.\n</code></pre> <pre><code>str: The results of the BioTools query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>def fetch_results(\n    self,\n    request_data: list[BioToolsQueryParameters],\n    retries: int | None = 3,  # noqa: ARG002\n) -&gt; str:\n    \"\"\"Submit the BioTools query and fetch the results directly.\n\n    No multi-step procedure, thus no wrapping of submission and retrieval in\n    this case.\n\n    Args:\n    ----\n        request_data: List of BioToolsQuery objects (Pydantic models)\n            containing the BioTools query parameters.\n\n        retries: The number of retries to fetch the results.\n\n    Returns:\n    -------\n        str: The results of the BioTools query.\n\n    \"\"\"\n    # For now, we only use the first query in the list\n    query = request_data[0]\n\n    # Submit the query and get the URL\n    params = query.dict(exclude_unset=True)\n    endpoint = params.pop(\"endpoint\")\n    params.pop(\"question_uuid\")\n    full_url = f\"{self.base_url}/{endpoint}\"\n    response = requests.get(full_url, headers=self.headers, params=params, timeout=30)\n    response.raise_for_status()\n\n    # Fetch the results from the URL\n    results_response = requests.get(response.url, headers=self.headers, timeout=30)\n    results_response.raise_for_status()\n\n    return results_response.text\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsInterpreter","title":"<code>BioToolsInterpreter</code>","text":"<p>               Bases: <code>BaseInterpreter</code></p> <p>A class for interpreting BioTools results.</p> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsInterpreter(BaseInterpreter):\n    \"\"\"A class for interpreting BioTools results.\"\"\"\n\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Extract the answer from the BLAST results.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n            conversation_factory: A BioChatter conversation object.\n            response_text (str): The response.text returned by bio.tools.\n\n        Returns:\n        -------\n            str: The extracted answer from the BLAST results.\n\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class bioinformatician who knows \"\n                    \"everything about bio.tools packages and the \"\n                    \"bioinformatics ecosystem. Your task is to interpret \"\n                    \"results from BioTools API calls and summarise \"\n                    \"them for the user.\",\n                ),\n                (\"user\", \"{input}\"),\n            ],\n        )\n        summary_prompt = BIOTOOLS_SUMMARY_PROMPT.format(\n            question=question,\n            context=response_text,\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>","text":"<p>Extract the answer from the BLAST results.</p> <pre><code>question (str): The question to be answered.\nconversation_factory: A BioChatter conversation object.\nresponse_text (str): The response.text returned by bio.tools.\n</code></pre> <pre><code>str: The extracted answer from the BLAST results.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Extract the answer from the BLAST results.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n        conversation_factory: A BioChatter conversation object.\n        response_text (str): The response.text returned by bio.tools.\n\n    Returns:\n    -------\n        str: The extracted answer from the BLAST results.\n\n    \"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a world class bioinformatician who knows \"\n                \"everything about bio.tools packages and the \"\n                \"bioinformatics ecosystem. Your task is to interpret \"\n                \"results from BioTools API calls and summarise \"\n                \"them for the user.\",\n            ),\n            (\"user\", \"{input}\"),\n        ],\n    )\n    summary_prompt = BIOTOOLS_SUMMARY_PROMPT.format(\n        question=question,\n        context=response_text,\n    )\n    output_parser = StrOutputParser()\n    conversation = conversation_factory()\n    chain = prompt | conversation.chat | output_parser\n    return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryBuilder","title":"<code>BioToolsQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building an BioToolsQuery object.</p> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building an BioToolsQuery object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: \"BioToolsQueryParameters\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[BioToolsQueryParameters]:\n        \"\"\"Generate an BioToolsQuery object.\n\n        Generate a BioToolsQuery object based on the given question, prompt,\n        and BioChatter conversation. Uses a Pydantic model to define the API\n        fields.  Creates a runnable that can be invoked on LLMs that are\n        qualified to parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                BioToolsQuery.\n\n        Returns:\n        -------\n            BioToolsQueryParameters: the parameterised query object (Pydantic\n                model)\n\n        \"\"\"\n        runnable = self.create_runnable(\n            query_parameters=BioToolsQueryParameters,\n            conversation=conversation,\n        )\n        oncokb_call_obj = runnable.invoke(\n            {\n                \"input\": f\"Answer:\\n{question} based on:\\n {BIOTOOLS_QUERY_PROMPT}\",\n            },\n        )\n        oncokb_call_obj.question_uuid = str(uuid.uuid4())\n        return [oncokb_call_obj]\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: \"BioToolsQueryParameters\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    return create_structured_output_runnable(\n        output_schema=query_parameters,\n        llm=conversation.chat,\n        prompt=self.structured_output_prompt,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate an BioToolsQuery object.</p> <p>Generate a BioToolsQuery object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields.  Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    BioToolsQuery.\n</code></pre> <pre><code>BioToolsQueryParameters: the parameterised query object (Pydantic\n    model)\n</code></pre> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[BioToolsQueryParameters]:\n    \"\"\"Generate an BioToolsQuery object.\n\n    Generate a BioToolsQuery object based on the given question, prompt,\n    and BioChatter conversation. Uses a Pydantic model to define the API\n    fields.  Creates a runnable that can be invoked on LLMs that are\n    qualified to parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            BioToolsQuery.\n\n    Returns:\n    -------\n        BioToolsQueryParameters: the parameterised query object (Pydantic\n            model)\n\n    \"\"\"\n    runnable = self.create_runnable(\n        query_parameters=BioToolsQueryParameters,\n        conversation=conversation,\n    )\n    oncokb_call_obj = runnable.invoke(\n        {\n            \"input\": f\"Answer:\\n{question} based on:\\n {BIOTOOLS_QUERY_PROMPT}\",\n        },\n    )\n    oncokb_call_obj.question_uuid = str(uuid.uuid4())\n    return [oncokb_call_obj]\n</code></pre>"},{"location":"BioChatter/api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryParameters","title":"<code>BioToolsQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the bio.tools API.</p> Source code in <code>biochatter/biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the bio.tools API.\"\"\"\n\n    base_url: str = Field(\n        default=\"https://bio.tools/api/\",\n        description=\"Base URL for the BioTools API.\",\n    )\n    endpoint: str = Field(\n        ...,\n        description=\"Specific API endpoint to hit. Example: 't/' for listing tools.\",\n    )\n    biotoolsID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Search for bio.tools tool ID (usually quoted - to get exact match)\",\n    )\n    name: str | None = Field(\n        None,\n        description=\"Search for tool name (quoted as needed: quoted for exact match, unquoted for fuzzy search)\",\n    )\n    homepage: str | None = Field(\n        None,\n        description=\"Exact search for tool homepage URL (**must** be quoted)\",\n    )\n    description: str | None = Field(\n        None,\n        description=\"Search over tool description (quoted as needed)\",\n    )\n    version: str | None = Field(\n        None,\n        description=\"Exact search for tool version (**must** be quoted)\",\n    )\n    topic: str | None = Field(\n        None,\n        description=\"Search for EDAM Topic (term) (quoted as needed)\",\n    )\n    topicID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for EDAM Topic (URI): **must** be quoted\",\n    )\n    function: str | None = Field(\n        None,\n        description=\"Fuzzy search over function (input, operation, output, note and command)\",\n    )\n    operation: str | None = Field(\n        None,\n        description=\"Fuzzy search for EDAM Operation (term) (quoted as needed)\",\n    )\n    operationID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for EDAM Operation (ID) (**must** be quoted)\",\n    )\n    dataType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input and output for EDAM Data (term) (quoted as needed)\",\n    )\n    dataTypeID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input and output for EDAM Data (ID) (**must** be quoted)\",\n    )\n    dataFormat: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input and output for EDAM Format (term) (quoted as needed)\",\n    )\n    dataFormatID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input and output for EDAM Format (ID) (**must** be quoted)\",\n    )\n    input: str | None = Field(\n        None,\n        description=\"Fuzzy search over input for EDAM Data and Format (term) (quoted as needed)\",\n    )\n    inputID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input for EDAM Data and Format (ID) (**must** be quoted)\",\n    )\n    inputDataType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input for EDAM Data (term) (quoted as needed)\",\n    )\n    inputDataTypeID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input for EDAM Data (ID) (**must** be quoted)\",\n    )\n    inputDataFormat: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input for EDAM Format (term) (quoted as needed)\",\n    )\n    inputDataFormatID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input for EDAM Format (ID) (**must** be quoted)\",\n    )\n    output: str | None = Field(\n        None,\n        description=\"Fuzzy search over output for EDAM Data and Format (term) (quoted as needed)\",\n    )\n    outputID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over output for EDAM Data and Format (ID) (**must** be quoted)\",\n    )\n    outputDataType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over output for EDAM Data (term) (quoted as needed)\",\n    )\n    outputDataTypeID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over output for EDAM Data (ID) (**must** be quoted)\",\n    )\n    outputDataFormat: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over output for EDAM Format (term) (quoted as needed)\",\n    )\n    outputDataFormatID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over output for EDAM Format (ID) (**must** be quoted)\",\n    )\n    toolType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool type\",\n    )\n    collectionID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool collection (normally quoted)\",\n    )\n    maturity: str | None = Field(\n        None,\n        description=\"Exact search for tool maturity\",\n    )\n    operatingSystem: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool operating system\",\n    )\n    language: str | None = Field(\n        None,\n        description=\"Exact search for programming language\",\n    )\n    cost: str | None = Field(\n        None,\n        description=\"Exact search for cost\",\n    )\n    license: str | None = Field(\n        None,\n        description=\"Exact search for software or data usage license (quoted as needed)\",\n    )\n    accessibility: str | None = Field(\n        None,\n        description=\"Exact search for tool accessibility\",\n    )\n    credit: str | None = Field(\n        None,\n        description=\"Fuzzy search over credit (name, email, URL, ORCID iD, type of entity, type of role and note)\",\n    )\n    creditName: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for name of credited entity\",\n    )\n    creditTypeRole: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for role of credited entity\",\n    )\n    creditTypeEntity: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of credited entity\",\n    )\n    creditOrcidID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for ORCID iD of credited entity (**must** be quoted)\",\n    )\n    publication: str | None = Field(\n        None,\n        description=(\n            \"Fuzzy search over publication (DOI, PMID, PMCID, publication type and tool version) (quoted as needed)\"\n        ),\n    )\n    publicationID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for publication ID (DOI, PMID or PMCID) (**must** be quoted)\",\n    )\n    publicationType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for publication type\",\n    )\n    publicationVersion: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool version associated with a publication (**must** be quoted)\",\n    )\n    link: str | None = Field(\n        None,\n        description=\"Fuzzy search over general link (URL, type and note) (quote as needed)\",\n    )\n    linkType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of information found at a link\",\n    )\n    documentation: str | None = Field(\n        None,\n        description=\"Fuzzy search over documentation link (URL, type and note) (quote as needed)\",\n    )\n    documentationType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of documentation\",\n    )\n    download: str | None = Field(\n        None,\n        description=\"Fuzzy search over download link (URL, type, version and note) (quote as needed)\",\n    )\n    downloadType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of download\",\n    )\n    downloadVersion: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool version associated with a download (**must** be quoted)\",\n    )\n    otherID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over alternate tool IDs (ID value, type of ID and version)\",\n    )\n    otherIDValue: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for value of alternate tool ID (**must** be quoted)\",\n    )\n    otherIDType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of alternate tool ID\",\n    )\n    otherIDVersion: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool version associated with an alternate ID (**must** be quoted)\",\n    )\n    question_uuid: str | None = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/kg/","title":"Knowledge Graph Agent Reference","text":"<p>Here we handle generation of use case-specific database prompts and their execution against a database using the database agent.</p>"},{"location":"BioChatter/api-docs/kg/#dynamic-prompt-generation-for-biocypher-knowledge-graphs","title":"Dynamic prompt generation for BioCypher knowledge graphs","text":""},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine","title":"<code>BioCypherPromptEngine</code>","text":"Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>class BioCypherPromptEngine:\n    def __init__(\n        self,\n        schema_config_or_info_path: str | None = None,\n        schema_config_or_info_dict: dict | None = None,\n        model_name: str = \"gpt-3.5-turbo\",\n        conversation_factory: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Given a biocypher schema configuration, extract the entities and\n        relationships, and for each extract their mode of representation (node\n        or edge), properties, and identifier namespace. Using these data, allow\n        the generation of prompts for a large language model, informing it of\n        the schema constituents and their properties, to enable the\n        parameterisation of function calls to a knowledge graph.\n\n        Args:\n        ----\n            schema_config_or_info_path: Path to a biocypher schema configuration\n                file or the extended schema information output generated by\n                BioCypher's `write_schema_info` function (preferred).\n\n            schema_config_or_info_dict: A dictionary containing the schema\n                configuration file or the extended schema information output\n                generated by BioCypher's `write_schema_info` function\n                (preferred).\n\n            model_name: The name of the model to use for the conversation.\n                DEPRECATED: This should now be set in the conversation factory.\n\n            conversation_factory: A function used to create a conversation for\n                creating the KG query. If not provided, a default function is\n                used (creating an OpenAI conversation with the specified model,\n                see `_get_conversation`).\n\n        \"\"\"\n        if not schema_config_or_info_path and not schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a path to a file or as a dictionary.\",\n            )\n\n        if schema_config_or_info_path and schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary, not both.\",\n            )\n\n        # set conversation factory or use default\n        self.conversation_factory = conversation_factory if conversation_factory is not None else self._get_conversation\n\n        if schema_config_or_info_path:\n            # read the schema configuration\n            with open(schema_config_or_info_path) as f:\n                schema_config = yaml.safe_load(f)\n        elif schema_config_or_info_dict:\n            schema_config = schema_config_or_info_dict\n\n        # check whether it is the original schema config or the output of\n        # biocypher info\n        is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n        # extract the entities and relationships: each top level key that has\n        # a 'represented_as' key\n        self.entities = {}\n        self.relationships = {}\n        if not is_schema_info:\n            for key, value in schema_config.items():\n                # hacky, better with biocypher output\n                name_indicates_relationship = \"interaction\" in key.lower() or \"association\" in key.lower()\n                if \"represented_as\" in value:\n                    if value[\"represented_as\"] == \"node\" and not name_indicates_relationship:\n                        self.entities[sentencecase_to_pascalcase(key)] = value\n                    elif (value[\"represented_as\"] == \"node\" and name_indicates_relationship) or value[\n                        \"represented_as\"\n                    ] == \"edge\":\n                        self.relationships[sentencecase_to_pascalcase(key)] = value\n        else:\n            for key, value in schema_config.items():\n                if not isinstance(value, dict):\n                    continue\n                if value.get(\"present_in_knowledge_graph\", None) == False:\n                    continue\n                if value.get(\"is_relationship\", None) == False:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif value.get(\"is_relationship\", None) == True:\n                    value = self._capitalise_source_and_target(value)\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n\n        self.question = \"\"\n        self.selected_entities = []\n        self.selected_relationships = []  # used in property selection\n        self.selected_relationship_labels = {}  # copy to deal with labels that\n        # are not the same as the relationship name, used in query generation\n        # dictionary to also include source and target types\n        self.rel_directions = {}\n        self.model_name = model_name\n\n    def _capitalise_source_and_target(self, relationship: dict) -&gt; dict:\n        \"\"\"Make sources and targets PascalCase to match the entities. Sources and\n        targets can be strings or lists of strings.\n        \"\"\"\n        if \"source\" in relationship:\n            if isinstance(relationship[\"source\"], str):\n                relationship[\"source\"] = sentencecase_to_pascalcase(\n                    relationship[\"source\"],\n                )\n            elif isinstance(relationship[\"source\"], list):\n                relationship[\"source\"] = [sentencecase_to_pascalcase(s) for s in relationship[\"source\"]]\n        if \"target\" in relationship:\n            if isinstance(relationship[\"target\"], str):\n                relationship[\"target\"] = sentencecase_to_pascalcase(\n                    relationship[\"target\"],\n                )\n            elif isinstance(relationship[\"target\"], list):\n                relationship[\"target\"] = [sentencecase_to_pascalcase(t) for t in relationship[\"target\"]]\n        return relationship\n\n    def _select_graph_entities_from_question(\n        self,\n        question: str,\n        conversation: Conversation,\n    ) -&gt; str:\n        conversation.reset()\n        success1 = self._select_entities(\n            question=question,\n            conversation=conversation,\n        )\n        if not success1:\n            raise ValueError(\n                \"Entity selection failed. Please try again with a different question.\",\n            )\n        conversation.reset()\n        success2 = self._select_relationships(conversation=conversation)\n        if not success2:\n            raise ValueError(\n                \"Relationship selection failed. Please try again with a different question.\",\n            )\n        conversation.reset()\n        success3 = self._select_properties(conversation=conversation)\n        if not success3:\n            raise ValueError(\n                \"Property selection failed. Please try again with a different question.\",\n            )\n\n    def _generate_query_prompt(\n        self,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str | None = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"Generate a prompt for a large language model to generate a database\n        query based on the selected entities, relationships, and properties.\n\n        Args:\n        ----\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n        -------\n            A prompt for a large language model to generate a database query.\n\n        \"\"\"\n        msg = (\n            f\"Generate a database query in {query_language} that answers \"\n            f\"the user's question. \"\n            f\"You can use the following entities: {entities}, \"\n            f\"relationships: {list(relationships.keys())}, and \"\n            f\"properties: {properties}. \"\n        )\n\n        for relationship, values in relationships.items():\n            self._expand_pairs(relationship, values)\n\n        if self.rel_directions:\n            msg += \"Given the following valid combinations of source, relationship, and target: \"\n            for key, value in self.rel_directions.items():\n                for pair in value:\n                    msg += f\"'(:{pair[0]})-(:{key})-&gt;(:{pair[1]})', \"\n            msg += f\"generate a {query_language} query using one of these combinations. \"\n\n        msg += \"Only return the query, without any additional text, symbols or characters --- just the query statement.\"\n        return msg\n\n    def generate_query_prompt(\n        self,\n        question: str,\n        query_language: str | None = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"Generate a prompt for a large language model to generate a database\n        query based on the user's question and class attributes informing about\n        the schema.\n\n        Args:\n        ----\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n        -------\n            A prompt for a large language model to generate a database query.\n\n        \"\"\"\n        self._select_graph_entities_from_question(\n            question,\n            self.conversation_factory(),\n        )\n        msg = self._generate_query_prompt(\n            self.selected_entities,\n            self.selected_relationship_labels,\n            self.selected_properties,\n            query_language,\n        )\n        return msg\n\n    def generate_query(\n        self,\n        question: str,\n        query_language: str | None = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"Wrap entity and property selection and query generation; return the\n        generated query.\n\n        Args:\n        ----\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n        -------\n            A database query that could answer the user's question.\n\n        \"\"\"\n        self._select_graph_entities_from_question(\n            question,\n            self.conversation_factory(),\n        )\n\n        return self._generate_query(\n            question=question,\n            entities=self.selected_entities,\n            relationships=self.selected_relationship_labels,\n            properties=self.selected_properties,\n            query_language=query_language,\n            conversation=self.conversation_factory(),\n        )\n\n    def _get_conversation(\n        self,\n        model_name: str | None = None,\n    ) -&gt; \"Conversation\":\n        \"\"\"Create a conversation object given a model name.\n\n        Args:\n        ----\n            model_name: The name of the model to use for the conversation.\n\n        Returns:\n        -------\n            A BioChatter Conversation object for connecting to the LLM.\n\n        Todo:\n        ----\n            Genericise to models outside of OpenAI.\n\n        \"\"\"\n        conversation = GptConversation(\n            model_name=model_name or self.model_name,\n            prompts={},\n            correct=False,\n        )\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            user=\"test_user\",\n        )\n        return conversation\n\n    def _select_entities(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; bool:\n        \"\"\"Given a question, select the entities that are relevant to the question\n        and store them in `selected_entities` and `selected_relationships`. Use\n        LLM conversation to do this.\n\n        Args:\n        ----\n            question: A user's question.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n        -------\n            True if at least one entity was selected, False otherwise.\n\n        \"\"\"\n        self.question = question\n\n        conversation.append_system_message(\n            \"You have access to a knowledge graph that contains \"\n            f\"these entity types: {', '.join(self.entities)}. Your task is \"\n            \"to select the entity types that are relevant to the user's question \"\n            \"for subsequent use in a query. Only return the entity types, \"\n            \"comma-separated, without any additional text. Do not return \"\n            \"entity names, relationships, or properties.\",\n        )\n\n        msg, token_usage, correction = conversation.query(question)\n\n        result = msg.split(\",\") if msg else []\n        # TODO: do we go back and retry if no entities were selected? or ask for\n        # a reason? offer visual selection of entities and relationships by the\n        # user?\n\n        if result:\n            for entity in result:\n                entity = entity.strip()\n                if entity in self.entities:\n                    self.selected_entities.append(entity)\n\n        return bool(result)\n\n    def _select_relationships(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"Given a question and the preselected entities, select relationships for\n        the query.\n\n        Args:\n        ----\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n        -------\n            True if at least one relationship was selected, False otherwise.\n\n        Todo:\n        ----\n            Now we have the problem that we discard all relationships that do\n            not have a source and target, if at least one relationship has a\n            source and target. At least communicate this all-or-nothing\n            behaviour to the user.\n\n        \"\"\"\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity selection first.\",\n            )\n\n        if not self.selected_entities:\n            raise ValueError(\n                \"No entities found. Please run the entity selection step first.\",\n            )\n\n        rels = {}\n        source_and_target_present = False\n        for key, value in self.relationships.items():\n            if \"source\" in value and \"target\" in value:\n                # if source or target is a list, expand to single pairs\n                source = ensure_iterable(value[\"source\"])\n                target = ensure_iterable(value[\"target\"])\n                pairs = []\n                for s in source:\n                    for t in target:\n                        pairs.append(\n                            (\n                                sentencecase_to_pascalcase(s),\n                                sentencecase_to_pascalcase(t),\n                            ),\n                        )\n                rels[key] = pairs\n                source_and_target_present = True\n            else:\n                rels[key] = {}\n\n        # prioritise relationships that have source and target, and discard\n        # relationships that do not have both source and target, if at least one\n        # relationship has both source and target. keep relationships that have\n        # either source or target, if none of the relationships have both source\n        # and target.\n\n        if source_and_target_present:\n            # First, separate the relationships into two groups: those with both\n            # source and target in the selected entities, and those with either\n            # source or target but not both.\n\n            rels_with_both = {}\n            rels_with_either = {}\n            for key, value in rels.items():\n                for pair in value:\n                    if pair[0] in self.selected_entities:\n                        if pair[1] in self.selected_entities:\n                            rels_with_both[key] = value\n                        else:\n                            rels_with_either[key] = value\n                    elif pair[1] in self.selected_entities:\n                        rels_with_either[key] = value\n\n            # If there are any relationships with both source and target,\n            # discard the others.\n\n            if rels_with_both:\n                rels = rels_with_both\n            else:\n                rels = rels_with_either\n\n            selected_rels = []\n            for key, value in rels.items():\n                if not value:\n                    continue\n\n                for pair in value:\n                    if pair[0] in self.selected_entities or pair[1] in self.selected_entities:\n                        selected_rels.append((key, pair))\n\n            rels = json.dumps(selected_rels)\n        else:\n            rels = json.dumps(self.relationships)\n\n        msg = (\n            \"You have access to a knowledge graph that contains \"\n            f\"these entities: {', '.join(self.selected_entities)}. \"\n            \"Your task is to select the relationships that are relevant \"\n            \"to the user's question for subsequent use in a query. Only \"\n            \"return the relationships without their sources or targets, \"\n            \"comma-separated, and without any additional text. Here are the \"\n            \"possible relationships and their source and target entities: \"\n            f\"{rels}.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        res, token_usage, correction = conversation.query(self.question)\n\n        result = res.split(\",\") if msg else []\n\n        if result:\n            for relationship in result:\n                relationship = relationship.strip()\n                if relationship in self.relationships:\n                    self.selected_relationships.append(relationship)\n                    rel_dict = self.relationships[relationship]\n                    label = rel_dict.get(\"label_as_edge\", relationship)\n                    if \"source\" in rel_dict and \"target\" in rel_dict:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": rel_dict[\"source\"],\n                            \"target\": rel_dict[\"target\"],\n                        }\n                    else:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": None,\n                            \"target\": None,\n                        }\n\n        # if we selected relationships that have either source or target which\n        # is not in the selected entities, we add those entities to the selected\n        # entities.\n\n        if self.selected_relationship_labels:\n            for key, value in self.selected_relationship_labels.items():\n                sources = ensure_iterable(value[\"source\"])\n                targets = ensure_iterable(value[\"target\"])\n                for source in sources:\n                    if source is None:\n                        continue\n                    if source not in self.selected_entities:\n                        self.selected_entities.append(\n                            sentencecase_to_pascalcase(source),\n                        )\n                for target in targets:\n                    if target is None:\n                        continue\n                    if target not in self.selected_entities:\n                        self.selected_entities.append(\n                            sentencecase_to_pascalcase(target),\n                        )\n\n        return bool(result)\n\n    @staticmethod\n    def _validate_json_str(json_str: str):\n        json_str = json_str.strip()\n        if json_str.startswith(\"```json\"):\n            json_str = json_str[7:]\n        if json_str.endswith(\"```\"):\n            json_str = json_str[:-3]\n        return json_str.strip()\n\n    def _select_properties(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"Given a question (optionally provided, but in the standard use case\n        reused from the entity selection step) and the selected entities, select\n        the properties that are relevant to the question and store them in\n        the dictionary `selected_properties`.\n\n        Returns\n        -------\n            True if at least one property was selected, False otherwise.\n\n        \"\"\"\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity and relationship selection first.\",\n            )\n\n        if not self.selected_entities and not self.selected_relationships:\n            raise ValueError(\n                \"No entities or relationships provided, and none available \"\n                \"from entity selection step. Please provide \"\n                \"entities/relationships or run the entity selection \"\n                \"(`select_entities()`) step first.\",\n            )\n\n        e_props = {}\n        for entity in self.selected_entities:\n            if self.entities[entity].get(\"properties\"):\n                e_props[entity] = list(\n                    self.entities[entity][\"properties\"].keys(),\n                )\n\n        r_props = {}\n        for relationship in self.selected_relationships:\n            if self.relationships[relationship].get(\"properties\"):\n                r_props[relationship] = list(\n                    self.relationships[relationship][\"properties\"].keys(),\n                )\n\n        msg = (\n            \"You have access to a knowledge graph that contains entities and \"\n            \"relationships. They have the following properties. Entities:\"\n            f\"{e_props}, Relationships: {r_props}. \"\n            \"Your task is to select the properties that are relevant to the \"\n            \"user's question for subsequent use in a query. Only return the \"\n            \"entities and relationships with their relevant properties in compact \"\n            \"JSON format, without any additional text. Return the \"\n            \"entities/relationships as top-level dictionary keys, and their \"\n            \"properties as dictionary values. \"\n            \"Do not return properties that are not relevant to the question.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        msg, token_usage, correction = conversation.query(self.question)\n        msg = BioCypherPromptEngine._validate_json_str(msg)\n\n        try:\n            self.selected_properties = json.loads(msg) if msg else {}\n        except json.decoder.JSONDecodeError:\n            self.selected_properties = {}\n\n        return bool(self.selected_properties)\n\n    def _generate_query(\n        self,\n        question: str,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str,\n        conversation: \"Conversation\",\n    ) -&gt; str:\n        \"\"\"Generate a query in the specified query language that answers the user's\n        question.\n\n        Args:\n        ----\n            question: A user's question.\n\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n        -------\n            A database query that could answer the user's question.\n\n        \"\"\"\n        msg = self._generate_query_prompt(\n            entities,\n            relationships,\n            properties,\n            query_language,\n        )\n\n        conversation.append_system_message(msg)\n\n        out_msg, token_usage, correction = conversation.query(question)\n\n        return out_msg.strip()\n\n    def _expand_pairs(self, relationship, values) -&gt; None:\n        if not self.rel_directions.get(relationship):\n            self.rel_directions[relationship] = []\n        if isinstance(values[\"source\"], list):\n            for source in values[\"source\"]:\n                if isinstance(values[\"target\"], list):\n                    for target in values[\"target\"]:\n                        self.rel_directions[relationship].append(\n                            (source, target),\n                        )\n                else:\n                    self.rel_directions[relationship].append(\n                        (source, values[\"target\"]),\n                    )\n        elif isinstance(values[\"target\"], list):\n            for target in values[\"target\"]:\n                self.rel_directions[relationship].append(\n                    (values[\"source\"], target),\n                )\n        else:\n            self.rel_directions[relationship].append(\n                (values[\"source\"], values[\"target\"]),\n            )\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine.__init__","title":"<code>__init__(schema_config_or_info_path=None, schema_config_or_info_dict=None, model_name='gpt-3.5-turbo', conversation_factory=None)</code>","text":"<p>Given a biocypher schema configuration, extract the entities and relationships, and for each extract their mode of representation (node or edge), properties, and identifier namespace. Using these data, allow the generation of prompts for a large language model, informing it of the schema constituents and their properties, to enable the parameterisation of function calls to a knowledge graph.</p> <pre><code>schema_config_or_info_path: Path to a biocypher schema configuration\n    file or the extended schema information output generated by\n    BioCypher's `write_schema_info` function (preferred).\n\nschema_config_or_info_dict: A dictionary containing the schema\n    configuration file or the extended schema information output\n    generated by BioCypher's `write_schema_info` function\n    (preferred).\n\nmodel_name: The name of the model to use for the conversation.\n    DEPRECATED: This should now be set in the conversation factory.\n\nconversation_factory: A function used to create a conversation for\n    creating the KG query. If not provided, a default function is\n    used (creating an OpenAI conversation with the specified model,\n    see `_get_conversation`).\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def __init__(\n    self,\n    schema_config_or_info_path: str | None = None,\n    schema_config_or_info_dict: dict | None = None,\n    model_name: str = \"gpt-3.5-turbo\",\n    conversation_factory: Callable | None = None,\n) -&gt; None:\n    \"\"\"Given a biocypher schema configuration, extract the entities and\n    relationships, and for each extract their mode of representation (node\n    or edge), properties, and identifier namespace. Using these data, allow\n    the generation of prompts for a large language model, informing it of\n    the schema constituents and their properties, to enable the\n    parameterisation of function calls to a knowledge graph.\n\n    Args:\n    ----\n        schema_config_or_info_path: Path to a biocypher schema configuration\n            file or the extended schema information output generated by\n            BioCypher's `write_schema_info` function (preferred).\n\n        schema_config_or_info_dict: A dictionary containing the schema\n            configuration file or the extended schema information output\n            generated by BioCypher's `write_schema_info` function\n            (preferred).\n\n        model_name: The name of the model to use for the conversation.\n            DEPRECATED: This should now be set in the conversation factory.\n\n        conversation_factory: A function used to create a conversation for\n            creating the KG query. If not provided, a default function is\n            used (creating an OpenAI conversation with the specified model,\n            see `_get_conversation`).\n\n    \"\"\"\n    if not schema_config_or_info_path and not schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a path to a file or as a dictionary.\",\n        )\n\n    if schema_config_or_info_path and schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary, not both.\",\n        )\n\n    # set conversation factory or use default\n    self.conversation_factory = conversation_factory if conversation_factory is not None else self._get_conversation\n\n    if schema_config_or_info_path:\n        # read the schema configuration\n        with open(schema_config_or_info_path) as f:\n            schema_config = yaml.safe_load(f)\n    elif schema_config_or_info_dict:\n        schema_config = schema_config_or_info_dict\n\n    # check whether it is the original schema config or the output of\n    # biocypher info\n    is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n    # extract the entities and relationships: each top level key that has\n    # a 'represented_as' key\n    self.entities = {}\n    self.relationships = {}\n    if not is_schema_info:\n        for key, value in schema_config.items():\n            # hacky, better with biocypher output\n            name_indicates_relationship = \"interaction\" in key.lower() or \"association\" in key.lower()\n            if \"represented_as\" in value:\n                if value[\"represented_as\"] == \"node\" and not name_indicates_relationship:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif (value[\"represented_as\"] == \"node\" and name_indicates_relationship) or value[\n                    \"represented_as\"\n                ] == \"edge\":\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n    else:\n        for key, value in schema_config.items():\n            if not isinstance(value, dict):\n                continue\n            if value.get(\"present_in_knowledge_graph\", None) == False:\n                continue\n            if value.get(\"is_relationship\", None) == False:\n                self.entities[sentencecase_to_pascalcase(key)] = value\n            elif value.get(\"is_relationship\", None) == True:\n                value = self._capitalise_source_and_target(value)\n                self.relationships[sentencecase_to_pascalcase(key)] = value\n\n    self.question = \"\"\n    self.selected_entities = []\n    self.selected_relationships = []  # used in property selection\n    self.selected_relationship_labels = {}  # copy to deal with labels that\n    # are not the same as the relationship name, used in query generation\n    # dictionary to also include source and target types\n    self.rel_directions = {}\n    self.model_name = model_name\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._capitalise_source_and_target","title":"<code>_capitalise_source_and_target(relationship)</code>","text":"<p>Make sources and targets PascalCase to match the entities. Sources and targets can be strings or lists of strings.</p> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _capitalise_source_and_target(self, relationship: dict) -&gt; dict:\n    \"\"\"Make sources and targets PascalCase to match the entities. Sources and\n    targets can be strings or lists of strings.\n    \"\"\"\n    if \"source\" in relationship:\n        if isinstance(relationship[\"source\"], str):\n            relationship[\"source\"] = sentencecase_to_pascalcase(\n                relationship[\"source\"],\n            )\n        elif isinstance(relationship[\"source\"], list):\n            relationship[\"source\"] = [sentencecase_to_pascalcase(s) for s in relationship[\"source\"]]\n    if \"target\" in relationship:\n        if isinstance(relationship[\"target\"], str):\n            relationship[\"target\"] = sentencecase_to_pascalcase(\n                relationship[\"target\"],\n            )\n        elif isinstance(relationship[\"target\"], list):\n            relationship[\"target\"] = [sentencecase_to_pascalcase(t) for t in relationship[\"target\"]]\n    return relationship\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._generate_query","title":"<code>_generate_query(question, entities, relationships, properties, query_language, conversation)</code>","text":"<p>Generate a query in the specified query language that answers the user's question.</p> <pre><code>question: A user's question.\n\nentities: A list of entities that are relevant to the question.\n\nrelationships: A list of relationships that are relevant to the\n    question.\n\nproperties: A dictionary of properties that are relevant to the\n    question.\n\nquery_language: The language of the query to generate.\n\nconversation: A BioChatter Conversation object for connecting to the\n    LLM.\n</code></pre> <pre><code>A database query that could answer the user's question.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _generate_query(\n    self,\n    question: str,\n    entities: list,\n    relationships: dict,\n    properties: dict,\n    query_language: str,\n    conversation: \"Conversation\",\n) -&gt; str:\n    \"\"\"Generate a query in the specified query language that answers the user's\n    question.\n\n    Args:\n    ----\n        question: A user's question.\n\n        entities: A list of entities that are relevant to the question.\n\n        relationships: A list of relationships that are relevant to the\n            question.\n\n        properties: A dictionary of properties that are relevant to the\n            question.\n\n        query_language: The language of the query to generate.\n\n        conversation: A BioChatter Conversation object for connecting to the\n            LLM.\n\n    Returns:\n    -------\n        A database query that could answer the user's question.\n\n    \"\"\"\n    msg = self._generate_query_prompt(\n        entities,\n        relationships,\n        properties,\n        query_language,\n    )\n\n    conversation.append_system_message(msg)\n\n    out_msg, token_usage, correction = conversation.query(question)\n\n    return out_msg.strip()\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._generate_query_prompt","title":"<code>_generate_query_prompt(entities, relationships, properties, query_language='Cypher')</code>","text":"<p>Generate a prompt for a large language model to generate a database query based on the selected entities, relationships, and properties.</p> <pre><code>entities: A list of entities that are relevant to the question.\n\nrelationships: A list of relationships that are relevant to the\n    question.\n\nproperties: A dictionary of properties that are relevant to the\n    question.\n\nquery_language: The language of the query to generate.\n</code></pre> <pre><code>A prompt for a large language model to generate a database query.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _generate_query_prompt(\n    self,\n    entities: list,\n    relationships: dict,\n    properties: dict,\n    query_language: str | None = \"Cypher\",\n) -&gt; str:\n    \"\"\"Generate a prompt for a large language model to generate a database\n    query based on the selected entities, relationships, and properties.\n\n    Args:\n    ----\n        entities: A list of entities that are relevant to the question.\n\n        relationships: A list of relationships that are relevant to the\n            question.\n\n        properties: A dictionary of properties that are relevant to the\n            question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n    -------\n        A prompt for a large language model to generate a database query.\n\n    \"\"\"\n    msg = (\n        f\"Generate a database query in {query_language} that answers \"\n        f\"the user's question. \"\n        f\"You can use the following entities: {entities}, \"\n        f\"relationships: {list(relationships.keys())}, and \"\n        f\"properties: {properties}. \"\n    )\n\n    for relationship, values in relationships.items():\n        self._expand_pairs(relationship, values)\n\n    if self.rel_directions:\n        msg += \"Given the following valid combinations of source, relationship, and target: \"\n        for key, value in self.rel_directions.items():\n            for pair in value:\n                msg += f\"'(:{pair[0]})-(:{key})-&gt;(:{pair[1]})', \"\n        msg += f\"generate a {query_language} query using one of these combinations. \"\n\n    msg += \"Only return the query, without any additional text, symbols or characters --- just the query statement.\"\n    return msg\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._get_conversation","title":"<code>_get_conversation(model_name=None)</code>","text":"<p>Create a conversation object given a model name.</p> <pre><code>model_name: The name of the model to use for the conversation.\n</code></pre> <pre><code>A BioChatter Conversation object for connecting to the LLM.\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._get_conversation--todo","title":"Todo:","text":"<pre><code>Genericise to models outside of OpenAI.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _get_conversation(\n    self,\n    model_name: str | None = None,\n) -&gt; \"Conversation\":\n    \"\"\"Create a conversation object given a model name.\n\n    Args:\n    ----\n        model_name: The name of the model to use for the conversation.\n\n    Returns:\n    -------\n        A BioChatter Conversation object for connecting to the LLM.\n\n    Todo:\n    ----\n        Genericise to models outside of OpenAI.\n\n    \"\"\"\n    conversation = GptConversation(\n        model_name=model_name or self.model_name,\n        prompts={},\n        correct=False,\n    )\n    conversation.set_api_key(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        user=\"test_user\",\n    )\n    return conversation\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._select_entities","title":"<code>_select_entities(question, conversation)</code>","text":"<p>Given a question, select the entities that are relevant to the question and store them in <code>selected_entities</code> and <code>selected_relationships</code>. Use LLM conversation to do this.</p> <pre><code>question: A user's question.\n\nconversation: A BioChatter Conversation object for connecting to the\n    LLM.\n</code></pre> <pre><code>True if at least one entity was selected, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _select_entities(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; bool:\n    \"\"\"Given a question, select the entities that are relevant to the question\n    and store them in `selected_entities` and `selected_relationships`. Use\n    LLM conversation to do this.\n\n    Args:\n    ----\n        question: A user's question.\n\n        conversation: A BioChatter Conversation object for connecting to the\n            LLM.\n\n    Returns:\n    -------\n        True if at least one entity was selected, False otherwise.\n\n    \"\"\"\n    self.question = question\n\n    conversation.append_system_message(\n        \"You have access to a knowledge graph that contains \"\n        f\"these entity types: {', '.join(self.entities)}. Your task is \"\n        \"to select the entity types that are relevant to the user's question \"\n        \"for subsequent use in a query. Only return the entity types, \"\n        \"comma-separated, without any additional text. Do not return \"\n        \"entity names, relationships, or properties.\",\n    )\n\n    msg, token_usage, correction = conversation.query(question)\n\n    result = msg.split(\",\") if msg else []\n    # TODO: do we go back and retry if no entities were selected? or ask for\n    # a reason? offer visual selection of entities and relationships by the\n    # user?\n\n    if result:\n        for entity in result:\n            entity = entity.strip()\n            if entity in self.entities:\n                self.selected_entities.append(entity)\n\n    return bool(result)\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._select_properties","title":"<code>_select_properties(conversation)</code>","text":"<p>Given a question (optionally provided, but in the standard use case reused from the entity selection step) and the selected entities, select the properties that are relevant to the question and store them in the dictionary <code>selected_properties</code>.</p>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._select_properties--returns","title":"Returns","text":"<pre><code>True if at least one property was selected, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _select_properties(self, conversation: \"Conversation\") -&gt; bool:\n    \"\"\"Given a question (optionally provided, but in the standard use case\n    reused from the entity selection step) and the selected entities, select\n    the properties that are relevant to the question and store them in\n    the dictionary `selected_properties`.\n\n    Returns\n    -------\n        True if at least one property was selected, False otherwise.\n\n    \"\"\"\n    if not self.question:\n        raise ValueError(\n            \"No question found. Please make sure to run entity and relationship selection first.\",\n        )\n\n    if not self.selected_entities and not self.selected_relationships:\n        raise ValueError(\n            \"No entities or relationships provided, and none available \"\n            \"from entity selection step. Please provide \"\n            \"entities/relationships or run the entity selection \"\n            \"(`select_entities()`) step first.\",\n        )\n\n    e_props = {}\n    for entity in self.selected_entities:\n        if self.entities[entity].get(\"properties\"):\n            e_props[entity] = list(\n                self.entities[entity][\"properties\"].keys(),\n            )\n\n    r_props = {}\n    for relationship in self.selected_relationships:\n        if self.relationships[relationship].get(\"properties\"):\n            r_props[relationship] = list(\n                self.relationships[relationship][\"properties\"].keys(),\n            )\n\n    msg = (\n        \"You have access to a knowledge graph that contains entities and \"\n        \"relationships. They have the following properties. Entities:\"\n        f\"{e_props}, Relationships: {r_props}. \"\n        \"Your task is to select the properties that are relevant to the \"\n        \"user's question for subsequent use in a query. Only return the \"\n        \"entities and relationships with their relevant properties in compact \"\n        \"JSON format, without any additional text. Return the \"\n        \"entities/relationships as top-level dictionary keys, and their \"\n        \"properties as dictionary values. \"\n        \"Do not return properties that are not relevant to the question.\"\n    )\n\n    conversation.append_system_message(msg)\n\n    msg, token_usage, correction = conversation.query(self.question)\n    msg = BioCypherPromptEngine._validate_json_str(msg)\n\n    try:\n        self.selected_properties = json.loads(msg) if msg else {}\n    except json.decoder.JSONDecodeError:\n        self.selected_properties = {}\n\n    return bool(self.selected_properties)\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._select_relationships","title":"<code>_select_relationships(conversation)</code>","text":"<p>Given a question and the preselected entities, select relationships for the query.</p> <pre><code>conversation: A BioChatter Conversation object for connecting to the\n    LLM.\n</code></pre> <pre><code>True if at least one relationship was selected, False otherwise.\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine._select_relationships--todo","title":"Todo:","text":"<pre><code>Now we have the problem that we discard all relationships that do\nnot have a source and target, if at least one relationship has a\nsource and target. At least communicate this all-or-nothing\nbehaviour to the user.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def _select_relationships(self, conversation: \"Conversation\") -&gt; bool:\n    \"\"\"Given a question and the preselected entities, select relationships for\n    the query.\n\n    Args:\n    ----\n        conversation: A BioChatter Conversation object for connecting to the\n            LLM.\n\n    Returns:\n    -------\n        True if at least one relationship was selected, False otherwise.\n\n    Todo:\n    ----\n        Now we have the problem that we discard all relationships that do\n        not have a source and target, if at least one relationship has a\n        source and target. At least communicate this all-or-nothing\n        behaviour to the user.\n\n    \"\"\"\n    if not self.question:\n        raise ValueError(\n            \"No question found. Please make sure to run entity selection first.\",\n        )\n\n    if not self.selected_entities:\n        raise ValueError(\n            \"No entities found. Please run the entity selection step first.\",\n        )\n\n    rels = {}\n    source_and_target_present = False\n    for key, value in self.relationships.items():\n        if \"source\" in value and \"target\" in value:\n            # if source or target is a list, expand to single pairs\n            source = ensure_iterable(value[\"source\"])\n            target = ensure_iterable(value[\"target\"])\n            pairs = []\n            for s in source:\n                for t in target:\n                    pairs.append(\n                        (\n                            sentencecase_to_pascalcase(s),\n                            sentencecase_to_pascalcase(t),\n                        ),\n                    )\n            rels[key] = pairs\n            source_and_target_present = True\n        else:\n            rels[key] = {}\n\n    # prioritise relationships that have source and target, and discard\n    # relationships that do not have both source and target, if at least one\n    # relationship has both source and target. keep relationships that have\n    # either source or target, if none of the relationships have both source\n    # and target.\n\n    if source_and_target_present:\n        # First, separate the relationships into two groups: those with both\n        # source and target in the selected entities, and those with either\n        # source or target but not both.\n\n        rels_with_both = {}\n        rels_with_either = {}\n        for key, value in rels.items():\n            for pair in value:\n                if pair[0] in self.selected_entities:\n                    if pair[1] in self.selected_entities:\n                        rels_with_both[key] = value\n                    else:\n                        rels_with_either[key] = value\n                elif pair[1] in self.selected_entities:\n                    rels_with_either[key] = value\n\n        # If there are any relationships with both source and target,\n        # discard the others.\n\n        if rels_with_both:\n            rels = rels_with_both\n        else:\n            rels = rels_with_either\n\n        selected_rels = []\n        for key, value in rels.items():\n            if not value:\n                continue\n\n            for pair in value:\n                if pair[0] in self.selected_entities or pair[1] in self.selected_entities:\n                    selected_rels.append((key, pair))\n\n        rels = json.dumps(selected_rels)\n    else:\n        rels = json.dumps(self.relationships)\n\n    msg = (\n        \"You have access to a knowledge graph that contains \"\n        f\"these entities: {', '.join(self.selected_entities)}. \"\n        \"Your task is to select the relationships that are relevant \"\n        \"to the user's question for subsequent use in a query. Only \"\n        \"return the relationships without their sources or targets, \"\n        \"comma-separated, and without any additional text. Here are the \"\n        \"possible relationships and their source and target entities: \"\n        f\"{rels}.\"\n    )\n\n    conversation.append_system_message(msg)\n\n    res, token_usage, correction = conversation.query(self.question)\n\n    result = res.split(\",\") if msg else []\n\n    if result:\n        for relationship in result:\n            relationship = relationship.strip()\n            if relationship in self.relationships:\n                self.selected_relationships.append(relationship)\n                rel_dict = self.relationships[relationship]\n                label = rel_dict.get(\"label_as_edge\", relationship)\n                if \"source\" in rel_dict and \"target\" in rel_dict:\n                    self.selected_relationship_labels[label] = {\n                        \"source\": rel_dict[\"source\"],\n                        \"target\": rel_dict[\"target\"],\n                    }\n                else:\n                    self.selected_relationship_labels[label] = {\n                        \"source\": None,\n                        \"target\": None,\n                    }\n\n    # if we selected relationships that have either source or target which\n    # is not in the selected entities, we add those entities to the selected\n    # entities.\n\n    if self.selected_relationship_labels:\n        for key, value in self.selected_relationship_labels.items():\n            sources = ensure_iterable(value[\"source\"])\n            targets = ensure_iterable(value[\"target\"])\n            for source in sources:\n                if source is None:\n                    continue\n                if source not in self.selected_entities:\n                    self.selected_entities.append(\n                        sentencecase_to_pascalcase(source),\n                    )\n            for target in targets:\n                if target is None:\n                    continue\n                if target not in self.selected_entities:\n                    self.selected_entities.append(\n                        sentencecase_to_pascalcase(target),\n                    )\n\n    return bool(result)\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine.generate_query","title":"<code>generate_query(question, query_language='Cypher')</code>","text":"<p>Wrap entity and property selection and query generation; return the generated query.</p> <pre><code>question: A user's question.\n\nquery_language: The language of the query to generate.\n</code></pre> <pre><code>A database query that could answer the user's question.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def generate_query(\n    self,\n    question: str,\n    query_language: str | None = \"Cypher\",\n) -&gt; str:\n    \"\"\"Wrap entity and property selection and query generation; return the\n    generated query.\n\n    Args:\n    ----\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n    -------\n        A database query that could answer the user's question.\n\n    \"\"\"\n    self._select_graph_entities_from_question(\n        question,\n        self.conversation_factory(),\n    )\n\n    return self._generate_query(\n        question=question,\n        entities=self.selected_entities,\n        relationships=self.selected_relationship_labels,\n        properties=self.selected_properties,\n        query_language=query_language,\n        conversation=self.conversation_factory(),\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.prompts.BioCypherPromptEngine.generate_query_prompt","title":"<code>generate_query_prompt(question, query_language='Cypher')</code>","text":"<p>Generate a prompt for a large language model to generate a database query based on the user's question and class attributes informing about the schema.</p> <pre><code>question: A user's question.\n\nquery_language: The language of the query to generate.\n</code></pre> <pre><code>A prompt for a large language model to generate a database query.\n</code></pre> Source code in <code>biochatter/biochatter/prompts.py</code> <pre><code>def generate_query_prompt(\n    self,\n    question: str,\n    query_language: str | None = \"Cypher\",\n) -&gt; str:\n    \"\"\"Generate a prompt for a large language model to generate a database\n    query based on the user's question and class attributes informing about\n    the schema.\n\n    Args:\n    ----\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n    -------\n        A prompt for a large language model to generate a database query.\n\n    \"\"\"\n    self._select_graph_entities_from_question(\n        question,\n        self.conversation_factory(),\n    )\n    msg = self._generate_query_prompt(\n        self.selected_entities,\n        self.selected_relationship_labels,\n        self.selected_properties,\n        query_language,\n    )\n    return msg\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#execution-of-prompts-against-the-database","title":"Execution of prompts against the database","text":""},{"location":"BioChatter/api-docs/kg/#biochatter.database_agent.DatabaseAgent","title":"<code>DatabaseAgent</code>","text":"Source code in <code>biochatter/biochatter/database_agent.py</code> <pre><code>class DatabaseAgent:\n    def __init__(\n        self,\n        model_name: str,\n        connection_args: dict,\n        schema_config_or_info_dict: dict,\n        conversation_factory: Callable,\n        use_reflexion: bool,\n    ) -&gt; None:\n        \"\"\"Create a DatabaseAgent analogous to the VectorDatabaseAgentMilvus class,\n        which can return results from a database using a query engine. Currently\n        limited to Neo4j for development.\n\n        Args:\n        ----\n            connection_args (dict): A dictionary of arguments to connect to the\n                database. Contains database name, URI, user, and password.\n\n            conversation_factory (Callable): A function to create a conversation\n                for creating the KG query.\n\n            use_reflexion (bool): Whether to use the ReflexionAgent to generate\n                the query.\n\n        \"\"\"\n        self.conversation_factory = conversation_factory\n        self.prompt_engine = BioCypherPromptEngine(\n            model_name=model_name,\n            schema_config_or_info_dict=schema_config_or_info_dict,\n            conversation_factory=conversation_factory,\n        )\n        self.connection_args = connection_args\n        self.driver = None\n        self.use_reflexion = use_reflexion\n\n    def connect(self) -&gt; None:\n        \"\"\"Connect to the database and authenticate.\"\"\"\n        db_name = self.connection_args.get(\"db_name\")\n        uri = f\"{self.connection_args.get('host')}:{self.connection_args.get('port')}\"\n        uri = uri if uri.startswith(\"bolt://\") else \"bolt://\" + uri\n        user = self.connection_args.get(\"user\")\n        password = self.connection_args.get(\"password\")\n        self.driver = nu.Driver(\n            db_name=db_name or \"neo4j\",\n            db_uri=uri,\n            user=user,\n            password=password,\n        )\n\n    def is_connected(self) -&gt; bool:\n        return self.driver is not None\n\n    def _generate_query(self, query: str):\n        if self.use_reflexion:\n            agent = KGQueryReflexionAgent(\n                self.conversation_factory,\n                self.connection_args,\n            )\n            query_prompt = self.prompt_engine.generate_query_prompt(query)\n            agent_result = agent.execute(query, query_prompt)\n            tool_result = [agent_result.tool_result] if agent_result.tool_result is not None else None\n            return agent_result.answer, tool_result\n        else:\n            query = self.prompt_engine.generate_query(query)\n            results = self.driver.query(query=query)\n            return query, results\n\n    def _build_response(\n        self,\n        results: list[dict],\n        cypher_query: str,\n        results_num: int | None = 3,\n    ) -&gt; list[Document]:\n        if len(results) == 0:\n            return [\n                Document(\n                    page_content=(\n                        \"I didn't find any result in knowledge graph, \"\n                        f\"but here is the query I used: {cypher_query}. \"\n                        \"You can ask user to refine the question. \"\n                        \"Note: please ensure to include the query in a code \"\n                        \"block in your response so that the user can refine \"\n                        \"their question effectively.\"\n                    ),\n                    metadata={\"cypher_query\": cypher_query},\n                ),\n            ]\n\n        clipped_results = results[:results_num] if results_num &gt; 0 else results\n        results_dump = json.dumps(clipped_results)\n\n        return [\n            Document(\n                page_content=(\n                    \"The results retrieved from knowledge graph are: \"\n                    f\"{results_dump}. \"\n                    f\"The query used is: {cypher_query}. \"\n                    \"Note: please ensure to include the query in a code block \"\n                    \"in your response so that the user can refine \"\n                    \"their question effectively.\"\n                ),\n                metadata={\"cypher_query\": cypher_query},\n            ),\n        ]\n\n    def get_query_results(self, query: str, k: int = 3) -&gt; list[Document]:\n        \"\"\"Generate a query using the prompt engine and return the results.\n        Replicates vector database similarity search API. Results are returned\n        as a list of Document objects to align with the vector database agent.\n\n        Args:\n        ----\n            query (str): A query string.\n\n            k (int): The number of results to return.\n\n        Returns:\n        -------\n            List[Document]: A list of Document objects. The page content values\n                are the literal dictionaries returned by the query, the metadata\n                values are the cypher query used to generate the results, for\n                now.\n\n        \"\"\"\n        (cypher_query, tool_result) = self._generate_query(\n            query,\n        )  # self.prompt_engine.generate_query(query)\n        # TODO some logic if it fails?\n        if tool_result is not None:\n            # If _generate_query() already returned tool_result, we won't connect\n            # to graph database to query result any more\n            results = tool_result\n        else:\n            results = self.driver.query(query=cypher_query)\n\n        # return first k results\n        # returned nodes can have any formatting, and can also be empty or fewer\n        # than k\n        if results is None or len(results) == 0 or results[0] is None:\n            return []\n        return self._build_response(\n            results=results[0],\n            cypher_query=cypher_query,\n            results_num=k,\n        )\n\n    def get_description(self):\n        result = self.driver.query(\"MATCH (n:Schema_info) RETURN n LIMIT 1\")\n\n        if result[0]:\n            schema_info_node = result[0][0][\"n\"]\n            schema_dict_content = schema_info_node[\"schema_info\"][:MAX_AGENT_DESC_LENGTH]  # limit to 1000 characters\n            return f\"the graph database contains the following nodes and edges: \\n\\n{schema_dict_content}\"\n\n        # schema_info is not found in database\n        nodes_query = \"MATCH (n) RETURN DISTINCT labels(n) LIMIT 300\"\n        node_results = self.driver.query(query=nodes_query)\n        edges_query = \"MATCH (n) RETURN DISTINCT type(n) LIMIT 300\"\n        edge_results = self.driver.query(query=edges_query)\n        desc = (\n            f\"The graph database contains the following nodes and edges: \\n\"\n            f\"nodes: \\n{node_results}\"\n            f\"edges: \\n{edge_results}\"\n        )\n        return desc[:MAX_AGENT_DESC_LENGTH]\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.database_agent.DatabaseAgent.__init__","title":"<code>__init__(model_name, connection_args, schema_config_or_info_dict, conversation_factory, use_reflexion)</code>","text":"<p>Create a DatabaseAgent analogous to the VectorDatabaseAgentMilvus class, which can return results from a database using a query engine. Currently limited to Neo4j for development.</p> <pre><code>connection_args (dict): A dictionary of arguments to connect to the\n    database. Contains database name, URI, user, and password.\n\nconversation_factory (Callable): A function to create a conversation\n    for creating the KG query.\n\nuse_reflexion (bool): Whether to use the ReflexionAgent to generate\n    the query.\n</code></pre> Source code in <code>biochatter/biochatter/database_agent.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    connection_args: dict,\n    schema_config_or_info_dict: dict,\n    conversation_factory: Callable,\n    use_reflexion: bool,\n) -&gt; None:\n    \"\"\"Create a DatabaseAgent analogous to the VectorDatabaseAgentMilvus class,\n    which can return results from a database using a query engine. Currently\n    limited to Neo4j for development.\n\n    Args:\n    ----\n        connection_args (dict): A dictionary of arguments to connect to the\n            database. Contains database name, URI, user, and password.\n\n        conversation_factory (Callable): A function to create a conversation\n            for creating the KG query.\n\n        use_reflexion (bool): Whether to use the ReflexionAgent to generate\n            the query.\n\n    \"\"\"\n    self.conversation_factory = conversation_factory\n    self.prompt_engine = BioCypherPromptEngine(\n        model_name=model_name,\n        schema_config_or_info_dict=schema_config_or_info_dict,\n        conversation_factory=conversation_factory,\n    )\n    self.connection_args = connection_args\n    self.driver = None\n    self.use_reflexion = use_reflexion\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.database_agent.DatabaseAgent.connect","title":"<code>connect()</code>","text":"<p>Connect to the database and authenticate.</p> Source code in <code>biochatter/biochatter/database_agent.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to the database and authenticate.\"\"\"\n    db_name = self.connection_args.get(\"db_name\")\n    uri = f\"{self.connection_args.get('host')}:{self.connection_args.get('port')}\"\n    uri = uri if uri.startswith(\"bolt://\") else \"bolt://\" + uri\n    user = self.connection_args.get(\"user\")\n    password = self.connection_args.get(\"password\")\n    self.driver = nu.Driver(\n        db_name=db_name or \"neo4j\",\n        db_uri=uri,\n        user=user,\n        password=password,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/kg/#biochatter.database_agent.DatabaseAgent.get_query_results","title":"<code>get_query_results(query, k=3)</code>","text":"<p>Generate a query using the prompt engine and return the results. Replicates vector database similarity search API. Results are returned as a list of Document objects to align with the vector database agent.</p> <pre><code>query (str): A query string.\n\nk (int): The number of results to return.\n</code></pre> <pre><code>List[Document]: A list of Document objects. The page content values\n    are the literal dictionaries returned by the query, the metadata\n    values are the cypher query used to generate the results, for\n    now.\n</code></pre> Source code in <code>biochatter/biochatter/database_agent.py</code> <pre><code>def get_query_results(self, query: str, k: int = 3) -&gt; list[Document]:\n    \"\"\"Generate a query using the prompt engine and return the results.\n    Replicates vector database similarity search API. Results are returned\n    as a list of Document objects to align with the vector database agent.\n\n    Args:\n    ----\n        query (str): A query string.\n\n        k (int): The number of results to return.\n\n    Returns:\n    -------\n        List[Document]: A list of Document objects. The page content values\n            are the literal dictionaries returned by the query, the metadata\n            values are the cypher query used to generate the results, for\n            now.\n\n    \"\"\"\n    (cypher_query, tool_result) = self._generate_query(\n        query,\n    )  # self.prompt_engine.generate_query(query)\n    # TODO some logic if it fails?\n    if tool_result is not None:\n        # If _generate_query() already returned tool_result, we won't connect\n        # to graph database to query result any more\n        results = tool_result\n    else:\n        results = self.driver.query(query=cypher_query)\n\n    # return first k results\n    # returned nodes can have any formatting, and can also be empty or fewer\n    # than k\n    if results is None or len(results) == 0 or results[0] is None:\n        return []\n    return self._build_response(\n        results=results[0],\n        cypher_query=cypher_query,\n        results_num=k,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/","title":"LLM Connectivity Reference","text":"<p>Here we handle connections to various LLM services, proprietary and open source.</p> <p>Module for managing connections to LLM providers and handling conversations.</p> <p>This module provides classes for connecting to different LLM APIs (OpenAI, Anthropic, Ollama, etc.) and managing conversations with them, including message history, context injection, and response correction capabilities.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation","title":"<code>AnthropicConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Anthropic model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class AnthropicConversation(Conversation):\n    \"\"\"Conversation class for the Anthropic model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Connect to Anthropic's API and set up a conversation with the user.\n\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n        ----\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n        self.ca_model_name = \"claude-3-5-sonnet-20240620\"\n        # TODO make accessible by drop-down\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Anthropic API.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the Anthropic API.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        client = anthropic.Anthropic(\n            api_key=api_key,\n        )\n        self.user = user\n\n        try:\n            client.count_tokens(\"Test connection\")\n            self.chat = ChatAnthropic(\n                model_name=self.model_name,\n                temperature=0,\n                api_key=api_key,\n            )\n            self.ca_chat = ChatAnthropic(\n                model_name=self.ca_model_name,\n                temperature=0,\n                api_key=api_key,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except anthropic._exceptions.AuthenticationError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _primary_query(self) -&gt; tuple:\n        \"\"\"Query the Anthropic API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Returns\n        -------\n            tuple: A tuple containing the response from the Anthropic API and\n                the token usage.\n\n        \"\"\"\n        try:\n            history = self._create_history()\n            response = self.chat.generate([history])\n        except (\n            anthropic._exceptions.APIError,\n            anthropic._exceptions.AnthropicError,\n            anthropic._exceptions.ConflictError,\n            anthropic._exceptions.NotFoundError,\n            anthropic._exceptions.APIStatusError,\n            anthropic._exceptions.RateLimitError,\n            anthropic._exceptions.APITimeoutError,\n            anthropic._exceptions.BadRequestError,\n            anthropic._exceptions.APIConnectionError,\n            anthropic._exceptions.AuthenticationError,\n            anthropic._exceptions.InternalServerError,\n            anthropic._exceptions.PermissionDeniedError,\n            anthropic._exceptions.UnprocessableEntityError,\n            anthropic._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self) -&gt; list:\n        \"\"\"Create a history of messages for the Anthropic API.\n\n        Returns\n        -------\n            list: A list of messages, with the last message being the most\n                recent.\n\n        \"\"\"\n        history = []\n        # extract text components from message contents\n        msg_texts = [m.content[0][\"text\"] if isinstance(m.content, list) else m.content for m in self.messages]\n\n        # check if last message is an image message\n        is_image_message = False\n        if isinstance(self.messages[-1].content, list):\n            is_image_message = self.messages[-1].content[1][\"type\"] == \"image_url\"\n\n        # find location of last AI message (if any)\n        last_ai_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, AIMessage):\n                last_ai_message = i\n\n        # Aggregate system messages into one message at the beginning\n        system_messages = [m.content for m in self.messages if isinstance(m, SystemMessage)]\n        if system_messages:\n            history.append(\n                SystemMessage(content=\"\\n\".join(system_messages)),\n            )\n\n        # concatenate all messages before the last AI message into one message\n        if last_ai_message is not None:\n            history.append(\n                HumanMessage(\n                    content=\"\\n\".join([m for m in msg_texts[:last_ai_message]]),\n                ),\n            )\n            # then append the last AI message\n            history.append(\n                AIMessage(\n                    content=msg_texts[last_ai_message],\n                ),\n            )\n\n            # then concatenate all messages after that\n            # into one HumanMessage\n            history.append(\n                HumanMessage(\n                    content=\"\\n\".join(\n                        [m for m in msg_texts[last_ai_message + 1 :]],\n                    ),\n                ),\n            )\n\n        # else add human message to history (without system messages)\n        else:\n            last_system_message = None\n            for i, m in enumerate(self.messages):\n                if isinstance(m, SystemMessage):\n                    last_system_message = i\n            history.append(\n                HumanMessage(\n                    content=\"\\n\".join(\n                        [m for m in msg_texts[last_system_message + 1 :]],\n                    ),\n                ),\n            )\n\n        # if the last message is an image message, add the image to the history\n        if is_image_message:\n            history[-1][\"content\"] = [\n                {\"type\": \"text\", \"text\": history[-1][\"content\"]},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"],\n                    },\n                },\n            ]\n        return history\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Anthropic API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the Anthropic API.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        return correction\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False)</code>","text":"<p>Connect to Anthropic's API and set up a conversation with the user.</p> <p>Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>model_name (str): The name of the model to use.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Connect to Anthropic's API and set up a conversation with the user.\n\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n    ----\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n\n    self.ca_model_name = \"claude-3-5-sonnet-20240620\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation._correct_response","title":"<code>_correct_response(msg)</code>","text":"<p>Correct the response from the Anthropic API.</p> <p>Send the response to a secondary language model. Optionally split the response into single sentences and correct each sentence individually. Update usage stats.</p> <pre><code>msg (str): The response from the Anthropic API.\n</code></pre> <pre><code>str: The corrected response (or OK if no correction necessary).\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _correct_response(self, msg: str) -&gt; str:\n    \"\"\"Correct the response from the Anthropic API.\n\n    Send the response to a secondary language model. Optionally split the\n    response into single sentences and correct each sentence individually.\n    Update usage stats.\n\n    Args:\n    ----\n        msg (str): The response from the Anthropic API.\n\n    Returns:\n    -------\n        str: The corrected response (or OK if no correction necessary).\n\n    \"\"\"\n    ca_messages = self.ca_messages.copy()\n    ca_messages.append(\n        HumanMessage(\n            content=msg,\n        ),\n    )\n    ca_messages.append(\n        SystemMessage(\n            content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n        ),\n    )\n\n    response = self.ca_chat.generate([ca_messages])\n\n    correction = response.generations[0][0].text\n    token_usage = response.llm_output.get(\"token_usage\")\n\n    return correction\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation._create_history","title":"<code>_create_history()</code>","text":"<p>Create a history of messages for the Anthropic API.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation._create_history--returns","title":"Returns","text":"<pre><code>list: A list of messages, with the last message being the most\n    recent.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _create_history(self) -&gt; list:\n    \"\"\"Create a history of messages for the Anthropic API.\n\n    Returns\n    -------\n        list: A list of messages, with the last message being the most\n            recent.\n\n    \"\"\"\n    history = []\n    # extract text components from message contents\n    msg_texts = [m.content[0][\"text\"] if isinstance(m.content, list) else m.content for m in self.messages]\n\n    # check if last message is an image message\n    is_image_message = False\n    if isinstance(self.messages[-1].content, list):\n        is_image_message = self.messages[-1].content[1][\"type\"] == \"image_url\"\n\n    # find location of last AI message (if any)\n    last_ai_message = None\n    for i, m in enumerate(self.messages):\n        if isinstance(m, AIMessage):\n            last_ai_message = i\n\n    # Aggregate system messages into one message at the beginning\n    system_messages = [m.content for m in self.messages if isinstance(m, SystemMessage)]\n    if system_messages:\n        history.append(\n            SystemMessage(content=\"\\n\".join(system_messages)),\n        )\n\n    # concatenate all messages before the last AI message into one message\n    if last_ai_message is not None:\n        history.append(\n            HumanMessage(\n                content=\"\\n\".join([m for m in msg_texts[:last_ai_message]]),\n            ),\n        )\n        # then append the last AI message\n        history.append(\n            AIMessage(\n                content=msg_texts[last_ai_message],\n            ),\n        )\n\n        # then concatenate all messages after that\n        # into one HumanMessage\n        history.append(\n            HumanMessage(\n                content=\"\\n\".join(\n                    [m for m in msg_texts[last_ai_message + 1 :]],\n                ),\n            ),\n        )\n\n    # else add human message to history (without system messages)\n    else:\n        last_system_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, SystemMessage):\n                last_system_message = i\n        history.append(\n            HumanMessage(\n                content=\"\\n\".join(\n                    [m for m in msg_texts[last_system_message + 1 :]],\n                ),\n            ),\n        )\n\n    # if the last message is an image message, add the image to the history\n    if is_image_message:\n        history[-1][\"content\"] = [\n            {\"type\": \"text\", \"text\": history[-1][\"content\"]},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"],\n                },\n            },\n        ]\n    return history\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation._primary_query","title":"<code>_primary_query()</code>","text":"<p>Query the Anthropic API with the user's message.</p> <p>Return the response using the message history (flattery system messages, prior conversation) as context. Correct the response if necessary.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation._primary_query--returns","title":"Returns","text":"<pre><code>tuple: A tuple containing the response from the Anthropic API and\n    the token usage.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _primary_query(self) -&gt; tuple:\n    \"\"\"Query the Anthropic API with the user's message.\n\n    Return the response using the message history (flattery system messages,\n    prior conversation) as context. Correct the response if necessary.\n\n    Returns\n    -------\n        tuple: A tuple containing the response from the Anthropic API and\n            the token usage.\n\n    \"\"\"\n    try:\n        history = self._create_history()\n        response = self.chat.generate([history])\n    except (\n        anthropic._exceptions.APIError,\n        anthropic._exceptions.AnthropicError,\n        anthropic._exceptions.ConflictError,\n        anthropic._exceptions.NotFoundError,\n        anthropic._exceptions.APIStatusError,\n        anthropic._exceptions.RateLimitError,\n        anthropic._exceptions.APITimeoutError,\n        anthropic._exceptions.BadRequestError,\n        anthropic._exceptions.APIConnectionError,\n        anthropic._exceptions.AuthenticationError,\n        anthropic._exceptions.InternalServerError,\n        anthropic._exceptions.PermissionDeniedError,\n        anthropic._exceptions.UnprocessableEntityError,\n        anthropic._exceptions.APIResponseValidationError,\n    ) as e:\n        return str(e), None\n\n    msg = response.generations[0][0].text\n    token_usage = response.llm_output.get(\"token_usage\")\n\n    self.append_ai_message(msg)\n\n    return msg, token_usage\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Anthropic API.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the Anthropic API.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Anthropic API.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the Anthropic API.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    client = anthropic.Anthropic(\n        api_key=api_key,\n    )\n    self.user = user\n\n    try:\n        client.count_tokens(\"Test connection\")\n        self.chat = ChatAnthropic(\n            model_name=self.model_name,\n            temperature=0,\n            api_key=api_key,\n        )\n        self.ca_chat = ChatAnthropic(\n            model_name=self.ca_model_name,\n            temperature=0,\n            api_key=api_key,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except anthropic._exceptions.AuthenticationError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AzureGptConversation","title":"<code>AzureGptConversation</code>","text":"<p>               Bases: <code>GptConversation</code></p> <p>Conversation class for the Azure GPT model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class AzureGptConversation(GptConversation):\n    \"\"\"Conversation class for the Azure GPT model.\"\"\"\n\n    def __init__(\n        self,\n        deployment_name: str,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        version: str | None = None,\n        base_url: str | None = None,\n        update_token_usage: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Connect to Azure's GPT API and set up a conversation with the user.\n\n        Extends GptConversation.\n\n        Args:\n        ----\n            deployment_name (str): The name of the Azure deployment to use.\n\n            model_name (str): The name of the model to use. This is distinct\n                from the deployment name.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            version (str): The version of the Azure API to use.\n\n            base_url (str): The base URL of the Azure API to use.\n\n            update_token_usage (Callable): A function to update the token usage\n                statistics.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            update_token_usage=update_token_usage,\n        )\n\n        self.version = version\n        self.base_url = base_url\n        self.deployment_name = deployment_name\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Azure API.\n\n        If the key is valid, initialise the conversational agent. No user stats\n        on Azure.\n\n        Args:\n        ----\n            api_key (str): The API key for the Azure API.\n\n            user (str, optional): The user for usage statistics.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        try:\n            self.chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n            self.ca_chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n\n            self.chat.generate([[HumanMessage(content=\"Hello\")]])\n            self.user = user if user is not None else \"Azure Community\"\n\n            return True\n\n        except openai._exceptions.AuthenticationError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        if self._update_token_usage is not None:\n            self._update_token_usage(self.user, model, token_usage)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AzureGptConversation.__init__","title":"<code>__init__(deployment_name, model_name, prompts, correct=False, split_correction=False, version=None, base_url=None, update_token_usage=None)</code>","text":"<p>Connect to Azure's GPT API and set up a conversation with the user.</p> <p>Extends GptConversation.</p> <pre><code>deployment_name (str): The name of the Azure deployment to use.\n\nmodel_name (str): The name of the model to use. This is distinct\n    from the deployment name.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n\nversion (str): The version of the Azure API to use.\n\nbase_url (str): The base URL of the Azure API to use.\n\nupdate_token_usage (Callable): A function to update the token usage\n    statistics.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    deployment_name: str,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    version: str | None = None,\n    base_url: str | None = None,\n    update_token_usage: Callable | None = None,\n) -&gt; None:\n    \"\"\"Connect to Azure's GPT API and set up a conversation with the user.\n\n    Extends GptConversation.\n\n    Args:\n    ----\n        deployment_name (str): The name of the Azure deployment to use.\n\n        model_name (str): The name of the model to use. This is distinct\n            from the deployment name.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        version (str): The version of the Azure API to use.\n\n        base_url (str): The base URL of the Azure API to use.\n\n        update_token_usage (Callable): A function to update the token usage\n            statistics.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        update_token_usage=update_token_usage,\n    )\n\n    self.version = version\n    self.base_url = base_url\n    self.deployment_name = deployment_name\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.AzureGptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Azure API.</p> <p>If the key is valid, initialise the conversational agent. No user stats on Azure.</p> <pre><code>api_key (str): The API key for the Azure API.\n\nuser (str, optional): The user for usage statistics.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Azure API.\n\n    If the key is valid, initialise the conversational agent. No user stats\n    on Azure.\n\n    Args:\n    ----\n        api_key (str): The API key for the Azure API.\n\n        user (str, optional): The user for usage statistics.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    try:\n        self.chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n        self.ca_chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n\n        self.chat.generate([[HumanMessage(content=\"Hello\")]])\n        self.user = user if user is not None else \"Azure Community\"\n\n        return True\n\n    except openai._exceptions.AuthenticationError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.BloomConversation","title":"<code>BloomConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Bloom model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class BloomConversation(Conversation):\n    \"\"\"Conversation class for the Bloom model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        split_correction: bool,\n    ) -&gt; None:\n        \"\"\"Initialise the BloomConversation class.\n\n        DEPRECATED: Superceded by XinferenceConversation.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            split_correction=split_correction,\n        )\n\n        self.messages = []\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the HuggingFace API.\n\n        If the key is valid, initialise the conversational agent.\n\n        Args:\n        ----\n            api_key (str): The API key for the HuggingFace API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        self.chat = HuggingFaceHub(\n            repo_id=self.model_name,\n            model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n            # as per https://huggingface.co/docs/api-inference/detailed_parameters\n            huggingfacehub_api_token=api_key,\n        )\n\n        try:\n            self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n            return True\n        except ValueError:\n            return False\n\n    def _cast_messages(self, messages: list) -&gt; str:\n        \"\"\"Render the different roles of the chat-based conversation.\"\"\"\n        cast = \"\"\n        for m in messages:\n            if isinstance(m, SystemMessage):\n                cast += f\"System: {m.content}\\n\"\n            elif isinstance(m, HumanMessage):\n                cast += f\"Human: {m.content}\\n\"\n            elif isinstance(m, AIMessage):\n                cast += f\"AI: {m.content}\\n\"\n            else:\n                error_msg = f\"Unknown message type: {type(m)}\"\n                raise TypeError(error_msg)\n\n        return cast\n\n    def _primary_query(self) -&gt; tuple:\n        response = self.chat.generate([self._cast_messages(self.messages)])\n\n        msg = response.generations[0][0].text\n        token_usage = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        return \"ok\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.BloomConversation.__init__","title":"<code>__init__(model_name, prompts, split_correction)</code>","text":"<p>Initialise the BloomConversation class.</p> <p>DEPRECATED: Superceded by XinferenceConversation.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    split_correction: bool,\n) -&gt; None:\n    \"\"\"Initialise the BloomConversation class.\n\n    DEPRECATED: Superceded by XinferenceConversation.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        split_correction=split_correction,\n    )\n\n    self.messages = []\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.BloomConversation._cast_messages","title":"<code>_cast_messages(messages)</code>","text":"<p>Render the different roles of the chat-based conversation.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _cast_messages(self, messages: list) -&gt; str:\n    \"\"\"Render the different roles of the chat-based conversation.\"\"\"\n    cast = \"\"\n    for m in messages:\n        if isinstance(m, SystemMessage):\n            cast += f\"System: {m.content}\\n\"\n        elif isinstance(m, HumanMessage):\n            cast += f\"Human: {m.content}\\n\"\n        elif isinstance(m, AIMessage):\n            cast += f\"AI: {m.content}\\n\"\n        else:\n            error_msg = f\"Unknown message type: {type(m)}\"\n            raise TypeError(error_msg)\n\n    return cast\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.BloomConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the HuggingFace API.</p> <p>If the key is valid, initialise the conversational agent.</p> <pre><code>api_key (str): The API key for the HuggingFace API.\n\nuser (str): The user for usage statistics.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the HuggingFace API.\n\n    If the key is valid, initialise the conversational agent.\n\n    Args:\n    ----\n        api_key (str): The API key for the HuggingFace API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    self.chat = HuggingFaceHub(\n        repo_id=self.model_name,\n        model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n        # as per https://huggingface.co/docs/api-inference/detailed_parameters\n        huggingfacehub_api_token=api_key,\n    )\n\n    try:\n        self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation","title":"<code>Conversation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Use this class to set up a connection to an LLM API.</p> <p>Can be used to set the user name and API key, append specific messages for system, user, and AI roles (if available), set up the general context as well as manual and tool-based data inputs, and finally to query the API with prompts made by the user.</p> <p>The conversation class is expected to have a <code>messages</code> attribute to store the conversation, and a <code>history</code> attribute, which is a list of messages in a specific format for logging / printing.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class Conversation(ABC):\n    \"\"\"Use this class to set up a connection to an LLM API.\n\n    Can be used to set the user name and API key, append specific messages for\n    system, user, and AI roles (if available), set up the general context as\n    well as manual and tool-based data inputs, and finally to query the API\n    with prompts made by the user.\n\n    The conversation class is expected to have a `messages` attribute to store\n    the conversation, and a `history` attribute, which is a list of messages in\n    a specific format for logging / printing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        use_ragagent_selector: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.prompts = prompts\n        self.correct = correct\n        self.split_correction = split_correction\n        self.rag_agents: list[RagAgent] = []\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n        self._use_ragagent_selector = use_ragagent_selector\n        self._chat = None\n        self._ca_chat = None\n\n    @property\n    def chat(self):\n        \"\"\"Access the chat attribute with error handling.\"\"\"\n        if self._chat is None:\n            msg = \"Chat attribute not initialized. Did you call set_api_key()?\"\n            logger.error(msg)\n            raise AttributeError(msg)\n        return self._chat\n\n    @chat.setter\n    def chat(self, value):\n        \"\"\"Set the chat attribute.\"\"\"\n        self._chat = value\n\n    @property\n    def ca_chat(self):\n        \"\"\"Access the correcting agent chat attribute with error handling.\"\"\"\n        if self._ca_chat is None:\n            msg = \"Correcting agent chat attribute not initialized. Did you call set_api_key()?\"\n            logger.error(msg)\n            raise AttributeError(msg)\n        return self._ca_chat\n\n    @ca_chat.setter\n    def ca_chat(self, value):\n        \"\"\"Set the correcting agent chat attribute.\"\"\"\n        self._ca_chat = value\n\n    @property\n    def use_ragagent_selector(self) -&gt; bool:\n        \"\"\"Whether to use the ragagent selector.\"\"\"\n        return self._use_ragagent_selector\n\n    @use_ragagent_selector.setter\n    def use_ragagent_selector(self, val: bool) -&gt; None:\n        \"\"\"Set the use_ragagent_selector attribute.\"\"\"\n        self._use_ragagent_selector = val\n\n    def set_user_name(self, user_name: str) -&gt; None:\n        \"\"\"Set the user name.\"\"\"\n        self.user_name = user_name\n\n    def set_rag_agent(self, agent: RagAgent) -&gt; None:\n        \"\"\"Update or insert rag_agent.\n\n        If the rag_agent with the same mode already exists, it will be updated.\n        Otherwise, the new rag_agent will be inserted.\n        \"\"\"\n        i, _ = self.find_rag_agent(agent.mode)\n        if i &lt; 0:\n            # insert\n            self.rag_agents.append(agent)\n        else:\n            # update\n            self.rag_agents[i] = agent\n\n    def find_rag_agent(self, mode: str) -&gt; tuple[int, RagAgent]:\n        \"\"\"Find the rag_agent with the given mode.\"\"\"\n        for i, val in enumerate(self.rag_agents):\n            if val.mode == mode:\n                return i, val\n        return -1, None\n\n    @abstractmethod\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; None:\n        \"\"\"Set the API key.\"\"\"\n\n    def get_prompts(self) -&gt; dict:\n        \"\"\"Get the prompts.\"\"\"\n        return self.prompts\n\n    def set_prompts(self, prompts: dict) -&gt; None:\n        \"\"\"Set the prompts.\"\"\"\n        self.prompts = prompts\n\n    def append_ai_message(self, message: str) -&gt; None:\n        \"\"\"Add a message from the AI to the conversation.\n\n        Args:\n        ----\n            message (str): The message from the AI.\n\n        \"\"\"\n        self.messages.append(\n            AIMessage(\n                content=message,\n            ),\n        )\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"Add a system message to the conversation.\n\n        Args:\n        ----\n            message (str): The system message.\n\n        \"\"\"\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"Add a message to the correcting agent conversation.\n\n        Args:\n        ----\n            message (str): The message to the correcting agent.\n\n        \"\"\"\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_user_message(self, message: str) -&gt; None:\n        \"\"\"Add a message from the user to the conversation.\n\n        Args:\n        ----\n            message (str): The message from the user.\n\n        \"\"\"\n        self.messages.append(\n            HumanMessage(\n                content=message,\n            ),\n        )\n\n    def append_image_message(\n        self,\n        message: str,\n        image_url: str,\n        local: bool = False,\n    ) -&gt; None:\n        \"\"\"Add a user message with an image to the conversation.\n\n        Also checks, in addition to the `local` flag, if the image URL is a\n        local file path. If it is local, the image will be encoded as a base64\n        string to be passed to the LLM.\n\n        Args:\n        ----\n            message (str): The message from the user.\n            image_url (str): The URL of the image.\n            local (bool): Whether the image is local or not. If local, it will\n                be encoded as a base64 string to be passed to the LLM.\n\n        \"\"\"\n        parsed_url = urllib.parse.urlparse(image_url)\n        if local or not parsed_url.netloc:\n            image_url = f\"data:image/jpeg;base64,{encode_image(image_url)}\"\n        else:\n            image_url = f\"data:image/jpeg;base64,{encode_image_from_url(image_url)}\"\n\n        self.messages.append(\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": message},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ],\n            ),\n        )\n\n    def setup(self, context: str) -&gt; None:\n        \"\"\"Set up the conversation with general prompts and a context.\"\"\"\n        for msg in self.prompts[\"primary_model_prompts\"]:\n            if msg:\n                self.append_system_message(msg)\n\n        for msg in self.prompts[\"correcting_agent_prompts\"]:\n            if msg:\n                self.append_ca_message(msg)\n\n        self.context = context\n        msg = f\"The topic of the research is {context}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_manual(self, data_input: str) -&gt; None:\n        \"\"\"Set up the data input manually.\"\"\"\n        self.data_input = data_input\n        msg = f\"The user has given information on the data input: {data_input}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_tool(self, df, input_file_name: str) -&gt; None:\n        \"\"\"Set up the data input tool.\"\"\"\n        self.data_input_tool = df\n\n        for tool_name in self.prompts[\"tool_prompts\"]:\n            if tool_name in input_file_name:\n                msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n                self.append_system_message(msg)\n\n    def query(\n        self,\n        text: str,\n        image_url: str | None = None,\n    ) -&gt; tuple[str, dict | None, str | None]:\n        \"\"\"Query the LLM API using the user's query.\n\n        Appends the most recent query to the conversation, optionally injects\n        context from the RAG agent, and runs the primary query method of the\n        child class.\n\n        Args:\n        ----\n            text (str): The user query.\n\n            image_url (str): The URL of an image to include in the conversation.\n                Optional and only supported for models with vision capabilities.\n\n        Returns:\n        -------\n            tuple: A tuple containing the response from the API, the token usage\n                information, and the correction if necessary/desired.\n\n        \"\"\"\n        if not image_url:\n            self.append_user_message(text)\n        else:\n            self.append_image_message(text, image_url)\n\n        self._inject_context(text)\n\n        msg, token_usage = self._primary_query()\n\n        if not token_usage:\n            # indicates error\n            return (msg, token_usage, None)\n\n        if not self.correct:\n            return (msg, token_usage, None)\n\n        cor_msg = \"Correcting (using single sentences) ...\" if self.split_correction else \"Correcting ...\"\n\n        if st:\n            with st.spinner(cor_msg):\n                corrections = self._correct_query(text)\n        else:\n            corrections = self._correct_query(text)\n\n        if not corrections:\n            return (msg, token_usage, None)\n\n        correction = \"\\n\".join(corrections)\n        return (msg, token_usage, correction)\n\n    def _correct_query(self, msg: str) -&gt; list[str]:\n        corrections = []\n        if self.split_correction:\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = tokenizer.tokenize(msg)\n            for sentence in sentences:\n                correction = self._correct_response(sentence)\n\n                if str(correction).lower() not in [\"ok\", \"ok.\"]:\n                    corrections.append(correction)\n        else:\n            correction = self._correct_response(msg)\n\n            if str(correction).lower() not in [\"ok\", \"ok.\"]:\n                corrections.append(correction)\n\n        return corrections\n\n    @abstractmethod\n    def _primary_query(self, text: str) -&gt; tuple[str, dict | None]:\n        \"\"\"Run the primary query.\"\"\"\n\n    @abstractmethod\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response.\"\"\"\n\n    def _inject_context_by_ragagent_selector(self, text: str) -&gt; list[str]:\n        \"\"\"Inject the context generated by RagAgentSelector.\n\n        The RagAgentSelector will choose the appropriate rag agent to generate\n        context according to user's question.\n\n        Args:\n        ----\n            text (str): The user query to be used for choosing rag agent\n\n        \"\"\"\n        rag_agents: list[RagAgent] = [agent for agent in self.rag_agents if agent.use_prompt]\n        decider_agent = RagAgentSelector(\n            rag_agents=rag_agents,\n            conversation_factory=lambda: self,\n        )\n        result = decider_agent.execute(text)\n        if result.tool_result is not None and len(result.tool_result) &gt; 0:\n            return result.tool_result\n        # find rag agent selected\n        rag_agent = next(\n            [agent for agent in rag_agents if agent.mode == result.answer],\n            None,\n        )\n        if rag_agent is None:\n            return None\n        return rag_agent.generate_responses(text)\n\n    def _inject_context(self, text: str) -&gt; None:\n        \"\"\"Inject the context received from the RAG agent into the prompt.\n\n        The RAG agent will find the most similar n text fragments and add them\n        to the message history object for usage in the next prompt. Uses the\n        document summarisation prompt set to inject the context. The ultimate\n        prompt should include the placeholder for the statements, `{statements}`\n        (used for formatting the string).\n\n        Args:\n        ----\n            text (str): The user query to be used for similarity search.\n\n        \"\"\"\n        sim_msg = \"Performing similarity search to inject fragments ...\"\n\n        if st:\n            with st.spinner(sim_msg):\n                statements = []\n                if self.use_ragagent_selector:\n                    statements = self._inject_context_by_ragagent_selector(text)\n                else:\n                    for agent in self.rag_agents:\n                        try:\n                            docs = agent.generate_responses(text)\n                            statements = statements + [doc[0] for doc in docs]\n                        except ValueError as e:\n                            logger.warning(e)\n\n        else:\n            statements = []\n            if self.use_ragagent_selector:\n                statements = self._inject_context_by_ragagent_selector(text)\n            else:\n                for agent in self.rag_agents:\n                    try:\n                        docs = agent.generate_responses(text)\n                        statements = statements + [doc[0] for doc in docs]\n                    except ValueError as e:\n                        logger.warning(e)\n\n        if statements and len(statements) &gt; 0:\n            prompts = self.prompts[\"rag_agent_prompts\"]\n            self.current_statements = statements\n            for i, prompt in enumerate(prompts):\n                # if last prompt, format the statements into the prompt\n                if i == len(prompts) - 1:\n                    self.append_system_message(\n                        prompt.format(statements=statements),\n                    )\n                else:\n                    self.append_system_message(prompt)\n\n    def get_last_injected_context(self) -&gt; list[dict]:\n        \"\"\"Get a formatted list of the last context.\n\n        Get the last context injected into the conversation. Contains one\n        dictionary for each RAG mode.\n\n        Returns\n        -------\n            List[dict]: A list of dictionaries containing the mode and context\n            for each RAG agent.\n\n        \"\"\"\n        return [{\"mode\": agent.mode, \"context\": agent.last_response} for agent in self.rag_agents]\n\n    def get_msg_json(self) -&gt; str:\n        \"\"\"Return a JSON representation of the conversation.\n\n        Returns a list of dicts of the messages in the conversation in JSON\n        format. The keys of the dicts are the roles, the values are the\n        messages.\n\n        Returns\n        -------\n            str: A JSON representation of the messages in the conversation.\n\n        \"\"\"\n        d = []\n        for msg in self.messages:\n            if isinstance(msg, SystemMessage):\n                role = \"system\"\n            elif isinstance(msg, HumanMessage):\n                role = \"user\"\n            elif isinstance(msg, AIMessage):\n                role = \"ai\"\n            else:\n                error_msg = f\"Unknown message type: {type(msg)}\"\n                raise TypeError(error_msg)\n\n            d.append({role: msg.content})\n\n        return json.dumps(d)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the conversation to the initial state.\"\"\"\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.ca_chat","title":"<code>ca_chat</code>  <code>property</code> <code>writable</code>","text":"<p>Access the correcting agent chat attribute with error handling.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.chat","title":"<code>chat</code>  <code>property</code> <code>writable</code>","text":"<p>Access the chat attribute with error handling.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.use_ragagent_selector","title":"<code>use_ragagent_selector</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to use the ragagent selector.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation._correct_response","title":"<code>_correct_response(msg)</code>  <code>abstractmethod</code>","text":"<p>Correct the response.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>@abstractmethod\ndef _correct_response(self, msg: str) -&gt; str:\n    \"\"\"Correct the response.\"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation._inject_context","title":"<code>_inject_context(text)</code>","text":"<p>Inject the context received from the RAG agent into the prompt.</p> <p>The RAG agent will find the most similar n text fragments and add them to the message history object for usage in the next prompt. Uses the document summarisation prompt set to inject the context. The ultimate prompt should include the placeholder for the statements, <code>{statements}</code> (used for formatting the string).</p> <pre><code>text (str): The user query to be used for similarity search.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _inject_context(self, text: str) -&gt; None:\n    \"\"\"Inject the context received from the RAG agent into the prompt.\n\n    The RAG agent will find the most similar n text fragments and add them\n    to the message history object for usage in the next prompt. Uses the\n    document summarisation prompt set to inject the context. The ultimate\n    prompt should include the placeholder for the statements, `{statements}`\n    (used for formatting the string).\n\n    Args:\n    ----\n        text (str): The user query to be used for similarity search.\n\n    \"\"\"\n    sim_msg = \"Performing similarity search to inject fragments ...\"\n\n    if st:\n        with st.spinner(sim_msg):\n            statements = []\n            if self.use_ragagent_selector:\n                statements = self._inject_context_by_ragagent_selector(text)\n            else:\n                for agent in self.rag_agents:\n                    try:\n                        docs = agent.generate_responses(text)\n                        statements = statements + [doc[0] for doc in docs]\n                    except ValueError as e:\n                        logger.warning(e)\n\n    else:\n        statements = []\n        if self.use_ragagent_selector:\n            statements = self._inject_context_by_ragagent_selector(text)\n        else:\n            for agent in self.rag_agents:\n                try:\n                    docs = agent.generate_responses(text)\n                    statements = statements + [doc[0] for doc in docs]\n                except ValueError as e:\n                    logger.warning(e)\n\n    if statements and len(statements) &gt; 0:\n        prompts = self.prompts[\"rag_agent_prompts\"]\n        self.current_statements = statements\n        for i, prompt in enumerate(prompts):\n            # if last prompt, format the statements into the prompt\n            if i == len(prompts) - 1:\n                self.append_system_message(\n                    prompt.format(statements=statements),\n                )\n            else:\n                self.append_system_message(prompt)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation._inject_context_by_ragagent_selector","title":"<code>_inject_context_by_ragagent_selector(text)</code>","text":"<p>Inject the context generated by RagAgentSelector.</p> <p>The RagAgentSelector will choose the appropriate rag agent to generate context according to user's question.</p> <pre><code>text (str): The user query to be used for choosing rag agent\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _inject_context_by_ragagent_selector(self, text: str) -&gt; list[str]:\n    \"\"\"Inject the context generated by RagAgentSelector.\n\n    The RagAgentSelector will choose the appropriate rag agent to generate\n    context according to user's question.\n\n    Args:\n    ----\n        text (str): The user query to be used for choosing rag agent\n\n    \"\"\"\n    rag_agents: list[RagAgent] = [agent for agent in self.rag_agents if agent.use_prompt]\n    decider_agent = RagAgentSelector(\n        rag_agents=rag_agents,\n        conversation_factory=lambda: self,\n    )\n    result = decider_agent.execute(text)\n    if result.tool_result is not None and len(result.tool_result) &gt; 0:\n        return result.tool_result\n    # find rag agent selected\n    rag_agent = next(\n        [agent for agent in rag_agents if agent.mode == result.answer],\n        None,\n    )\n    if rag_agent is None:\n        return None\n    return rag_agent.generate_responses(text)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation._primary_query","title":"<code>_primary_query(text)</code>  <code>abstractmethod</code>","text":"<p>Run the primary query.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>@abstractmethod\ndef _primary_query(self, text: str) -&gt; tuple[str, dict | None]:\n    \"\"\"Run the primary query.\"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_ai_message","title":"<code>append_ai_message(message)</code>","text":"<p>Add a message from the AI to the conversation.</p> <pre><code>message (str): The message from the AI.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_ai_message(self, message: str) -&gt; None:\n    \"\"\"Add a message from the AI to the conversation.\n\n    Args:\n    ----\n        message (str): The message from the AI.\n\n    \"\"\"\n    self.messages.append(\n        AIMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Add a message to the correcting agent conversation.</p> <pre><code>message (str): The message to the correcting agent.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"Add a message to the correcting agent conversation.\n\n    Args:\n    ----\n        message (str): The message to the correcting agent.\n\n    \"\"\"\n    self.ca_messages.append(\n        SystemMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_image_message","title":"<code>append_image_message(message, image_url, local=False)</code>","text":"<p>Add a user message with an image to the conversation.</p> <p>Also checks, in addition to the <code>local</code> flag, if the image URL is a local file path. If it is local, the image will be encoded as a base64 string to be passed to the LLM.</p> <pre><code>message (str): The message from the user.\nimage_url (str): The URL of the image.\nlocal (bool): Whether the image is local or not. If local, it will\n    be encoded as a base64 string to be passed to the LLM.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_image_message(\n    self,\n    message: str,\n    image_url: str,\n    local: bool = False,\n) -&gt; None:\n    \"\"\"Add a user message with an image to the conversation.\n\n    Also checks, in addition to the `local` flag, if the image URL is a\n    local file path. If it is local, the image will be encoded as a base64\n    string to be passed to the LLM.\n\n    Args:\n    ----\n        message (str): The message from the user.\n        image_url (str): The URL of the image.\n        local (bool): Whether the image is local or not. If local, it will\n            be encoded as a base64 string to be passed to the LLM.\n\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(image_url)\n    if local or not parsed_url.netloc:\n        image_url = f\"data:image/jpeg;base64,{encode_image(image_url)}\"\n    else:\n        image_url = f\"data:image/jpeg;base64,{encode_image_from_url(image_url)}\"\n\n    self.messages.append(\n        HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": message},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n            ],\n        ),\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Add a system message to the conversation.</p> <pre><code>message (str): The system message.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"Add a system message to the conversation.\n\n    Args:\n    ----\n        message (str): The system message.\n\n    \"\"\"\n    self.messages.append(\n        SystemMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_user_message","title":"<code>append_user_message(message)</code>","text":"<p>Add a message from the user to the conversation.</p> <pre><code>message (str): The message from the user.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_user_message(self, message: str) -&gt; None:\n    \"\"\"Add a message from the user to the conversation.\n\n    Args:\n    ----\n        message (str): The message from the user.\n\n    \"\"\"\n    self.messages.append(\n        HumanMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.find_rag_agent","title":"<code>find_rag_agent(mode)</code>","text":"<p>Find the rag_agent with the given mode.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def find_rag_agent(self, mode: str) -&gt; tuple[int, RagAgent]:\n    \"\"\"Find the rag_agent with the given mode.\"\"\"\n    for i, val in enumerate(self.rag_agents):\n        if val.mode == mode:\n            return i, val\n    return -1, None\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_last_injected_context","title":"<code>get_last_injected_context()</code>","text":"<p>Get a formatted list of the last context.</p> <p>Get the last context injected into the conversation. Contains one dictionary for each RAG mode.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_last_injected_context--returns","title":"Returns","text":"<pre><code>List[dict]: A list of dictionaries containing the mode and context\nfor each RAG agent.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def get_last_injected_context(self) -&gt; list[dict]:\n    \"\"\"Get a formatted list of the last context.\n\n    Get the last context injected into the conversation. Contains one\n    dictionary for each RAG mode.\n\n    Returns\n    -------\n        List[dict]: A list of dictionaries containing the mode and context\n        for each RAG agent.\n\n    \"\"\"\n    return [{\"mode\": agent.mode, \"context\": agent.last_response} for agent in self.rag_agents]\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_msg_json","title":"<code>get_msg_json()</code>","text":"<p>Return a JSON representation of the conversation.</p> <p>Returns a list of dicts of the messages in the conversation in JSON format. The keys of the dicts are the roles, the values are the messages.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_msg_json--returns","title":"Returns","text":"<pre><code>str: A JSON representation of the messages in the conversation.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def get_msg_json(self) -&gt; str:\n    \"\"\"Return a JSON representation of the conversation.\n\n    Returns a list of dicts of the messages in the conversation in JSON\n    format. The keys of the dicts are the roles, the values are the\n    messages.\n\n    Returns\n    -------\n        str: A JSON representation of the messages in the conversation.\n\n    \"\"\"\n    d = []\n    for msg in self.messages:\n        if isinstance(msg, SystemMessage):\n            role = \"system\"\n        elif isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"ai\"\n        else:\n            error_msg = f\"Unknown message type: {type(msg)}\"\n            raise TypeError(error_msg)\n\n        d.append({role: msg.content})\n\n    return json.dumps(d)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_prompts","title":"<code>get_prompts()</code>","text":"<p>Get the prompts.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def get_prompts(self) -&gt; dict:\n    \"\"\"Get the prompts.\"\"\"\n    return self.prompts\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.query","title":"<code>query(text, image_url=None)</code>","text":"<p>Query the LLM API using the user's query.</p> <p>Appends the most recent query to the conversation, optionally injects context from the RAG agent, and runs the primary query method of the child class.</p> <pre><code>text (str): The user query.\n\nimage_url (str): The URL of an image to include in the conversation.\n    Optional and only supported for models with vision capabilities.\n</code></pre> <pre><code>tuple: A tuple containing the response from the API, the token usage\n    information, and the correction if necessary/desired.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def query(\n    self,\n    text: str,\n    image_url: str | None = None,\n) -&gt; tuple[str, dict | None, str | None]:\n    \"\"\"Query the LLM API using the user's query.\n\n    Appends the most recent query to the conversation, optionally injects\n    context from the RAG agent, and runs the primary query method of the\n    child class.\n\n    Args:\n    ----\n        text (str): The user query.\n\n        image_url (str): The URL of an image to include in the conversation.\n            Optional and only supported for models with vision capabilities.\n\n    Returns:\n    -------\n        tuple: A tuple containing the response from the API, the token usage\n            information, and the correction if necessary/desired.\n\n    \"\"\"\n    if not image_url:\n        self.append_user_message(text)\n    else:\n        self.append_image_message(text, image_url)\n\n    self._inject_context(text)\n\n    msg, token_usage = self._primary_query()\n\n    if not token_usage:\n        # indicates error\n        return (msg, token_usage, None)\n\n    if not self.correct:\n        return (msg, token_usage, None)\n\n    cor_msg = \"Correcting (using single sentences) ...\" if self.split_correction else \"Correcting ...\"\n\n    if st:\n        with st.spinner(cor_msg):\n            corrections = self._correct_query(text)\n    else:\n        corrections = self._correct_query(text)\n\n    if not corrections:\n        return (msg, token_usage, None)\n\n    correction = \"\\n\".join(corrections)\n    return (msg, token_usage, correction)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.reset","title":"<code>reset()</code>","text":"<p>Reset the conversation to the initial state.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the conversation to the initial state.\"\"\"\n    self.history = []\n    self.messages = []\n    self.ca_messages = []\n    self.current_statements = []\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>  <code>abstractmethod</code>","text":"<p>Set the API key.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>@abstractmethod\ndef set_api_key(self, api_key: str, user: str | None = None) -&gt; None:\n    \"\"\"Set the API key.\"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_prompts","title":"<code>set_prompts(prompts)</code>","text":"<p>Set the prompts.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_prompts(self, prompts: dict) -&gt; None:\n    \"\"\"Set the prompts.\"\"\"\n    self.prompts = prompts\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_rag_agent","title":"<code>set_rag_agent(agent)</code>","text":"<p>Update or insert rag_agent.</p> <p>If the rag_agent with the same mode already exists, it will be updated. Otherwise, the new rag_agent will be inserted.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_rag_agent(self, agent: RagAgent) -&gt; None:\n    \"\"\"Update or insert rag_agent.\n\n    If the rag_agent with the same mode already exists, it will be updated.\n    Otherwise, the new rag_agent will be inserted.\n    \"\"\"\n    i, _ = self.find_rag_agent(agent.mode)\n    if i &lt; 0:\n        # insert\n        self.rag_agents.append(agent)\n    else:\n        # update\n        self.rag_agents[i] = agent\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_user_name","title":"<code>set_user_name(user_name)</code>","text":"<p>Set the user name.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_user_name(self, user_name: str) -&gt; None:\n    \"\"\"Set the user name.\"\"\"\n    self.user_name = user_name\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.setup","title":"<code>setup(context)</code>","text":"<p>Set up the conversation with general prompts and a context.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def setup(self, context: str) -&gt; None:\n    \"\"\"Set up the conversation with general prompts and a context.\"\"\"\n    for msg in self.prompts[\"primary_model_prompts\"]:\n        if msg:\n            self.append_system_message(msg)\n\n    for msg in self.prompts[\"correcting_agent_prompts\"]:\n        if msg:\n            self.append_ca_message(msg)\n\n    self.context = context\n    msg = f\"The topic of the research is {context}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.setup_data_input_manual","title":"<code>setup_data_input_manual(data_input)</code>","text":"<p>Set up the data input manually.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def setup_data_input_manual(self, data_input: str) -&gt; None:\n    \"\"\"Set up the data input manually.\"\"\"\n    self.data_input = data_input\n    msg = f\"The user has given information on the data input: {data_input}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.Conversation.setup_data_input_tool","title":"<code>setup_data_input_tool(df, input_file_name)</code>","text":"<p>Set up the data input tool.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def setup_data_input_tool(self, df, input_file_name: str) -&gt; None:\n    \"\"\"Set up the data input tool.\"\"\"\n    self.data_input_tool = df\n\n    for tool_name in self.prompts[\"tool_prompts\"]:\n        if tool_name in input_file_name:\n            msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n            self.append_system_message(msg)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation","title":"<code>GptConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the OpenAI GPT model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class GptConversation(Conversation):\n    \"\"\"Conversation class for the OpenAI GPT model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        base_url: str = None,\n        update_token_usage: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Connect to OpenAI's GPT API and set up a conversation with the user.\n\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n        ----\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            base_url (str): Optional OpenAI base_url value to use custom\n                endpoint URL instead of default\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.base_url = base_url\n        self.ca_model_name = \"gpt-3.5-turbo\"\n        # TODO make accessible by drop-down\n\n        self._update_token_usage = update_token_usage\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the OpenAI API.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the OpenAI API.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        client = openai.OpenAI(\n            api_key=api_key,\n            base_url=self.base_url,\n        )\n        self.user = user\n\n        try:\n            client.models.list()\n            self.chat = ChatOpenAI(\n                model_name=self.model_name,\n                temperature=0,\n                openai_api_key=api_key,\n                base_url=self.base_url,\n            )\n            self.ca_chat = ChatOpenAI(\n                model_name=self.ca_model_name,\n                temperature=0,\n                openai_api_key=api_key,\n                base_url=self.base_url,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except openai._exceptions.AuthenticationError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _primary_query(self) -&gt; tuple:\n        \"\"\"Query the OpenAI API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Returns\n        -------\n            tuple: A tuple containing the response from the OpenAI API and the\n                token usage.\n\n        \"\"\"\n        try:\n            response = self.chat.generate([self.messages])\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the OpenAI API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the OpenAI API.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update redis database with token usage statistics.\n\n        Use the usage_stats object with the increment method.\n\n        Args:\n        ----\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n\n        \"\"\"\n        if self.user == \"community\":\n            # Only process integer values\n            stats_dict = {f\"{k}:{model}\": v for k, v in token_usage.items() if isinstance(v, int | float)}\n            self.usage_stats.increment(\n                \"usage:[date]:[user]\",\n                stats_dict,\n            )\n\n        if self._update_token_usage is not None:\n            self._update_token_usage(self.user, model, token_usage)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False, base_url=None, update_token_usage=None)</code>","text":"<p>Connect to OpenAI's GPT API and set up a conversation with the user.</p> <p>Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>model_name (str): The name of the model to use.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n\nbase_url (str): Optional OpenAI base_url value to use custom\n    endpoint URL instead of default\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    base_url: str = None,\n    update_token_usage: Callable | None = None,\n) -&gt; None:\n    \"\"\"Connect to OpenAI's GPT API and set up a conversation with the user.\n\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n    ----\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        base_url (str): Optional OpenAI base_url value to use custom\n            endpoint URL instead of default\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.base_url = base_url\n    self.ca_model_name = \"gpt-3.5-turbo\"\n    # TODO make accessible by drop-down\n\n    self._update_token_usage = update_token_usage\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation._correct_response","title":"<code>_correct_response(msg)</code>","text":"<p>Correct the response from the OpenAI API.</p> <p>Send the response to a secondary language model. Optionally split the response into single sentences and correct each sentence individually. Update usage stats.</p> <pre><code>msg (str): The response from the OpenAI API.\n</code></pre> <pre><code>str: The corrected response (or OK if no correction necessary).\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _correct_response(self, msg: str) -&gt; str:\n    \"\"\"Correct the response from the OpenAI API.\n\n    Send the response to a secondary language model. Optionally split the\n    response into single sentences and correct each sentence individually.\n    Update usage stats.\n\n    Args:\n    ----\n        msg (str): The response from the OpenAI API.\n\n    Returns:\n    -------\n        str: The corrected response (or OK if no correction necessary).\n\n    \"\"\"\n    ca_messages = self.ca_messages.copy()\n    ca_messages.append(\n        HumanMessage(\n            content=msg,\n        ),\n    )\n    ca_messages.append(\n        SystemMessage(\n            content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n        ),\n    )\n\n    response = self.ca_chat.generate([ca_messages])\n\n    correction = response.generations[0][0].text\n    token_usage = response.llm_output.get(\"token_usage\")\n\n    self._update_usage_stats(self.ca_model_name, token_usage)\n\n    return correction\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation._primary_query","title":"<code>_primary_query()</code>","text":"<p>Query the OpenAI API with the user's message.</p> <p>Return the response using the message history (flattery system messages, prior conversation) as context. Correct the response if necessary.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation._primary_query--returns","title":"Returns","text":"<pre><code>tuple: A tuple containing the response from the OpenAI API and the\n    token usage.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _primary_query(self) -&gt; tuple:\n    \"\"\"Query the OpenAI API with the user's message.\n\n    Return the response using the message history (flattery system messages,\n    prior conversation) as context. Correct the response if necessary.\n\n    Returns\n    -------\n        tuple: A tuple containing the response from the OpenAI API and the\n            token usage.\n\n    \"\"\"\n    try:\n        response = self.chat.generate([self.messages])\n    except (\n        openai._exceptions.APIError,\n        openai._exceptions.OpenAIError,\n        openai._exceptions.ConflictError,\n        openai._exceptions.NotFoundError,\n        openai._exceptions.APIStatusError,\n        openai._exceptions.RateLimitError,\n        openai._exceptions.APITimeoutError,\n        openai._exceptions.BadRequestError,\n        openai._exceptions.APIConnectionError,\n        openai._exceptions.AuthenticationError,\n        openai._exceptions.InternalServerError,\n        openai._exceptions.PermissionDeniedError,\n        openai._exceptions.UnprocessableEntityError,\n        openai._exceptions.APIResponseValidationError,\n    ) as e:\n        return str(e), None\n\n    msg = response.generations[0][0].text\n    token_usage = response.llm_output.get(\"token_usage\")\n\n    self._update_usage_stats(self.model_name, token_usage)\n\n    self.append_ai_message(msg)\n\n    return msg, token_usage\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation._update_usage_stats","title":"<code>_update_usage_stats(model, token_usage)</code>","text":"<p>Update redis database with token usage statistics.</p> <p>Use the usage_stats object with the increment method.</p> <pre><code>model (str): The model name.\n\ntoken_usage (dict): The token usage statistics.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n    \"\"\"Update redis database with token usage statistics.\n\n    Use the usage_stats object with the increment method.\n\n    Args:\n    ----\n        model (str): The model name.\n\n        token_usage (dict): The token usage statistics.\n\n    \"\"\"\n    if self.user == \"community\":\n        # Only process integer values\n        stats_dict = {f\"{k}:{model}\": v for k, v in token_usage.items() if isinstance(v, int | float)}\n        self.usage_stats.increment(\n            \"usage:[date]:[user]\",\n            stats_dict,\n        )\n\n    if self._update_token_usage is not None:\n        self._update_token_usage(self.user, model, token_usage)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.GptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the OpenAI API.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the OpenAI API.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the OpenAI API.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the OpenAI API.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    client = openai.OpenAI(\n        api_key=api_key,\n        base_url=self.base_url,\n    )\n    self.user = user\n\n    try:\n        client.models.list()\n        self.chat = ChatOpenAI(\n            model_name=self.model_name,\n            temperature=0,\n            openai_api_key=api_key,\n            base_url=self.base_url,\n        )\n        self.ca_chat = ChatOpenAI(\n            model_name=self.ca_model_name,\n            temperature=0,\n            openai_api_key=api_key,\n            base_url=self.base_url,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except openai._exceptions.AuthenticationError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation","title":"<code>OllamaConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Ollama model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class OllamaConversation(Conversation):\n    \"\"\"Conversation class for the Ollama model.\"\"\"\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Ollama API. Not implemented.\n\n        Args:\n        ----\n            api_key (str): The API key for the Ollama API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        err = \"Ollama does not require an API key.\"\n        raise NotImplementedError(err)\n\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"llama3\",\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Connect to an Ollama LLM via the Ollama/Langchain library.\n\n        Set up a conversation with the user. Also initialise a second\n        conversational agent to provide corrections to the model output, if\n        necessary.\n\n        Args:\n        ----\n            base_url (str): The base URL of the Ollama instance.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Can be any model\n                name available in your Ollama instance.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each sentence\n                individually.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.model_name = model_name\n        self.model = ChatOllama(\n            base_url=base_url,\n            model=self.model_name,\n            temperature=0.0,\n        )\n\n        self.ca_model_name = \"mixtral:latest\"\n\n        self.ca_model = ChatOllama(\n            base_url=base_url,\n            model_name=self.ca_model_name,\n            temperature=0.0,\n        )\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition.\n\n        Ollama does not accept multiple system messages. Concatenate them if\n        there are multiple.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition for the correcting agent.\n\n        Ollama does not accept multiple system messages. Concatenate them if\n        there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self) -&gt; tuple:\n        \"\"\"Query the Ollama client API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Returns\n        -------\n            tuple: A tuple containing the response from the Ollama API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        try:\n            messages = self._create_history(self.messages)\n            response = self.model.invoke(\n                messages,\n                # ,generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n        response_dict = response.dict()\n        msg = response_dict[\"content\"]\n        token_usage = response_dict[\"response_metadata\"][\"eval_count\"]\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self, messages: list) -&gt; list:\n        history = []\n        for _, m in enumerate(messages):\n            if isinstance(m, AIMessage):\n                history.append(AIMessage(content=m.content))\n            elif isinstance(m, HumanMessage):\n                history.append(HumanMessage(content=m.content))\n            elif isinstance(m, SystemMessage):\n                history.append(SystemMessage(content=m.content))\n\n        return history\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Ollama API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the model.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n        response = self.ca_model.invoke(\n            chat_history=self._create_history(self.messages),\n        ).dict()\n        correction = response[\"content\"]\n        token_usage = response[\"eval_count\"]\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update redis database with token usage statistics.\n\n        Use the usage_stats object with the increment method.\n\n        Args:\n        ----\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n\n        \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='llama3', correct=False, split_correction=False)</code>","text":"<p>Connect to an Ollama LLM via the Ollama/Langchain library.</p> <p>Set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>base_url (str): The base URL of the Ollama instance.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Can be any model\n    name available in your Ollama instance.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each sentence\n    individually.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"llama3\",\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Connect to an Ollama LLM via the Ollama/Langchain library.\n\n    Set up a conversation with the user. Also initialise a second\n    conversational agent to provide corrections to the model output, if\n    necessary.\n\n    Args:\n    ----\n        base_url (str): The base URL of the Ollama instance.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Can be any model\n            name available in your Ollama instance.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.model_name = model_name\n    self.model = ChatOllama(\n        base_url=base_url,\n        model=self.model_name,\n        temperature=0.0,\n    )\n\n    self.ca_model_name = \"mixtral:latest\"\n\n    self.ca_model = ChatOllama(\n        base_url=base_url,\n        model_name=self.ca_model_name,\n        temperature=0.0,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation._correct_response","title":"<code>_correct_response(msg)</code>","text":"<p>Correct the response from the Ollama API.</p> <p>Send the response to a secondary language model. Optionally split the response into single sentences and correct each sentence individually. Update usage stats.</p> <pre><code>msg (str): The response from the model.\n</code></pre> <pre><code>str: The corrected response (or OK if no correction necessary).\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _correct_response(self, msg: str) -&gt; str:\n    \"\"\"Correct the response from the Ollama API.\n\n    Send the response to a secondary language model. Optionally split the\n    response into single sentences and correct each sentence individually.\n    Update usage stats.\n\n    Args:\n    ----\n        msg (str): The response from the model.\n\n    Returns:\n    -------\n        str: The corrected response (or OK if no correction necessary).\n\n    \"\"\"\n    ca_messages = self.ca_messages.copy()\n    ca_messages.append(\n        HumanMessage(\n            content=msg,\n        ),\n    )\n    ca_messages.append(\n        SystemMessage(\n            content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n        ),\n    )\n    response = self.ca_model.invoke(\n        chat_history=self._create_history(self.messages),\n    ).dict()\n    correction = response[\"content\"]\n    token_usage = response[\"eval_count\"]\n\n    self._update_usage_stats(self.ca_model_name, token_usage)\n\n    return correction\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation._primary_query","title":"<code>_primary_query()</code>","text":"<p>Query the Ollama client API with the user's message.</p> <p>Return the response using the message history (flattery system messages, prior conversation) as context. Correct the response if necessary.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation._primary_query--returns","title":"Returns","text":"<pre><code>tuple: A tuple containing the response from the Ollama API\n(formatted similarly to responses from the OpenAI API) and the token\nusage.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _primary_query(self) -&gt; tuple:\n    \"\"\"Query the Ollama client API with the user's message.\n\n    Return the response using the message history (flattery system messages,\n    prior conversation) as context. Correct the response if necessary.\n\n    Returns\n    -------\n        tuple: A tuple containing the response from the Ollama API\n        (formatted similarly to responses from the OpenAI API) and the token\n        usage.\n\n    \"\"\"\n    try:\n        messages = self._create_history(self.messages)\n        response = self.model.invoke(\n            messages,\n            # ,generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n    except (\n        openai._exceptions.APIError,\n        openai._exceptions.OpenAIError,\n        openai._exceptions.ConflictError,\n        openai._exceptions.NotFoundError,\n        openai._exceptions.APIStatusError,\n        openai._exceptions.RateLimitError,\n        openai._exceptions.APITimeoutError,\n        openai._exceptions.BadRequestError,\n        openai._exceptions.APIConnectionError,\n        openai._exceptions.AuthenticationError,\n        openai._exceptions.InternalServerError,\n        openai._exceptions.PermissionDeniedError,\n        openai._exceptions.UnprocessableEntityError,\n        openai._exceptions.APIResponseValidationError,\n    ) as e:\n        return str(e), None\n    response_dict = response.dict()\n    msg = response_dict[\"content\"]\n    token_usage = response_dict[\"response_metadata\"][\"eval_count\"]\n\n    self._update_usage_stats(self.model_name, token_usage)\n\n    self.append_ai_message(msg)\n\n    return msg, token_usage\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation._update_usage_stats","title":"<code>_update_usage_stats(model, token_usage)</code>","text":"<p>Update redis database with token usage statistics.</p> <p>Use the usage_stats object with the increment method.</p> <pre><code>model (str): The model name.\n\ntoken_usage (dict): The token usage statistics.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n    \"\"\"Update redis database with token usage statistics.\n\n    Use the usage_stats object with the increment method.\n\n    Args:\n    ----\n        model (str): The model name.\n\n        token_usage (dict): The token usage statistics.\n\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Override the system message addition for the correcting agent.</p> <p>Ollama does not accept multiple system messages. Concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition for the correcting agent.\n\n    Ollama does not accept multiple system messages. Concatenate them if\n    there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Override the system message addition.</p> <p>Ollama does not accept multiple system messages. Concatenate them if there are multiple.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition.\n\n    Ollama does not accept multiple system messages. Concatenate them if\n    there are multiple.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Ollama API. Not implemented.</p> <pre><code>api_key (str): The API key for the Ollama API.\n\nuser (str): The user for usage statistics.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Ollama API. Not implemented.\n\n    Args:\n    ----\n        api_key (str): The API key for the Ollama API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    err = \"Ollama does not require an API key.\"\n    raise NotImplementedError(err)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation","title":"<code>WasmConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the wasm model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class WasmConversation(Conversation):\n    \"\"\"Conversation class for the wasm model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the WasmConversation class.\n\n        This class is used to return the complete query as a string to be used\n        in the frontend running the wasm model. It does not call the API itself,\n        but updates the message history similarly to the other conversation\n        classes. It overrides the `query` method from the `Conversation` class\n        to return a plain string that contains the entire message for the model\n        as the first element of the tuple. The second and third elements are\n        `None` as there is no token usage or correction for the wasm model.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n    def query(self, text: str) -&gt; tuple:\n        \"\"\"Return the entire message history as a single string.\n\n        This is the message that is sent to the wasm model.\n\n        Args:\n        ----\n            text (str): The user query.\n\n        Returns:\n        -------\n            tuple: A tuple containing the message history as a single string,\n                and `None` for the second and third elements of the tuple.\n\n        \"\"\"\n        self.append_user_message(text)\n\n        self._inject_context(text)\n\n        return (self._primary_query(), None, None)\n\n    def _primary_query(self):\n        \"\"\"Concatenate all messages in the conversation.\n\n        Build a single string from all messages in the conversation.\n        Currently discards information about roles (system, user).\n\n        Returns\n        -------\n            str: A single string from all messages in the conversation.\n\n        \"\"\"\n        return \"\\n\".join([m.content for m in self.messages])\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Do not use for the wasm model.\"\"\"\n        return \"ok\"\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Do not use for the wasm model.\"\"\"\n        return True\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False)</code>","text":"<p>Initialize the WasmConversation class.</p> <p>This class is used to return the complete query as a string to be used in the frontend running the wasm model. It does not call the API itself, but updates the message history similarly to the other conversation classes. It overrides the <code>query</code> method from the <code>Conversation</code> class to return a plain string that contains the entire message for the model as the first element of the tuple. The second and third elements are <code>None</code> as there is no token usage or correction for the wasm model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the WasmConversation class.\n\n    This class is used to return the complete query as a string to be used\n    in the frontend running the wasm model. It does not call the API itself,\n    but updates the message history similarly to the other conversation\n    classes. It overrides the `query` method from the `Conversation` class\n    to return a plain string that contains the entire message for the model\n    as the first element of the tuple. The second and third elements are\n    `None` as there is no token usage or correction for the wasm model.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation._correct_response","title":"<code>_correct_response(msg)</code>","text":"<p>Do not use for the wasm model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _correct_response(self, msg: str) -&gt; str:\n    \"\"\"Do not use for the wasm model.\"\"\"\n    return \"ok\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation._primary_query","title":"<code>_primary_query()</code>","text":"<p>Concatenate all messages in the conversation.</p> <p>Build a single string from all messages in the conversation. Currently discards information about roles (system, user).</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation._primary_query--returns","title":"Returns","text":"<pre><code>str: A single string from all messages in the conversation.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _primary_query(self):\n    \"\"\"Concatenate all messages in the conversation.\n\n    Build a single string from all messages in the conversation.\n    Currently discards information about roles (system, user).\n\n    Returns\n    -------\n        str: A single string from all messages in the conversation.\n\n    \"\"\"\n    return \"\\n\".join([m.content for m in self.messages])\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation.query","title":"<code>query(text)</code>","text":"<p>Return the entire message history as a single string.</p> <p>This is the message that is sent to the wasm model.</p> <pre><code>text (str): The user query.\n</code></pre> <pre><code>tuple: A tuple containing the message history as a single string,\n    and `None` for the second and third elements of the tuple.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def query(self, text: str) -&gt; tuple:\n    \"\"\"Return the entire message history as a single string.\n\n    This is the message that is sent to the wasm model.\n\n    Args:\n    ----\n        text (str): The user query.\n\n    Returns:\n    -------\n        tuple: A tuple containing the message history as a single string,\n            and `None` for the second and third elements of the tuple.\n\n    \"\"\"\n    self.append_user_message(text)\n\n    self._inject_context(text)\n\n    return (self._primary_query(), None, None)\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.WasmConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Do not use for the wasm model.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Do not use for the wasm model.\"\"\"\n    return True\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation","title":"<code>XinferenceConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Xinference deployment.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>class XinferenceConversation(Conversation):\n    \"\"\"Conversation class for the Xinference deployment.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"auto\",\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Connect to an open-source LLM via the Xinference client.\n\n        Connect to a running Xinference deployment and set up a conversation\n        with the user. Also initialise a second conversational agent to\n        provide corrections to the model output, if necessary.\n\n        Args:\n        ----\n            base_url (str): The base URL of the Xinference instance (should not\n            include the /v1 part).\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Will be mapped to\n            the according uid from the list of available models. Can be set to\n            \"auto\" to use the first available model.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n        \"\"\"\n        # Shaohong: Please keep this xinference importing code here, so that,\n        # we don't need to depend on xinference if we dont need it (xinference\n        # is expensive to install)\n        from xinference.client import Client\n\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.client = Client(base_url=base_url)\n\n        self.models = {}\n        self.load_models()\n\n        self.ca_model_name = model_name\n\n        self.set_api_key()\n\n        # TODO make accessible by drop-down\n\n    def load_models(self) -&gt; None:\n        \"\"\"Load the models from the Xinference client.\"\"\"\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition.\n\n        Xinference does not accept multiple system messages. We concatenate them\n        if there are multiple.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition for the correcting agent.\n\n        Xinference does not accept multiple system messages. We concatenate them\n        if there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self) -&gt; tuple:\n        \"\"\"Query the Xinference client API.\n\n        Use the user's message and return the response using the message history\n        (flattery system messages, prior conversation) as context. Correct the\n        response if necessary.\n\n        LLaMA2 architecture does not accept separate system messages, so we\n        concatenate the system message with the user message to form the prompt.\n        'LLaMA enforces a strict rule that chats should alternate\n        user/assistant/user/assistant, and the system message, if present,\n        should be embedded into the first user message.' (from\n        https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3)\n\n        Returns\n        -------\n            tuple: A tuple containing the response from the Xinference API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        try:\n            history = self._create_history()\n            # TODO this is for LLaMA2 arch, may be different for newer models\n            prompt = history.pop()\n            response = self.model.chat(\n                prompt=prompt[\"content\"],\n                chat_history=history,\n                generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self) -&gt; list:\n        \"\"\"Create a history of messages from the conversation.\n\n        Returns\n        -------\n            list: A list of messages from the conversation.\n\n        \"\"\"\n        history = []\n        # extract text components from message contents\n        msg_texts = [m.content[0][\"text\"] if isinstance(m.content, list) else m.content for m in self.messages]\n\n        # check if last message is an image message\n        is_image_message = False\n        if isinstance(self.messages[-1].content, list):\n            is_image_message = self.messages[-1].content[1][\"type\"] == \"image_url\"\n\n        # find location of last AI message (if any)\n        last_ai_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, AIMessage):\n                last_ai_message = i\n\n        # concatenate all messages before the last AI message into one message\n        if last_ai_message:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m for m in msg_texts[:last_ai_message]],\n                    ),\n                },\n            )\n            # then append the last AI message\n            history.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": msg_texts[last_ai_message],\n                },\n            )\n\n            # then concatenate all messages after that\n            # into one HumanMessage\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m for m in msg_texts[last_ai_message + 1 :]],\n                    ),\n                },\n            )\n\n        # if there is no AI message, concatenate all messages into one user\n        # message\n        else:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join([m for m in msg_texts[:]]),\n                },\n            )\n\n        # if the last message is an image message, add the image to the history\n        if is_image_message:\n            history[-1][\"content\"] = [\n                {\"type\": \"text\", \"text\": history[-1][\"content\"]},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"],\n                    },\n                },\n            ]\n        return history\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Xinference API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the model.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n        history = []\n        for m in self.messages:\n            if isinstance(m, SystemMessage):\n                history.append({\"role\": \"system\", \"content\": m.content})\n            elif isinstance(m, HumanMessage):\n                history.append({\"role\": \"user\", \"content\": m.content})\n            elif isinstance(m, AIMessage):\n                history.append({\"role\": \"assistant\", \"content\": m.content})\n        prompt = history.pop()\n        response = self.ca_model.chat(\n            prompt=prompt[\"content\"],\n            chat_history=history,\n            generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n\n        correction = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update redis database with token usage statistics.\n\n        Use the usage_stats object with the increment method.\n\n        Args:\n        ----\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n\n        \"\"\"\n\n    def set_api_key(self) -&gt; bool:\n        \"\"\"Try to get the Xinference model from the client API.\n\n        If the model is found, initialise the conversational agent. If the model\n        is not found, `get_model` will raise a RuntimeError.\n\n        Returns\n        -------\n            bool: True if the model is found, False otherwise.\n\n        \"\"\"\n        try:\n            if self.model_name is None or self.model_name == \"auto\":\n                self.model_name = self.list_models_by_type(\"chat\")[0]\n            self.model = self.client.get_model(\n                self.models[self.model_name][\"id\"],\n            )\n\n            if self.ca_model_name is None or self.ca_model_name == \"auto\":\n                self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n            self.ca_model = self.client.get_model(\n                self.models[self.ca_model_name][\"id\"],\n            )\n            return True\n\n        except RuntimeError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def list_models_by_type(self, model_type: str) -&gt; list[str]:\n        \"\"\"List the models by type.\n\n        Args:\n        ----\n            model_type (str): The type of model to list.\n\n        Returns:\n        -------\n            list[str]: A list of model names.\n\n        \"\"\"\n        names = []\n        if model_type in [\"embed\", \"embedding\"]:\n            for name, model in self.models.items():\n                if \"model_ability\" in model:\n                    if \"embed\" in model[\"model_ability\"]:\n                        names.append(name)\n                elif model[\"model_type\"] == \"embedding\":\n                    names.append(name)\n            return names\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if model_type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == model_type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='auto', correct=False, split_correction=False)</code>","text":"<p>Connect to an open-source LLM via the Xinference client.</p> <p>Connect to a running Xinference deployment and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>base_url (str): The base URL of the Xinference instance (should not\ninclude the /v1 part).\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Will be mapped to\nthe according uid from the list of available models. Can be set to\n\"auto\" to use the first available model.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\nsplitting the output into sentences and correcting each sentence\nindividually.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"auto\",\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Connect to an open-source LLM via the Xinference client.\n\n    Connect to a running Xinference deployment and set up a conversation\n    with the user. Also initialise a second conversational agent to\n    provide corrections to the model output, if necessary.\n\n    Args:\n    ----\n        base_url (str): The base URL of the Xinference instance (should not\n        include the /v1 part).\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Will be mapped to\n        the according uid from the list of available models. Can be set to\n        \"auto\" to use the first available model.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n        splitting the output into sentences and correcting each sentence\n        individually.\n\n    \"\"\"\n    # Shaohong: Please keep this xinference importing code here, so that,\n    # we don't need to depend on xinference if we dont need it (xinference\n    # is expensive to install)\n    from xinference.client import Client\n\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.client = Client(base_url=base_url)\n\n    self.models = {}\n    self.load_models()\n\n    self.ca_model_name = model_name\n\n    self.set_api_key()\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation._correct_response","title":"<code>_correct_response(msg)</code>","text":"<p>Correct the response from the Xinference API.</p> <p>Send the response to a secondary language model. Optionally split the response into single sentences and correct each sentence individually. Update usage stats.</p> <pre><code>msg (str): The response from the model.\n</code></pre> <pre><code>str: The corrected response (or OK if no correction necessary).\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _correct_response(self, msg: str) -&gt; str:\n    \"\"\"Correct the response from the Xinference API.\n\n    Send the response to a secondary language model. Optionally split the\n    response into single sentences and correct each sentence individually.\n    Update usage stats.\n\n    Args:\n    ----\n        msg (str): The response from the model.\n\n    Returns:\n    -------\n        str: The corrected response (or OK if no correction necessary).\n\n    \"\"\"\n    ca_messages = self.ca_messages.copy()\n    ca_messages.append(\n        HumanMessage(\n            content=msg,\n        ),\n    )\n    ca_messages.append(\n        SystemMessage(\n            content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n        ),\n    )\n    history = []\n    for m in self.messages:\n        if isinstance(m, SystemMessage):\n            history.append({\"role\": \"system\", \"content\": m.content})\n        elif isinstance(m, HumanMessage):\n            history.append({\"role\": \"user\", \"content\": m.content})\n        elif isinstance(m, AIMessage):\n            history.append({\"role\": \"assistant\", \"content\": m.content})\n    prompt = history.pop()\n    response = self.ca_model.chat(\n        prompt=prompt[\"content\"],\n        chat_history=history,\n        generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n    )\n\n    correction = response[\"choices\"][0][\"message\"][\"content\"]\n    token_usage = response[\"usage\"]\n\n    self._update_usage_stats(self.ca_model_name, token_usage)\n\n    return correction\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation._create_history","title":"<code>_create_history()</code>","text":"<p>Create a history of messages from the conversation.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation._create_history--returns","title":"Returns","text":"<pre><code>list: A list of messages from the conversation.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _create_history(self) -&gt; list:\n    \"\"\"Create a history of messages from the conversation.\n\n    Returns\n    -------\n        list: A list of messages from the conversation.\n\n    \"\"\"\n    history = []\n    # extract text components from message contents\n    msg_texts = [m.content[0][\"text\"] if isinstance(m.content, list) else m.content for m in self.messages]\n\n    # check if last message is an image message\n    is_image_message = False\n    if isinstance(self.messages[-1].content, list):\n        is_image_message = self.messages[-1].content[1][\"type\"] == \"image_url\"\n\n    # find location of last AI message (if any)\n    last_ai_message = None\n    for i, m in enumerate(self.messages):\n        if isinstance(m, AIMessage):\n            last_ai_message = i\n\n    # concatenate all messages before the last AI message into one message\n    if last_ai_message:\n        history.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"\\n\".join(\n                    [m for m in msg_texts[:last_ai_message]],\n                ),\n            },\n        )\n        # then append the last AI message\n        history.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": msg_texts[last_ai_message],\n            },\n        )\n\n        # then concatenate all messages after that\n        # into one HumanMessage\n        history.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"\\n\".join(\n                    [m for m in msg_texts[last_ai_message + 1 :]],\n                ),\n            },\n        )\n\n    # if there is no AI message, concatenate all messages into one user\n    # message\n    else:\n        history.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"\\n\".join([m for m in msg_texts[:]]),\n            },\n        )\n\n    # if the last message is an image message, add the image to the history\n    if is_image_message:\n        history[-1][\"content\"] = [\n            {\"type\": \"text\", \"text\": history[-1][\"content\"]},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"],\n                },\n            },\n        ]\n    return history\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation._primary_query","title":"<code>_primary_query()</code>","text":"<p>Query the Xinference client API.</p> <p>Use the user's message and return the response using the message history (flattery system messages, prior conversation) as context. Correct the response if necessary.</p> <p>LLaMA2 architecture does not accept separate system messages, so we concatenate the system message with the user message to form the prompt. 'LLaMA enforces a strict rule that chats should alternate user/assistant/user/assistant, and the system message, if present, should be embedded into the first user message.' (from https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3)</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation._primary_query--returns","title":"Returns","text":"<pre><code>tuple: A tuple containing the response from the Xinference API\n(formatted similarly to responses from the OpenAI API) and the token\nusage.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _primary_query(self) -&gt; tuple:\n    \"\"\"Query the Xinference client API.\n\n    Use the user's message and return the response using the message history\n    (flattery system messages, prior conversation) as context. Correct the\n    response if necessary.\n\n    LLaMA2 architecture does not accept separate system messages, so we\n    concatenate the system message with the user message to form the prompt.\n    'LLaMA enforces a strict rule that chats should alternate\n    user/assistant/user/assistant, and the system message, if present,\n    should be embedded into the first user message.' (from\n    https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3)\n\n    Returns\n    -------\n        tuple: A tuple containing the response from the Xinference API\n        (formatted similarly to responses from the OpenAI API) and the token\n        usage.\n\n    \"\"\"\n    try:\n        history = self._create_history()\n        # TODO this is for LLaMA2 arch, may be different for newer models\n        prompt = history.pop()\n        response = self.model.chat(\n            prompt=prompt[\"content\"],\n            chat_history=history,\n            generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n    except (\n        openai._exceptions.APIError,\n        openai._exceptions.OpenAIError,\n        openai._exceptions.ConflictError,\n        openai._exceptions.NotFoundError,\n        openai._exceptions.APIStatusError,\n        openai._exceptions.RateLimitError,\n        openai._exceptions.APITimeoutError,\n        openai._exceptions.BadRequestError,\n        openai._exceptions.APIConnectionError,\n        openai._exceptions.AuthenticationError,\n        openai._exceptions.InternalServerError,\n        openai._exceptions.PermissionDeniedError,\n        openai._exceptions.UnprocessableEntityError,\n        openai._exceptions.APIResponseValidationError,\n    ) as e:\n        return str(e), None\n\n    msg = response[\"choices\"][0][\"message\"][\"content\"]\n    token_usage = response[\"usage\"]\n\n    self._update_usage_stats(self.model_name, token_usage)\n\n    self.append_ai_message(msg)\n\n    return msg, token_usage\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation._update_usage_stats","title":"<code>_update_usage_stats(model, token_usage)</code>","text":"<p>Update redis database with token usage statistics.</p> <p>Use the usage_stats object with the increment method.</p> <pre><code>model (str): The model name.\n\ntoken_usage (dict): The token usage statistics.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n    \"\"\"Update redis database with token usage statistics.\n\n    Use the usage_stats object with the increment method.\n\n    Args:\n    ----\n        model (str): The model name.\n\n        token_usage (dict): The token usage statistics.\n\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Override the system message addition for the correcting agent.</p> <p>Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition for the correcting agent.\n\n    Xinference does not accept multiple system messages. We concatenate them\n    if there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Override the system message addition.</p> <p>Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition.\n\n    Xinference does not accept multiple system messages. We concatenate them\n    if there are multiple.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.list_models_by_type","title":"<code>list_models_by_type(model_type)</code>","text":"<p>List the models by type.</p> <pre><code>model_type (str): The type of model to list.\n</code></pre> <pre><code>list[str]: A list of model names.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def list_models_by_type(self, model_type: str) -&gt; list[str]:\n    \"\"\"List the models by type.\n\n    Args:\n    ----\n        model_type (str): The type of model to list.\n\n    Returns:\n    -------\n        list[str]: A list of model names.\n\n    \"\"\"\n    names = []\n    if model_type in [\"embed\", \"embedding\"]:\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if \"embed\" in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == \"embedding\":\n                names.append(name)\n        return names\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if model_type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == model_type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.load_models","title":"<code>load_models()</code>","text":"<p>Load the models from the Xinference client.</p> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def load_models(self) -&gt; None:\n    \"\"\"Load the models from the Xinference client.\"\"\"\n    for id, model in self.client.list_models().items():\n        model[\"id\"] = id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.set_api_key","title":"<code>set_api_key()</code>","text":"<p>Try to get the Xinference model from the client API.</p> <p>If the model is found, initialise the conversational agent. If the model is not found, <code>get_model</code> will raise a RuntimeError.</p>"},{"location":"BioChatter/api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.set_api_key--returns","title":"Returns","text":"<pre><code>bool: True if the model is found, False otherwise.\n</code></pre> Source code in <code>biochatter/biochatter/llm_connect.py</code> <pre><code>def set_api_key(self) -&gt; bool:\n    \"\"\"Try to get the Xinference model from the client API.\n\n    If the model is found, initialise the conversational agent. If the model\n    is not found, `get_model` will raise a RuntimeError.\n\n    Returns\n    -------\n        bool: True if the model is found, False otherwise.\n\n    \"\"\"\n    try:\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"chat\")[0]\n        self.model = self.client.get_model(\n            self.models[self.model_name][\"id\"],\n        )\n\n        if self.ca_model_name is None or self.ca_model_name == \"auto\":\n            self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n        self.ca_model = self.client.get_model(\n            self.models[self.ca_model_name][\"id\"],\n        )\n        return True\n\n    except RuntimeError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/","title":"Podcast Reference","text":"<p>Here we handle generation of podcasts from texts.</p>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster","title":"<code>Podcaster</code>","text":"Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>class Podcaster:\n    def __init__(\n        self,\n        document: Document,\n        model_name: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"Orchestrates the podcasting of a document.\"\"\"\n        self.document = document\n        self.model_name = model_name\n\n    def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n        \"\"\"Podcasts the document.\n\n        Todo:\n        ----\n        - chain of density prompting for variable summary length\n\n        \"\"\"\n        full_text = self.document[0].page_content\n\n        # split text by sentence\n        sentences = self._split_text(full_text)\n\n        # could embed sentences and cluster on cosine similarity to identify\n        # paragraphs here\n\n        # preprocess text\n        for i, sentence in enumerate(sentences):\n            # special cases i.e. and e.g. - if sentence ends with one of these,\n            # append next sentence\n            special_cases = [\"i.e.\", \"e.g.\"]\n            if sentence.endswith(tuple(special_cases)):\n                sentences[i] = sentence + \" \" + sentences[i + 1]\n                del sentences[i + 1]\n\n        # concatenate first 5 sentences for title and author extraction\n        first_5 = \"\\n\".join(sentences[:5])\n        self.podcast_info = self._title_and_authors(first_5)\n\n        # LLM to determine section breaks?\n\n        # go through sections and summarise each\n        self.processed_sections = self._process_sections(\n            sentences,\n            characters_per_paragraph,\n        )\n\n        # summarise the summaries\n\n    def _split_text(self, text: str) -&gt; list[str]:\n        \"\"\"Splits consecutive text into sentences.\"\"\"\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        return tokenizer.tokenize(text)\n\n    def _title_and_authors(self, text: str) -&gt; str:\n        \"\"\"Extracts title and authors from document.\n\n        Args:\n        ----\n            text (str): text to extract title and authors from\n\n        Returns:\n        -------\n            str: title and authors\n\n        \"\"\"\n        # first sentence - extract title, authors\n        c_first = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c_first.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        c_first.append_system_message(FIRST_PROMPT)\n        msg, token_usage, correction = c_first.query(text)\n        # split at authors ('Authors:' or '\\nAuthors:')\n        if \"Authors:\" in msg:\n            title = msg.split(\"Title:\")[1].split(\"Authors:\")[0].strip()\n            authors = msg.split(\"Authors:\")[1].strip()\n            return f\"{title}, by {authors}, podcasted by biochatter.\"\n        else:\n            return \"A podcast by biochatter.\"\n\n    def _process_section(self, text: str, summarise: bool = False) -&gt; str:\n        \"\"\"Processes a section of the document. Summarises if summarise is True,\n        otherwise just makes the text more listenable.\n\n        Args:\n        ----\n            text (str): text to summarise\n\n            summarise (bool): whether to summarise the text\n\n        Returns:\n        -------\n            str: summarised text\n\n        \"\"\"\n        # summarise section\n        c = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        if summarise:\n            c.append_system_message(SUMMARISE_PROMPT)\n        else:\n            c.append_system_message(PROCESS_PROMPT)\n        msg, token_usage, correction = c.query(text)\n        return msg\n\n    def _process_sections(\n        self,\n        sentences: list,\n        characters_per_paragraph: int,\n    ) -&gt; list:\n        \"\"\"Processes sections of the document. Concatenates sentences until\n        characters_per_paragraph is reached, removing each sentence from the\n        list as it is added to the section to be processed.\n\n        Args:\n        ----\n            sentences (list): list of sentences to summarise\n\n            characters_per_paragraph (int): number of characters per paragraph\n\n        Returns:\n        -------\n            list: list of processed sections\n\n        \"\"\"\n        summarised_sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences.pop(0)\n            tmp = section + sentence\n            if len(tmp) &lt; characters_per_paragraph and sentences:\n                section += sentence\n            else:\n                if sentences:\n                    sentences.insert(0, sentence)\n                summarised_section = self._process_section(section)\n                # filter \"no content\" sections\n                if not (\"no content\" in summarised_section.lower() and len(summarised_section) &lt; 30):\n                    summarised_sections.append(summarised_section)\n                section = \"\"\n\n        return summarised_sections\n\n    def podcast_to_file(\n        self,\n        path: str,\n        model: str = \"gtts\",\n        voice: str = \"alloy\",\n    ) -&gt; None:\n        \"\"\"Uses text-to-speech to generate audio for the summarised paper podcast.\n\n        Args:\n        ----\n            path (str): path to save audio file to\n\n            model (str): model to use for text-to-speech. Currently supported:\n                'gtts' (Google Text-to-Speech, free),\n                'tts-1' (OpenAI API, paid, prioritises speed),\n                'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n            voice (str): voice to use for text-to-speech. See OpenAI API\n                documentation for available voices.\n\n        \"\"\"\n        full_text = self.podcast_to_text()\n\n        if model == \"gtts\":\n            audio = gTTS(text=full_text)\n            audio.save(path)\n        else:\n            client = OpenAI()\n\n            # Save the intro to the original file\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=(\n                    \"You are listening to: \\n\\n\" + self.podcast_info + \"\\n\\n\" + \" Text-to-speech generated by OpenAI.\"\n                ),\n            )\n            first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n            response.stream_to_file(first_path)\n\n            # Concatenate the sections\n            full_text = \"\"\n            for i, section in enumerate(self.processed_sections):\n                full_text += section + \"\\n\\n\"\n\n            # Make sections of 4000 characters max (at sentence boundaries)\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = deque(\n                tokenizer.tokenize(full_text),\n            )  # Use a deque instead of a list\n\n            # Split the text into sections by filling one section until it\n            # exceeds 4000 characters, then starting a new section (not adding\n            # the sentence that would exceed the limit)\n            sections = []\n            section = \"\"\n            while sentences:\n                sentence = sentences[0]\n                tmp = section + sentence\n                if len(tmp) &lt; 4000:\n                    section += sentences.popleft()\n                else:\n                    sections.append(section)\n                    section = \"\"\n\n            sections.append(section)  # Add the penultimate section\n\n            # Last section: conclude the podcast\n            sections.append(\n                f\"This was {self.podcast_info}. Thank you for listening.\",\n            )\n\n            # Save each section to a separate file with an integer suffix\n            for i, section in enumerate(sections):\n                response = client.audio.speech.create(\n                    model=model,\n                    voice=voice,\n                    input=section,\n                )\n                # Insert the integer suffix just before the .mp3 extension\n                section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n                response.stream_to_file(section_path)\n\n    def podcast_to_text(self):\n        \"\"\"Returns the summarised paper podcast as text.\"\"\"\n        full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n        for section in self.processed_sections:\n            full_text += section + \"\\n\\n\"\n        return full_text\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster.__init__","title":"<code>__init__(document, model_name='gpt-3.5-turbo')</code>","text":"<p>Orchestrates the podcasting of a document.</p> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def __init__(\n    self,\n    document: Document,\n    model_name: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"Orchestrates the podcasting of a document.\"\"\"\n    self.document = document\n    self.model_name = model_name\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster._process_section","title":"<code>_process_section(text, summarise=False)</code>","text":"<p>Processes a section of the document. Summarises if summarise is True, otherwise just makes the text more listenable.</p> <pre><code>text (str): text to summarise\n\nsummarise (bool): whether to summarise the text\n</code></pre> <pre><code>str: summarised text\n</code></pre> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def _process_section(self, text: str, summarise: bool = False) -&gt; str:\n    \"\"\"Processes a section of the document. Summarises if summarise is True,\n    otherwise just makes the text more listenable.\n\n    Args:\n    ----\n        text (str): text to summarise\n\n        summarise (bool): whether to summarise the text\n\n    Returns:\n    -------\n        str: summarised text\n\n    \"\"\"\n    # summarise section\n    c = GptConversation(\n        model_name=self.model_name,\n        prompts={},\n        correct=False,\n    )\n    c.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n    if summarise:\n        c.append_system_message(SUMMARISE_PROMPT)\n    else:\n        c.append_system_message(PROCESS_PROMPT)\n    msg, token_usage, correction = c.query(text)\n    return msg\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster._process_sections","title":"<code>_process_sections(sentences, characters_per_paragraph)</code>","text":"<p>Processes sections of the document. Concatenates sentences until characters_per_paragraph is reached, removing each sentence from the list as it is added to the section to be processed.</p> <pre><code>sentences (list): list of sentences to summarise\n\ncharacters_per_paragraph (int): number of characters per paragraph\n</code></pre> <pre><code>list: list of processed sections\n</code></pre> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def _process_sections(\n    self,\n    sentences: list,\n    characters_per_paragraph: int,\n) -&gt; list:\n    \"\"\"Processes sections of the document. Concatenates sentences until\n    characters_per_paragraph is reached, removing each sentence from the\n    list as it is added to the section to be processed.\n\n    Args:\n    ----\n        sentences (list): list of sentences to summarise\n\n        characters_per_paragraph (int): number of characters per paragraph\n\n    Returns:\n    -------\n        list: list of processed sections\n\n    \"\"\"\n    summarised_sections = []\n    section = \"\"\n    while sentences:\n        sentence = sentences.pop(0)\n        tmp = section + sentence\n        if len(tmp) &lt; characters_per_paragraph and sentences:\n            section += sentence\n        else:\n            if sentences:\n                sentences.insert(0, sentence)\n            summarised_section = self._process_section(section)\n            # filter \"no content\" sections\n            if not (\"no content\" in summarised_section.lower() and len(summarised_section) &lt; 30):\n                summarised_sections.append(summarised_section)\n            section = \"\"\n\n    return summarised_sections\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster._split_text","title":"<code>_split_text(text)</code>","text":"<p>Splits consecutive text into sentences.</p> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def _split_text(self, text: str) -&gt; list[str]:\n    \"\"\"Splits consecutive text into sentences.\"\"\"\n    nltk.download(\"punkt\")\n    tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n    return tokenizer.tokenize(text)\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster._title_and_authors","title":"<code>_title_and_authors(text)</code>","text":"<p>Extracts title and authors from document.</p> <pre><code>text (str): text to extract title and authors from\n</code></pre> <pre><code>str: title and authors\n</code></pre> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def _title_and_authors(self, text: str) -&gt; str:\n    \"\"\"Extracts title and authors from document.\n\n    Args:\n    ----\n        text (str): text to extract title and authors from\n\n    Returns:\n    -------\n        str: title and authors\n\n    \"\"\"\n    # first sentence - extract title, authors\n    c_first = GptConversation(\n        model_name=self.model_name,\n        prompts={},\n        correct=False,\n    )\n    c_first.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n    c_first.append_system_message(FIRST_PROMPT)\n    msg, token_usage, correction = c_first.query(text)\n    # split at authors ('Authors:' or '\\nAuthors:')\n    if \"Authors:\" in msg:\n        title = msg.split(\"Title:\")[1].split(\"Authors:\")[0].strip()\n        authors = msg.split(\"Authors:\")[1].strip()\n        return f\"{title}, by {authors}, podcasted by biochatter.\"\n    else:\n        return \"A podcast by biochatter.\"\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster.generate_podcast","title":"<code>generate_podcast(characters_per_paragraph)</code>","text":"<p>Podcasts the document.</p>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster.generate_podcast--todo","title":"Todo:","text":"<ul> <li>chain of density prompting for variable summary length</li> </ul> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n    \"\"\"Podcasts the document.\n\n    Todo:\n    ----\n    - chain of density prompting for variable summary length\n\n    \"\"\"\n    full_text = self.document[0].page_content\n\n    # split text by sentence\n    sentences = self._split_text(full_text)\n\n    # could embed sentences and cluster on cosine similarity to identify\n    # paragraphs here\n\n    # preprocess text\n    for i, sentence in enumerate(sentences):\n        # special cases i.e. and e.g. - if sentence ends with one of these,\n        # append next sentence\n        special_cases = [\"i.e.\", \"e.g.\"]\n        if sentence.endswith(tuple(special_cases)):\n            sentences[i] = sentence + \" \" + sentences[i + 1]\n            del sentences[i + 1]\n\n    # concatenate first 5 sentences for title and author extraction\n    first_5 = \"\\n\".join(sentences[:5])\n    self.podcast_info = self._title_and_authors(first_5)\n\n    # LLM to determine section breaks?\n\n    # go through sections and summarise each\n    self.processed_sections = self._process_sections(\n        sentences,\n        characters_per_paragraph,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster.podcast_to_file","title":"<code>podcast_to_file(path, model='gtts', voice='alloy')</code>","text":"<p>Uses text-to-speech to generate audio for the summarised paper podcast.</p> <pre><code>path (str): path to save audio file to\n\nmodel (str): model to use for text-to-speech. Currently supported:\n    'gtts' (Google Text-to-Speech, free),\n    'tts-1' (OpenAI API, paid, prioritises speed),\n    'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\nvoice (str): voice to use for text-to-speech. See OpenAI API\n    documentation for available voices.\n</code></pre> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def podcast_to_file(\n    self,\n    path: str,\n    model: str = \"gtts\",\n    voice: str = \"alloy\",\n) -&gt; None:\n    \"\"\"Uses text-to-speech to generate audio for the summarised paper podcast.\n\n    Args:\n    ----\n        path (str): path to save audio file to\n\n        model (str): model to use for text-to-speech. Currently supported:\n            'gtts' (Google Text-to-Speech, free),\n            'tts-1' (OpenAI API, paid, prioritises speed),\n            'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n        voice (str): voice to use for text-to-speech. See OpenAI API\n            documentation for available voices.\n\n    \"\"\"\n    full_text = self.podcast_to_text()\n\n    if model == \"gtts\":\n        audio = gTTS(text=full_text)\n        audio.save(path)\n    else:\n        client = OpenAI()\n\n        # Save the intro to the original file\n        response = client.audio.speech.create(\n            model=model,\n            voice=voice,\n            input=(\n                \"You are listening to: \\n\\n\" + self.podcast_info + \"\\n\\n\" + \" Text-to-speech generated by OpenAI.\"\n            ),\n        )\n        first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n        response.stream_to_file(first_path)\n\n        # Concatenate the sections\n        full_text = \"\"\n        for i, section in enumerate(self.processed_sections):\n            full_text += section + \"\\n\\n\"\n\n        # Make sections of 4000 characters max (at sentence boundaries)\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        sentences = deque(\n            tokenizer.tokenize(full_text),\n        )  # Use a deque instead of a list\n\n        # Split the text into sections by filling one section until it\n        # exceeds 4000 characters, then starting a new section (not adding\n        # the sentence that would exceed the limit)\n        sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences[0]\n            tmp = section + sentence\n            if len(tmp) &lt; 4000:\n                section += sentences.popleft()\n            else:\n                sections.append(section)\n                section = \"\"\n\n        sections.append(section)  # Add the penultimate section\n\n        # Last section: conclude the podcast\n        sections.append(\n            f\"This was {self.podcast_info}. Thank you for listening.\",\n        )\n\n        # Save each section to a separate file with an integer suffix\n        for i, section in enumerate(sections):\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=section,\n            )\n            # Insert the integer suffix just before the .mp3 extension\n            section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n            response.stream_to_file(section_path)\n</code></pre>"},{"location":"BioChatter/api-docs/podcast/#biochatter.podcast.Podcaster.podcast_to_text","title":"<code>podcast_to_text()</code>","text":"<p>Returns the summarised paper podcast as text.</p> Source code in <code>biochatter/biochatter/podcast.py</code> <pre><code>def podcast_to_text(self):\n    \"\"\"Returns the summarised paper podcast as text.\"\"\"\n    full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n    for section in self.processed_sections:\n        full_text += section + \"\\n\\n\"\n    return full_text\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/","title":"Reflexion Agent Reference","text":"<p>Here we handle the implementation of a LangGraph-based multi-agent system for reflexion on a user-defined task.</p>"},{"location":"BioChatter/api-docs/reflexion/#the-base-module","title":"The base module","text":""},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent","title":"<code>ReflexionAgent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>LLM agent reflexion framework:</p> <p>start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end                     /|\\                        |                      ---------------------------</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>class ReflexionAgent(ABC):\n    r\"\"\"LLM agent reflexion framework:\n\n    start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end\n                        /|\\                        |\n                         ---------------------------\n    \"\"\"\n\n    RECURSION_LIMIT = 30\n\n    def __init__(\n        self,\n        conversation_factory: Callable,\n        max_steps: int | None = 20,\n        agent_logger: ReflexionAgentLogger | None = ReflexionAgentLogger(),\n    ):\n        \"\"\"Args:\n        ----\n          conversation_factory Callable: the callback to create Conversation\n          max_steps int: max steps for reflextion loop\n\n        \"\"\"\n        if max_steps &lt;= 0:\n            max_steps = ReflexionAgent.RECURSION_LIMIT\n        recursion_limit = ReflexionAgent.RECURSION_LIMIT\n        if recursion_limit &lt; max_steps:\n            recursion_limit = max_steps\n        self.initial_responder = None\n        self.revise_responder = None\n        self.max_steps = max_steps\n        self.recursion_limit = recursion_limit\n        self.conversation = conversation_factory()\n        self.agent_logger = agent_logger\n\n    def _should_continue(self, state: list[BaseMessage]):\n        \"\"\"Determine if we need to continue reflexion\n        Args:\n          state List[BaseMessage]: message history\n        \"\"\"\n        num_steps = ReflexionAgent._get_num_iterations(state)\n        if num_steps &gt; self.max_steps:\n            return END\n        return EXECUTE_TOOL_NODE\n\n    @abstractmethod\n    def _tool_function(self, state: list[BaseMessage]) -&gt; ToolMessage:\n        \"\"\"Tool function, execute tool based on initial draft or revised answer\n        Args:\n          state List[BaseMessage]: message history\n        Returns:\n          ToolMessage\n        \"\"\"\n\n    @abstractmethod\n    def _create_initial_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        \"\"\"Draft responder, draft initial answer\n        Args:\n          prompt str: prompt for LLM to draft initial answer\n        \"\"\"\n\n    @abstractmethod\n    def _create_revise_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        \"\"\"Revise responder, revise answer according to tool function result\n        Args:\n          prompt str: prompt for LLM to draft initial answer\n        \"\"\"\n\n    @abstractmethod\n    def _parse_final_result(\n        self,\n        messages: list[BaseMessage],\n    ) -&gt; ReflexionAgentResult:\n        \"\"\"Parse the result of the last step\n        Args:\n          output BaseMessage: last step message\n        Returns:\n          ReflexionAgentResult: the parsed reuslt of the last step\n        \"\"\"\n\n    def get_logs(self):\n        return self.agent_logger.logs\n\n    @staticmethod\n    def _get_num_iterations(state: list[BaseMessage]):\n        \"\"\"Calculate iteration number\n        Args:\n          state List[BaseMessage]: message history\n\n        Returns\n        -------\n          int: the iterations number\n\n        \"\"\"\n        i = 0\n        for m in state[::-1]:\n            if not isinstance(m, (ToolMessage, AIMessage)):\n                break\n            i += 1\n        return i\n\n    @staticmethod\n    def _get_user_question(state: list[BaseMessage]):\n        \"\"\"Get user's question from messages array\"\"\"\n        for m in state:\n            if not isinstance(m, HumanMessage):\n                continue\n            return m.content\n        return None\n\n    @staticmethod\n    def _get_last_tool_result(messages: list[BaseMessage]):\n        \"\"\"Get result of the last tool node\"\"\"\n        for m in messages[::-1]:\n            if not isinstance(m, ToolMessage):\n                continue\n            content = json.loads(m.content)\n            return content[\"result\"]\n        return None\n\n    def _build_graph(self, prompt: str | None = None):\n        \"\"\"Build Langgraph graph for execution of chained LLM processes.\n\n        Args:\n        ----\n          prompt str: prompt for LLM\n\n        Returns:\n        -------\n          CompiledGraph | None: a Langgraph graph or None in case of errors\n\n        \"\"\"\n        try:\n            self.initial_responder = self._create_initial_responder(prompt)\n            self.revise_responder = self._create_revise_responder(prompt)\n            builder = MessageGraph()\n            builder.add_node(DRAFT_NODE, self.initial_responder.respond)\n            builder.add_node(EXECUTE_TOOL_NODE, self._tool_function)\n            builder.add_node(REVISE_NODE, self.revise_responder.respond)\n            builder.add_edge(DRAFT_NODE, EXECUTE_TOOL_NODE)\n            builder.add_edge(EXECUTE_TOOL_NODE, REVISE_NODE)\n\n            builder.add_conditional_edges(REVISE_NODE, self._should_continue)\n            builder.set_entry_point(DRAFT_NODE)\n            graph = builder.compile()\n            return graph\n        except Exception as e:\n            logger.error(e)\n            return None\n\n    def _execute_graph(\n        self,\n        graph: CompiledGraph | None = None,\n        question: str | None = \"\",\n    ) -&gt; ReflexionAgentResult:\n        \"\"\"Execute Langgraph graph\n        Args:\n          graph CompiledGraph: Langgraph graph\n          question str: user question\n\n        Returns\n        -------\n          answer str | None: string answer parsed from Langgraph graph execution\n\n        \"\"\"\n        if graph is None:\n            return None\n        if len(question) == 0:\n            return None\n\n        events = graph.stream(\n            [HumanMessage(content=question)],\n            {\n                \"recursion_limit\": self.recursion_limit,\n            },\n        )\n        messages = [HumanMessage(content=question)]\n        for i, step in enumerate(events):\n            if isinstance(step, list):\n                node, output = (f\"{i}\", step[i])\n            else:\n                node, output = next(iter(step.items()))\n            self.agent_logger.log_step_message(i + 1, node, output)\n            messages.append(output)\n\n        final_result = self._parse_final_result(messages)\n        self.agent_logger.log_final_result(final_result)\n        return final_result\n\n    def execute(\n        self,\n        question: str,\n        prompt: str | None = None,\n    ) -&gt; ReflexionAgentResult:\n        \"\"\"Execute ReflexionAgent. Wrapper for building a graph and executing it,\n        returning the final answer.\n\n        Args:\n        ----\n          question str: user question\n          prompt str: user prompt\n\n        Returns:\n        -------\n          answer str | None: If it executes successfully, an answer to the\n            question will be returned, otherwise, it returns None\n\n        \"\"\"\n        if len(question) == 0:\n            return None\n        graph = self._build_graph(prompt)\n        return self._execute_graph(graph, question)\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent.__init__","title":"<code>__init__(conversation_factory, max_steps=20, agent_logger=ReflexionAgentLogger())</code>","text":"<p>conversation_factory Callable: the callback to create Conversation   max_steps int: max steps for reflextion loop</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def __init__(\n    self,\n    conversation_factory: Callable,\n    max_steps: int | None = 20,\n    agent_logger: ReflexionAgentLogger | None = ReflexionAgentLogger(),\n):\n    \"\"\"Args:\n    ----\n      conversation_factory Callable: the callback to create Conversation\n      max_steps int: max steps for reflextion loop\n\n    \"\"\"\n    if max_steps &lt;= 0:\n        max_steps = ReflexionAgent.RECURSION_LIMIT\n    recursion_limit = ReflexionAgent.RECURSION_LIMIT\n    if recursion_limit &lt; max_steps:\n        recursion_limit = max_steps\n    self.initial_responder = None\n    self.revise_responder = None\n    self.max_steps = max_steps\n    self.recursion_limit = recursion_limit\n    self.conversation = conversation_factory()\n    self.agent_logger = agent_logger\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._build_graph","title":"<code>_build_graph(prompt=None)</code>","text":"<p>Build Langgraph graph for execution of chained LLM processes.</p> <p>prompt str: prompt for LLM</p> <p>CompiledGraph | None: a Langgraph graph or None in case of errors</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def _build_graph(self, prompt: str | None = None):\n    \"\"\"Build Langgraph graph for execution of chained LLM processes.\n\n    Args:\n    ----\n      prompt str: prompt for LLM\n\n    Returns:\n    -------\n      CompiledGraph | None: a Langgraph graph or None in case of errors\n\n    \"\"\"\n    try:\n        self.initial_responder = self._create_initial_responder(prompt)\n        self.revise_responder = self._create_revise_responder(prompt)\n        builder = MessageGraph()\n        builder.add_node(DRAFT_NODE, self.initial_responder.respond)\n        builder.add_node(EXECUTE_TOOL_NODE, self._tool_function)\n        builder.add_node(REVISE_NODE, self.revise_responder.respond)\n        builder.add_edge(DRAFT_NODE, EXECUTE_TOOL_NODE)\n        builder.add_edge(EXECUTE_TOOL_NODE, REVISE_NODE)\n\n        builder.add_conditional_edges(REVISE_NODE, self._should_continue)\n        builder.set_entry_point(DRAFT_NODE)\n        graph = builder.compile()\n        return graph\n    except Exception as e:\n        logger.error(e)\n        return None\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._create_initial_responder","title":"<code>_create_initial_responder(prompt=None)</code>  <code>abstractmethod</code>","text":"<p>Draft responder, draft initial answer Args:   prompt str: prompt for LLM to draft initial answer</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@abstractmethod\ndef _create_initial_responder(\n    self,\n    prompt: str | None = None,\n) -&gt; ResponderWithRetries:\n    \"\"\"Draft responder, draft initial answer\n    Args:\n      prompt str: prompt for LLM to draft initial answer\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._create_revise_responder","title":"<code>_create_revise_responder(prompt=None)</code>  <code>abstractmethod</code>","text":"<p>Revise responder, revise answer according to tool function result Args:   prompt str: prompt for LLM to draft initial answer</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@abstractmethod\ndef _create_revise_responder(\n    self,\n    prompt: str | None = None,\n) -&gt; ResponderWithRetries:\n    \"\"\"Revise responder, revise answer according to tool function result\n    Args:\n      prompt str: prompt for LLM to draft initial answer\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._execute_graph","title":"<code>_execute_graph(graph=None, question='')</code>","text":"<p>Execute Langgraph graph Args:   graph CompiledGraph: Langgraph graph   question str: user question</p>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._execute_graph--returns","title":"Returns","text":"<p>answer str | None: string answer parsed from Langgraph graph execution</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def _execute_graph(\n    self,\n    graph: CompiledGraph | None = None,\n    question: str | None = \"\",\n) -&gt; ReflexionAgentResult:\n    \"\"\"Execute Langgraph graph\n    Args:\n      graph CompiledGraph: Langgraph graph\n      question str: user question\n\n    Returns\n    -------\n      answer str | None: string answer parsed from Langgraph graph execution\n\n    \"\"\"\n    if graph is None:\n        return None\n    if len(question) == 0:\n        return None\n\n    events = graph.stream(\n        [HumanMessage(content=question)],\n        {\n            \"recursion_limit\": self.recursion_limit,\n        },\n    )\n    messages = [HumanMessage(content=question)]\n    for i, step in enumerate(events):\n        if isinstance(step, list):\n            node, output = (f\"{i}\", step[i])\n        else:\n            node, output = next(iter(step.items()))\n        self.agent_logger.log_step_message(i + 1, node, output)\n        messages.append(output)\n\n    final_result = self._parse_final_result(messages)\n    self.agent_logger.log_final_result(final_result)\n    return final_result\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._get_last_tool_result","title":"<code>_get_last_tool_result(messages)</code>  <code>staticmethod</code>","text":"<p>Get result of the last tool node</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@staticmethod\ndef _get_last_tool_result(messages: list[BaseMessage]):\n    \"\"\"Get result of the last tool node\"\"\"\n    for m in messages[::-1]:\n        if not isinstance(m, ToolMessage):\n            continue\n        content = json.loads(m.content)\n        return content[\"result\"]\n    return None\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._get_num_iterations","title":"<code>_get_num_iterations(state)</code>  <code>staticmethod</code>","text":"<p>Calculate iteration number Args:   state List[BaseMessage]: message history</p>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._get_num_iterations--returns","title":"Returns","text":"<p>int: the iterations number</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@staticmethod\ndef _get_num_iterations(state: list[BaseMessage]):\n    \"\"\"Calculate iteration number\n    Args:\n      state List[BaseMessage]: message history\n\n    Returns\n    -------\n      int: the iterations number\n\n    \"\"\"\n    i = 0\n    for m in state[::-1]:\n        if not isinstance(m, (ToolMessage, AIMessage)):\n            break\n        i += 1\n    return i\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._get_user_question","title":"<code>_get_user_question(state)</code>  <code>staticmethod</code>","text":"<p>Get user's question from messages array</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@staticmethod\ndef _get_user_question(state: list[BaseMessage]):\n    \"\"\"Get user's question from messages array\"\"\"\n    for m in state:\n        if not isinstance(m, HumanMessage):\n            continue\n        return m.content\n    return None\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._parse_final_result","title":"<code>_parse_final_result(messages)</code>  <code>abstractmethod</code>","text":"<p>Parse the result of the last step Args:   output BaseMessage: last step message Returns:   ReflexionAgentResult: the parsed reuslt of the last step</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@abstractmethod\ndef _parse_final_result(\n    self,\n    messages: list[BaseMessage],\n) -&gt; ReflexionAgentResult:\n    \"\"\"Parse the result of the last step\n    Args:\n      output BaseMessage: last step message\n    Returns:\n      ReflexionAgentResult: the parsed reuslt of the last step\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._should_continue","title":"<code>_should_continue(state)</code>","text":"<p>Determine if we need to continue reflexion Args:   state List[BaseMessage]: message history</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def _should_continue(self, state: list[BaseMessage]):\n    \"\"\"Determine if we need to continue reflexion\n    Args:\n      state List[BaseMessage]: message history\n    \"\"\"\n    num_steps = ReflexionAgent._get_num_iterations(state)\n    if num_steps &gt; self.max_steps:\n        return END\n    return EXECUTE_TOOL_NODE\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent._tool_function","title":"<code>_tool_function(state)</code>  <code>abstractmethod</code>","text":"<p>Tool function, execute tool based on initial draft or revised answer Args:   state List[BaseMessage]: message history Returns:   ToolMessage</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@abstractmethod\ndef _tool_function(self, state: list[BaseMessage]) -&gt; ToolMessage:\n    \"\"\"Tool function, execute tool based on initial draft or revised answer\n    Args:\n      state List[BaseMessage]: message history\n    Returns:\n      ToolMessage\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent.execute","title":"<code>execute(question, prompt=None)</code>","text":"<p>Execute ReflexionAgent. Wrapper for building a graph and executing it, returning the final answer.</p> <p>question str: user question   prompt str: user prompt</p> <p>answer str | None: If it executes successfully, an answer to the     question will be returned, otherwise, it returns None</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def execute(\n    self,\n    question: str,\n    prompt: str | None = None,\n) -&gt; ReflexionAgentResult:\n    \"\"\"Execute ReflexionAgent. Wrapper for building a graph and executing it,\n    returning the final answer.\n\n    Args:\n    ----\n      question str: user question\n      prompt str: user prompt\n\n    Returns:\n    -------\n      answer str | None: If it executes successfully, an answer to the\n        question will be returned, otherwise, it returns None\n\n    \"\"\"\n    if len(question) == 0:\n        return None\n    graph = self._build_graph(prompt)\n    return self._execute_graph(graph, question)\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger","title":"<code>ReflexionAgentLogger</code>","text":"Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>class ReflexionAgentLogger:\n    def __init__(self) -&gt; None:\n        self._logs: str = \"\"\n\n    def log_step_message(\n        self,\n        step: int,\n        node_name: str,\n        output: BaseMessage,\n    ):\n        \"\"\"Log step message\n        Args:\n          step int: step index\n          output BaseMessage: step message\n        \"\"\"\n\n    def log_final_result(self, final_result: dict[str, Any]) -&gt; None:\n        \"\"\"Log final result\n        Args:\n          output BaseMessage: last step message\n        \"\"\"\n\n    def _log_message(\n        self,\n        msg: str = \"\",\n        level: Literal[\"info\", \"error\", \"warn\"] | None = \"info\",\n    ):\n        \"\"\"Save log message\n\n        Args:\n        ----\n            msg: the message to be logged\n\n            level: the log level to write\n\n        \"\"\"\n        logger_func = logger.info if level == \"info\" else (logger.error if level == \"error\" else logger.warning)\n        logger_func(msg)\n        self._logs = self._logs + f\"[{level}]\" + f\"{datetime.now().isoformat()} - {msg}\\n\"\n\n    @property\n    def logs(self):\n        return self._logs\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger._log_message","title":"<code>_log_message(msg='', level='info')</code>","text":"<p>Save log message</p> <pre><code>msg: the message to be logged\n\nlevel: the log level to write\n</code></pre> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def _log_message(\n    self,\n    msg: str = \"\",\n    level: Literal[\"info\", \"error\", \"warn\"] | None = \"info\",\n):\n    \"\"\"Save log message\n\n    Args:\n    ----\n        msg: the message to be logged\n\n        level: the log level to write\n\n    \"\"\"\n    logger_func = logger.info if level == \"info\" else (logger.error if level == \"error\" else logger.warning)\n    logger_func(msg)\n    self._logs = self._logs + f\"[{level}]\" + f\"{datetime.now().isoformat()} - {msg}\\n\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger.log_final_result","title":"<code>log_final_result(final_result)</code>","text":"<p>Log final result Args:   output BaseMessage: last step message</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def log_final_result(self, final_result: dict[str, Any]) -&gt; None:\n    \"\"\"Log final result\n    Args:\n      output BaseMessage: last step message\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger.log_step_message","title":"<code>log_step_message(step, node_name, output)</code>","text":"<p>Log step message Args:   step int: step index   output BaseMessage: step message</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def log_step_message(\n    self,\n    step: int,\n    node_name: str,\n    output: BaseMessage,\n):\n    \"\"\"Log step message\n    Args:\n      step int: step index\n      output BaseMessage: step message\n    \"\"\"\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries","title":"<code>ResponderWithRetries</code>","text":"<p>Raise request to LLM with 3 retries</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>class ResponderWithRetries:\n    \"\"\"Raise request to LLM with 3 retries\"\"\"\n\n    def __init__(self, runnable, validator):\n        \"\"\"Args:\n        ----\n        runnable: LLM agent\n        validator: used to validate response\n\n        \"\"\"\n        self.runnable = runnable\n        self.validator = validator\n\n    @traceable\n    def respond(self, state: list[BaseMessage]):\n        \"\"\"Invoke LLM agent, this function will be called by LangGraph\n        Args:\n        state List[BaseMessage]: message history\n        \"\"\"\n        response = []\n        for attempt in range(3):\n            try:\n                response = self.runnable.invoke({\"messages\": state})\n                self.validator.invoke(response)\n                return response\n            except ValidationError as e:\n                state = state + [HumanMessage(content=repr(e))]\n        return response\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries.__init__","title":"<code>__init__(runnable, validator)</code>","text":""},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries.__init__--args","title":"Args:","text":"<p>runnable: LLM agent validator: used to validate response</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>def __init__(self, runnable, validator):\n    \"\"\"Args:\n    ----\n    runnable: LLM agent\n    validator: used to validate response\n\n    \"\"\"\n    self.runnable = runnable\n    self.validator = validator\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries.respond","title":"<code>respond(state)</code>","text":"<p>Invoke LLM agent, this function will be called by LangGraph Args: state List[BaseMessage]: message history</p> Source code in <code>biochatter/biochatter/langgraph_agent_base.py</code> <pre><code>@traceable\ndef respond(self, state: list[BaseMessage]):\n    \"\"\"Invoke LLM agent, this function will be called by LangGraph\n    Args:\n    state List[BaseMessage]: message history\n    \"\"\"\n    response = []\n    for attempt in range(3):\n        try:\n            response = self.runnable.invoke({\"messages\": state})\n            self.validator.invoke(response)\n            return response\n        except ValidationError as e:\n            state = state + [HumanMessage(content=repr(e))]\n    return response\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#the-kg-based-reflexion-agent","title":"The KG-based reflexion agent","text":""},{"location":"BioChatter/api-docs/reflexion/#biochatter.kg_langgraph_agent.GenerateQuery","title":"<code>GenerateQuery</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generate the query.</p> Source code in <code>biochatter/biochatter/kg_langgraph_agent.py</code> <pre><code>class GenerateQuery(BaseModel):\n    \"\"\"Generate the query.\"\"\"\n\n    answer: str = Field(\n        description=\"Cypher query for graph database according to user's question.\",\n    )\n    reflection: str = Field(\n        description=\"Your reflection on the initial answer, critique of what to improve\",\n    )\n    search_queries: list[str] = Field(description=SEARCH_QUERIES_DESCRIPTION)\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.kg_langgraph_agent.KGQueryReflexionAgent","title":"<code>KGQueryReflexionAgent</code>","text":"<p>               Bases: <code>ReflexionAgent</code></p> Source code in <code>biochatter/biochatter/kg_langgraph_agent.py</code> <pre><code>class KGQueryReflexionAgent(ReflexionAgent):\n    def __init__(\n        self,\n        conversation_factory: Callable,\n        connection_args: dict[str, str],\n        query_lang: str | None = \"Cypher\",\n        max_steps: int | None = 20,\n    ):\n        r\"\"\"LLM agent reflexion framework:\n\n        start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end\n                            /|\\                        |\n                             ---------------------------\n\n        Adapts base class to build and refine a knowledge graph query, default\n        language Cypher. Currently hardcoded to connect to Neo4j for the KG\n        query implementation.\n\n        Args:\n        ----\n            conversation_factory: function to return the Conversation to use for\n                the LLM connection\n\n            connection_args: connection arguments for connecting to the database\n\n            query_lang: graph query language to use\n\n            max_steps: the maximum number of steps to execute in the graph\n\n        \"\"\"\n        super().__init__(\n            conversation_factory,\n            max_steps,\n            agent_logger=KGQueryReflexionAgentLogger(),\n        )\n        self.actor_prompt_template = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    (\n                        \"As a senior biomedical researcher and graph database expert, \"\n                        f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n                        \"\"\"Current time {time}. {instruction}\"\"\"\n                    ),\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                (\n                    \"system\",\n                    (\n                        \"Note: 1. Only generate query according to the user's question above.\\n\"\n                        \"2. Please limit the results to a maximum of 30 items\"\n                    ),\n                ),\n            ],\n        ).partial(time=lambda: datetime.now().isoformat())\n        self.parser = JsonOutputToolsParser(return_id=True)\n        self.connection_args = connection_args\n        self.neodriver = None\n\n    def _connect_db(self):\n        if self.neodriver is not None:\n            return\n        try:\n            db_uri = \"bolt://\" + self.connection_args.get(\"host\") + \":\" + self.connection_args.get(\"port\")\n            self.neodriver = nu.Driver(\n                db_name=self.connection_args.get(\"db_name\") or \"neo4j\",\n                db_uri=db_uri,\n            )\n        except Exception as e:\n            logger.error(e)\n\n    def _query_graph_database(self, query: str):\n        \"\"\"Try to execute the query in Neo4j and return the result.\n\n        Args:\n        ----\n            query: the query string\n\n        \"\"\"\n        self._connect_db()\n        try:\n            return self.neodriver.query(query)\n        except Exception as e:\n            logger.error(str(e))\n            return []  # empty result\n\n    def _create_initial_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        llm: ChatOpenAI = self.conversation.chat\n        initial_chain = self.actor_prompt_template.partial(\n            instruction=prompt if prompt is not None else \"\",\n        ) | llm.bind_tools(\n            tools=[GenerateQuery],\n            tool_choice=\"GenerateQuery\",\n        )\n        validator = PydanticToolsParser(tools=[GenerateQuery])\n        return ResponderWithRetries(runnable=initial_chain, validator=validator)\n\n    def _create_revise_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        revision_instruction = \"\"\"\n        Revise your previous query using the query result and follow the guidelines:\n        1. If you consistently obtain empty results, please consider removing constraints such as relationship constraints to try to obtain a result.\n        2. You should use previous critique to improve your query.\n        3. Only generate a query without returning any other text.\n        \"\"\"\n        llm: ChatOpenAI = self.conversation.chat\n        revision_chain = self.actor_prompt_template.partial(\n            instruction=revision_instruction,\n        ) | llm.bind_tools(\n            tools=[ReviseQuery],\n            tool_choice=\"ReviseQuery\",\n        )\n        validator = PydanticToolsParser(tools=[ReviseQuery])\n        return ResponderWithRetries(\n            runnable=revision_chain,\n            validator=validator,\n        )\n\n    def _tool_function(self, state: list[BaseMessage]):\n        tool_message: AIMessage = state[-1]\n        parsed_tool_messages = self.parser.invoke(tool_message)\n        results = []\n        for parsed_message in parsed_tool_messages:\n            try:\n                parsed_args = parsed_message[\"args\"]\n                query = (\n                    parsed_args[REVISED_QUERY]\n                    if REVISED_QUERY in parsed_args\n                    else (parsed_args[REVISED_QUERY_DESCRIPTION] if REVISED_QUERY_DESCRIPTION in parsed_args else None)\n                )\n                if query is not None:\n                    result = self._query_graph_database(query)\n                    results.append({\"query\": query, \"result\": result[0]})\n                    continue\n                queries = (\n                    parsed_args[SEARCH_QUERIES]\n                    if SEARCH_QUERIES in parsed_args\n                    else parsed_args[SEARCH_QUERIES_DESCRIPTION]\n                )\n                queries = queries if len(queries) &gt; 0 else [parsed_args[ANSWER]]\n                for query in queries:\n                    result = self._query_graph_database(query)\n                    results.append(\n                        {\n                            \"query\": query,\n                            \"result\": result[0] if len(result) &gt; 0 else [],\n                        },\n                    )\n            except Exception as e:\n                logger.error(f\"Error occurred: {e!s}\")\n\n        content = None\n        if len(results) &gt; 1:\n            # If there are multiple results, we only return\n            # the first non-empty result\n            for res in results:\n                if res[\"result\"] and len(res[\"result\"]) &gt; 0:\n                    content = json.dumps(res)\n        if content is None:\n            content = json.dumps(results[0]) if len(results) &gt; 0 else \"\"\n        return ToolMessage(\n            content=content,\n            tool_call_id=parsed_message[\"id\"],\n        )\n\n    @staticmethod\n    def _get_last_tool_results_num(state: list[BaseMessage]):\n        i = 0\n        for m in state[::-1]:\n            if not isinstance(m, ToolMessage):\n                continue\n            message: ToolMessage = m\n            logger.info(f\"query result: {message.content}\")\n            results = (\n                json.loads(message.content)\n                if message.content is not None and len(message.content) &gt; 0\n                else {\"result\": []}\n            )\n            empty = True\n            if len(results[\"result\"]) &gt; 0:\n                # check if it is really not empty, remove the case: {\"result\": [{\"c.name\": None}]}\n                for res in results[\"result\"]:\n                    for k in res.keys():\n                        if res[k] is None:\n                            continue\n                        if isinstance(res[k], str) and (res[k] == \"None\" or res[k] == \"null\"):\n                            continue\n                        empty = False\n                        break\n                    if not empty:\n                        break\n            return len(results[\"result\"]) if not empty else 0\n\n        return 0\n\n    def _get_last_score(self, state: list[BaseMessage]) -&gt; int | None:\n        for m in state[::-1]:\n            if not isinstance(m, AIMessage):\n                continue\n            message: AIMessage = m\n            parsed_msg = self.parser.invoke(message)\n            try:\n                score = parsed_msg[0][\"args\"][\"score\"]\n                return int(score)\n            except Exception:\n                return None\n        return None\n\n    def _should_continue(self, state: list[BaseMessage]):\n        res = super()._should_continue(state)\n        if res == END:\n            return res\n        score = self._get_last_score(state)\n        if score is not None and score &gt;= 7:\n            return END\n        query_results_num = KGQueryReflexionAgent._get_last_tool_results_num(\n            state,\n        )\n        return END if query_results_num &gt; 0 else EXECUTE_TOOL_NODE\n\n    def _parse_final_result(\n        self,\n        messages: list[BaseMessage],\n    ) -&gt; ReflexionAgentResult:\n        output = messages[-1]\n        result = self.parser.invoke(output)[0][\"args\"]\n        tool_result = ReflexionAgent._get_last_tool_result(messages)\n        return ReflexionAgentResult(\n            answer=result[\"answer\"] if \"answer\" in result else None,\n            tool_result=tool_result,\n        )\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.kg_langgraph_agent.KGQueryReflexionAgent.__init__","title":"<code>__init__(conversation_factory, connection_args, query_lang='Cypher', max_steps=20)</code>","text":"<p>LLM agent reflexion framework:</p> <p>start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end                     /|\\                        |                      ---------------------------</p> <p>Adapts base class to build and refine a knowledge graph query, default language Cypher. Currently hardcoded to connect to Neo4j for the KG query implementation.</p> <pre><code>conversation_factory: function to return the Conversation to use for\n    the LLM connection\n\nconnection_args: connection arguments for connecting to the database\n\nquery_lang: graph query language to use\n\nmax_steps: the maximum number of steps to execute in the graph\n</code></pre> Source code in <code>biochatter/biochatter/kg_langgraph_agent.py</code> <pre><code>def __init__(\n    self,\n    conversation_factory: Callable,\n    connection_args: dict[str, str],\n    query_lang: str | None = \"Cypher\",\n    max_steps: int | None = 20,\n):\n    r\"\"\"LLM agent reflexion framework:\n\n    start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end\n                        /|\\                        |\n                         ---------------------------\n\n    Adapts base class to build and refine a knowledge graph query, default\n    language Cypher. Currently hardcoded to connect to Neo4j for the KG\n    query implementation.\n\n    Args:\n    ----\n        conversation_factory: function to return the Conversation to use for\n            the LLM connection\n\n        connection_args: connection arguments for connecting to the database\n\n        query_lang: graph query language to use\n\n        max_steps: the maximum number of steps to execute in the graph\n\n    \"\"\"\n    super().__init__(\n        conversation_factory,\n        max_steps,\n        agent_logger=KGQueryReflexionAgentLogger(),\n    )\n    self.actor_prompt_template = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                (\n                    \"As a senior biomedical researcher and graph database expert, \"\n                    f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n                    \"\"\"Current time {time}. {instruction}\"\"\"\n                ),\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            (\n                \"system\",\n                (\n                    \"Note: 1. Only generate query according to the user's question above.\\n\"\n                    \"2. Please limit the results to a maximum of 30 items\"\n                ),\n            ),\n        ],\n    ).partial(time=lambda: datetime.now().isoformat())\n    self.parser = JsonOutputToolsParser(return_id=True)\n    self.connection_args = connection_args\n    self.neodriver = None\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.kg_langgraph_agent.KGQueryReflexionAgent._query_graph_database","title":"<code>_query_graph_database(query)</code>","text":"<p>Try to execute the query in Neo4j and return the result.</p> <pre><code>query: the query string\n</code></pre> Source code in <code>biochatter/biochatter/kg_langgraph_agent.py</code> <pre><code>def _query_graph_database(self, query: str):\n    \"\"\"Try to execute the query in Neo4j and return the result.\n\n    Args:\n    ----\n        query: the query string\n\n    \"\"\"\n    self._connect_db()\n    try:\n        return self.neodriver.query(query)\n    except Exception as e:\n        logger.error(str(e))\n        return []  # empty result\n</code></pre>"},{"location":"BioChatter/api-docs/reflexion/#biochatter.kg_langgraph_agent.ReviseQuery","title":"<code>ReviseQuery</code>","text":"<p>               Bases: <code>GenerateQuery</code></p> <p>Revise your previous query according to your question.</p> Source code in <code>biochatter/biochatter/kg_langgraph_agent.py</code> <pre><code>class ReviseQuery(GenerateQuery):\n    \"\"\"Revise your previous query according to your question.\"\"\"\n\n    revised_query: str = Field(description=REVISED_QUERY_DESCRIPTION)\n    score: str = Field(description=SCORE_DESCRIPTION)\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/","title":"Vectorstore Agent Reference","text":"<p>Here we handle the application of vectorstore services to retrieval-augmented generation tasks by embedding documents and connections/management of vectorstore services and semantic search.</p>"},{"location":"BioChatter/api-docs/vectorstore/#vectorstore-implementation","title":"Vectorstore Implementation","text":"<p>Module for handling document embedding, storage and retrieval.</p> <p>This module provides classes for splitting documents into chunks, embedding them using various LLM providers (OpenAI, Xinference, Ollama), storing them in vector databases, and retrieving relevant passages through similarity search.</p>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder","title":"<code>DocumentEmbedder</code>","text":"<p>Handle retrieval-augmented generation (RAG) functionality of BioChatter.</p> <p>This class is responsible for: - Splitting text documents into manageable chunks - Embedding these chunks using various LLM providers - Storing embeddings in vector databases - Performing similarity searches for retrieval</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>class DocumentEmbedder:\n    \"\"\"Handle retrieval-augmented generation (RAG) functionality of BioChatter.\n\n    This class is responsible for:\n    - Splitting text documents into manageable chunks\n    - Embedding these chunks using various LLM providers\n    - Storing embeddings in vector databases\n    - Performing similarity searches for retrieval\n    \"\"\"\n\n    def __init__(\n        self,\n        used: bool = False,\n        online: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: list | None = None,\n        n_results: int = 3,\n        model: str | None = \"text-embedding-ada-002\",\n        vector_db_vendor: str | None = None,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n        base_url: str | None = None,\n        embeddings: OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings | AzureOpenAIEmbeddings | None = None,\n        documentids_workspace: list[str] | None = None,\n    ) -&gt; None:\n        r\"\"\"Initialize the DocumentEmbedder with the specified configuration.\n\n        Args:\n        ----\n            used (bool, optional): whether RAG has been used (frontend setting).\n                Defaults to False.\n\n            online (bool, optional): whether we are running the frontend online.\n                Defaults to False.\n\n            chunk_size (int, optional): size of chunks to split text into.\n                Defaults to 1000.\n\n            chunk_overlap (int, optional): overlap between chunks.\n                Defaults to 0.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens. Defaults to True.\n\n            separators (Optional[list], optional): list of separators to use\n                when splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n            n_results (int, optional): number of results to return from\n                similarity search. Defaults to 3.\n\n            model (Optional[str], optional): name of model to use for\n                embeddings. Defaults to 'text-embedding-ada-002'.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use. Defaults to Milvus.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection. Defaults to None.\n\n            base_url (Optional[str], optional): base url of OpenAI API.\n\n            embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n                optional): Embeddings object to use. Defaults to OpenAI.\n\n            documentids_workspace (Optional[List[str]], optional): a list of\n                document IDs that defines the scope within which RAG operations\n                (remove, similarity search, and get all) occur. Defaults to\n                None, which means the operations will be performed across all\n                documents in the database.\n\n        \"\"\"\n        self.used = used\n        self.online = online\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators or [\" \", \",\", \"\\n\"]\n        self.n_results = n_results\n        self.split_by_characters = split_by_characters\n        self.model_name = model\n\n        # TODO API Key handling to central config?\n        if base_url:\n            openai.api_base = base_url\n\n        self.embeddings = embeddings\n\n        # connection arguments\n        self.connection_args = connection_args or {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"19530\",\n        }\n        self.embedding_collection_name = embedding_collection_name\n        self.metadata_collection_name = metadata_collection_name\n        self.documentids_workspace = documentids_workspace\n\n        # TODO: vector db selection\n        self.vector_db_vendor = vector_db_vendor or \"milvus\"\n        # instantiate VectorDatabaseHost\n        self.database_host = None\n        self._init_database_host()\n\n    def _set_embeddings(\n        self,\n        embeddings: (OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings | AzureOpenAIEmbeddings),\n    ) -&gt; None:\n        print(\"setting embedder\")\n        self.embeddings = embeddings\n\n    def _init_database_host(self) -&gt; None:\n        if self.vector_db_vendor == \"milvus\":\n            self.database_host = VectorDatabaseAgentMilvus(\n                embedding_func=self.embeddings,\n                connection_args=self.connection_args,\n                embedding_collection_name=self.embedding_collection_name,\n                metadata_collection_name=self.metadata_collection_name,\n            )\n        else:\n            raise NotImplementedError(self.vector_db_vendor)\n\n    def set_chunk_size(self, chunk_size: int) -&gt; None:\n        \"\"\"Set the chunk size for the text splitter.\"\"\"\n        self.chunk_size = chunk_size\n\n    def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n        \"\"\"Set the chunk overlap for the text splitter.\"\"\"\n        self.chunk_overlap = chunk_overlap\n\n    def set_separators(self, separators: list) -&gt; None:\n        \"\"\"Set the separators for the text splitter.\"\"\"\n        self.separators = separators\n\n    def _characters_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _tokens_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n        HUGGINGFACE_MODELS = [\"bigscience/bloom\"]\n        if self.model_name and self.model_name in HUGGINGFACE_MODELS:\n            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n            return RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n                tokenizer,\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n\n        return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            encoding_name=\"\",\n            model_name=(DEFAULT_OPENAI_MODEL if not self.model_name else self.model_name),\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _text_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return self._characters_splitter() if self.split_by_characters else self._tokens_splitter()\n\n    def save_document(self, doc: list[Document]) -&gt; str:\n        \"\"\"Save a list of documents to the vector database.\n\n        Args:\n        ----\n            doc (List[Document]): document content, read with `DocumentReader`\n                functions `load_document()`, `document_from_pdf()`, or\n                `document_from_txt()`\n\n        Returns:\n        -------\n            str: document id, which can be used to remove an uploaded document\n                with `remove_document()`\n\n        \"\"\"\n        splitted = self._split_document(doc)\n        return self._store_embeddings(splitted)\n\n    def _split_document(self, document: list[Document]) -&gt; list[Document]:\n        \"\"\"Split a document into chunks.\"\"\"\n        text_splitter = self._text_splitter()\n        return text_splitter.split_documents(document)\n\n    def _store_embeddings(self, doc: list[Document]) -&gt; str:\n        \"\"\"Store embeddings for a list of documents.\"\"\"\n        return self.database_host.store_embeddings(documents=doc)\n\n    def connect(self) -&gt; None:\n        \"\"\"Connect to the vector database.\"\"\"\n        self.database_host.connect()\n\n    def get_all_documents(self) -&gt; list[dict]:\n        \"\"\"Get all documents from the vector database.\"\"\"\n        return self.database_host.get_all_documents(\n            doc_ids=self.documentids_workspace,\n        )\n\n    def remove_document(self, doc_id: str) -&gt; None:\n        \"\"\"Remove a document from the vector database.\"\"\"\n        return self.database_host.remove_document(\n            doc_id,\n            self.documentids_workspace,\n        )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.__init__","title":"<code>__init__(used=False, online=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='text-embedding-ada-002', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, base_url=None, embeddings=None, documentids_workspace=None)</code>","text":"<p>Initialize the DocumentEmbedder with the specified configuration.</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n    Defaults to False.\n\nonline (bool, optional): whether we are running the frontend online.\n    Defaults to False.\n\nchunk_size (int, optional): size of chunks to split text into.\n    Defaults to 1000.\n\nchunk_overlap (int, optional): overlap between chunks.\n    Defaults to 0.\n\nsplit_by_characters (bool, optional): whether to split by characters\n    or tokens. Defaults to True.\n\nseparators (Optional[list], optional): list of separators to use\n    when splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\nn_results (int, optional): number of results to return from\n    similarity search. Defaults to 3.\n\nmodel (Optional[str], optional): name of model to use for\n    embeddings. Defaults to 'text-embedding-ada-002'.\n\nvector_db_vendor (Optional[str], optional): name of vector database\n    to use. Defaults to Milvus.\n\nconnection_args (Optional[dict], optional): arguments to pass to\n    vector database connection. Defaults to None.\n\nbase_url (Optional[str], optional): base url of OpenAI API.\n\nembeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n    optional): Embeddings object to use. Defaults to OpenAI.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of\n    document IDs that defines the scope within which RAG operations\n    (remove, similarity search, and get all) occur. Defaults to\n    None, which means the operations will be performed across all\n    documents in the database.\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    online: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: list | None = None,\n    n_results: int = 3,\n    model: str | None = \"text-embedding-ada-002\",\n    vector_db_vendor: str | None = None,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n    base_url: str | None = None,\n    embeddings: OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings | AzureOpenAIEmbeddings | None = None,\n    documentids_workspace: list[str] | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the DocumentEmbedder with the specified configuration.\n\n    Args:\n    ----\n        used (bool, optional): whether RAG has been used (frontend setting).\n            Defaults to False.\n\n        online (bool, optional): whether we are running the frontend online.\n            Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks.\n            Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use\n            when splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n        n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for\n            embeddings. Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        documentids_workspace (Optional[List[str]], optional): a list of\n            document IDs that defines the scope within which RAG operations\n            (remove, similarity search, and get all) occur. Defaults to\n            None, which means the operations will be performed across all\n            documents in the database.\n\n    \"\"\"\n    self.used = used\n    self.online = online\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.separators = separators or [\" \", \",\", \"\\n\"]\n    self.n_results = n_results\n    self.split_by_characters = split_by_characters\n    self.model_name = model\n\n    # TODO API Key handling to central config?\n    if base_url:\n        openai.api_base = base_url\n\n    self.embeddings = embeddings\n\n    # connection arguments\n    self.connection_args = connection_args or {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n    }\n    self.embedding_collection_name = embedding_collection_name\n    self.metadata_collection_name = metadata_collection_name\n    self.documentids_workspace = documentids_workspace\n\n    # TODO: vector db selection\n    self.vector_db_vendor = vector_db_vendor or \"milvus\"\n    # instantiate VectorDatabaseHost\n    self.database_host = None\n    self._init_database_host()\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder._split_document","title":"<code>_split_document(document)</code>","text":"<p>Split a document into chunks.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def _split_document(self, document: list[Document]) -&gt; list[Document]:\n    \"\"\"Split a document into chunks.\"\"\"\n    text_splitter = self._text_splitter()\n    return text_splitter.split_documents(document)\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder._store_embeddings","title":"<code>_store_embeddings(doc)</code>","text":"<p>Store embeddings for a list of documents.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def _store_embeddings(self, doc: list[Document]) -&gt; str:\n    \"\"\"Store embeddings for a list of documents.\"\"\"\n    return self.database_host.store_embeddings(documents=doc)\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.connect","title":"<code>connect()</code>","text":"<p>Connect to the vector database.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to the vector database.\"\"\"\n    self.database_host.connect()\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.get_all_documents","title":"<code>get_all_documents()</code>","text":"<p>Get all documents from the vector database.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def get_all_documents(self) -&gt; list[dict]:\n    \"\"\"Get all documents from the vector database.\"\"\"\n    return self.database_host.get_all_documents(\n        doc_ids=self.documentids_workspace,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.remove_document","title":"<code>remove_document(doc_id)</code>","text":"<p>Remove a document from the vector database.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def remove_document(self, doc_id: str) -&gt; None:\n    \"\"\"Remove a document from the vector database.\"\"\"\n    return self.database_host.remove_document(\n        doc_id,\n        self.documentids_workspace,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.save_document","title":"<code>save_document(doc)</code>","text":"<p>Save a list of documents to the vector database.</p> <pre><code>doc (List[Document]): document content, read with `DocumentReader`\n    functions `load_document()`, `document_from_pdf()`, or\n    `document_from_txt()`\n</code></pre> <pre><code>str: document id, which can be used to remove an uploaded document\n    with `remove_document()`\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def save_document(self, doc: list[Document]) -&gt; str:\n    \"\"\"Save a list of documents to the vector database.\n\n    Args:\n    ----\n        doc (List[Document]): document content, read with `DocumentReader`\n            functions `load_document()`, `document_from_pdf()`, or\n            `document_from_txt()`\n\n    Returns:\n    -------\n        str: document id, which can be used to remove an uploaded document\n            with `remove_document()`\n\n    \"\"\"\n    splitted = self._split_document(doc)\n    return self._store_embeddings(splitted)\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.set_chunk_overlap","title":"<code>set_chunk_overlap(chunk_overlap)</code>","text":"<p>Set the chunk overlap for the text splitter.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n    \"\"\"Set the chunk overlap for the text splitter.\"\"\"\n    self.chunk_overlap = chunk_overlap\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.set_chunk_size","title":"<code>set_chunk_size(chunk_size)</code>","text":"<p>Set the chunk size for the text splitter.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def set_chunk_size(self, chunk_size: int) -&gt; None:\n    \"\"\"Set the chunk size for the text splitter.\"\"\"\n    self.chunk_size = chunk_size\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.set_separators","title":"<code>set_separators(separators)</code>","text":"<p>Set the separators for the text splitter.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def set_separators(self, separators: list) -&gt; None:\n    \"\"\"Set the separators for the text splitter.\"\"\"\n    self.separators = separators\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentReader","title":"<code>DocumentReader</code>","text":"<p>Class for reading documents from various sources.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>class DocumentReader:\n    \"\"\"Class for reading documents from various sources.\"\"\"\n\n    def load_document(self, path: str) -&gt; list[Document]:\n        \"\"\"Load a document from a path; accept txt and pdf files.\n\n        Txt files are loaded as-is, pdf files are converted to text using\n        `fitz`.\n\n        Args:\n        ----\n            path (str): path to document\n\n        Returns:\n        -------\n            List[Document]: list of documents\n\n        Raises:\n        ------\n            ValueError: If file extension is not supported\n\n        \"\"\"\n        if path.endswith(\".txt\"):\n            loader = TextLoader(path)\n            return loader.load()\n\n        if path.endswith(\".pdf\"):\n            doc = fitz.open(path)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n\n            meta = {k: v for k, v in doc.metadata.items() if v}\n            meta.update({\"source\": path})\n\n            return [\n                Document(\n                    page_content=text,\n                    metadata=meta,\n                ),\n            ]\n\n        err_msg = f\"Unsupported file extension in {path}. File must be .txt or .pdf\"\n        raise ValueError(err_msg)\n\n    def document_from_pdf(self, pdf: bytes) -&gt; list[Document]:\n        \"\"\"Return a list of Documents from a pdf file byte representation.\n\n        Receive a byte representation of a pdf file and return a list of\n        Documents with metadata.\n\n        Args:\n        ----\n            pdf (bytes): byte representation of pdf file\n\n        Returns:\n        -------\n            List[Document]: list of documents\n\n        \"\"\"\n        doc = fitz.open(stream=pdf, filetype=\"pdf\")\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": \"pdf\"})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            ),\n        ]\n\n    def document_from_txt(self, txt: bytes) -&gt; list[Document]:\n        \"\"\"Return a list of Documents from a txt file byte representation.\n\n        Receive a byte representation of a txt file and return a list of\n        Documents with metadata.\n\n        Args:\n        ----\n            txt (bytes): byte representation of txt file\n\n        Returns:\n        -------\n            List[Document]: list of documents\n\n        \"\"\"\n        meta = {\"source\": \"txt\"}\n        return [\n            Document(\n                page_content=txt,\n                metadata=meta,\n            ),\n        ]\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentReader.document_from_pdf","title":"<code>document_from_pdf(pdf)</code>","text":"<p>Return a list of Documents from a pdf file byte representation.</p> <p>Receive a byte representation of a pdf file and return a list of Documents with metadata.</p> <pre><code>pdf (bytes): byte representation of pdf file\n</code></pre> <pre><code>List[Document]: list of documents\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def document_from_pdf(self, pdf: bytes) -&gt; list[Document]:\n    \"\"\"Return a list of Documents from a pdf file byte representation.\n\n    Receive a byte representation of a pdf file and return a list of\n    Documents with metadata.\n\n    Args:\n    ----\n        pdf (bytes): byte representation of pdf file\n\n    Returns:\n    -------\n        List[Document]: list of documents\n\n    \"\"\"\n    doc = fitz.open(stream=pdf, filetype=\"pdf\")\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n\n    meta = {k: v for k, v in doc.metadata.items() if v}\n    meta.update({\"source\": \"pdf\"})\n\n    return [\n        Document(\n            page_content=text,\n            metadata=meta,\n        ),\n    ]\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentReader.document_from_txt","title":"<code>document_from_txt(txt)</code>","text":"<p>Return a list of Documents from a txt file byte representation.</p> <p>Receive a byte representation of a txt file and return a list of Documents with metadata.</p> <pre><code>txt (bytes): byte representation of txt file\n</code></pre> <pre><code>List[Document]: list of documents\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def document_from_txt(self, txt: bytes) -&gt; list[Document]:\n    \"\"\"Return a list of Documents from a txt file byte representation.\n\n    Receive a byte representation of a txt file and return a list of\n    Documents with metadata.\n\n    Args:\n    ----\n        txt (bytes): byte representation of txt file\n\n    Returns:\n    -------\n        List[Document]: list of documents\n\n    \"\"\"\n    meta = {\"source\": \"txt\"}\n    return [\n        Document(\n            page_content=txt,\n            metadata=meta,\n        ),\n    ]\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.DocumentReader.load_document","title":"<code>load_document(path)</code>","text":"<p>Load a document from a path; accept txt and pdf files.</p> <p>Txt files are loaded as-is, pdf files are converted to text using <code>fitz</code>.</p> <pre><code>path (str): path to document\n</code></pre> <pre><code>List[Document]: list of documents\n</code></pre> <pre><code>ValueError: If file extension is not supported\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def load_document(self, path: str) -&gt; list[Document]:\n    \"\"\"Load a document from a path; accept txt and pdf files.\n\n    Txt files are loaded as-is, pdf files are converted to text using\n    `fitz`.\n\n    Args:\n    ----\n        path (str): path to document\n\n    Returns:\n    -------\n        List[Document]: list of documents\n\n    Raises:\n    ------\n        ValueError: If file extension is not supported\n\n    \"\"\"\n    if path.endswith(\".txt\"):\n        loader = TextLoader(path)\n        return loader.load()\n\n    if path.endswith(\".pdf\"):\n        doc = fitz.open(path)\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": path})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            ),\n        ]\n\n    err_msg = f\"Unsupported file extension in {path}. File must be .txt or .pdf\"\n    raise ValueError(err_msg)\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.OllamaDocumentEmbedder","title":"<code>OllamaDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Extension of the DocumentEmbedder class to Ollama.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>class OllamaDocumentEmbedder(DocumentEmbedder):\n    \"\"\"Extension of the DocumentEmbedder class to Ollama.\"\"\"\n\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: list | None = None,\n        n_results: int = 3,\n        model: str | None = \"nomic-embed-text\",\n        vector_db_vendor: str | None = None,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n        base_url: str | None = None,\n        documentids_workspace: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize with the specified configuration.\n\n        Args:\n        ----\n            used (bool, optional): whether RAG has been used (frontend setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens.\n\n            separators (Optional[list], optional): list of separators to use\n                when splitting by characters.\n\n            n_results (int, optional): number of results to return from\n                similarity search.\n\n            model (Optional[str], optional): name of model to use for\n                embeddings. Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n                collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n                collection to store metadata in.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of\n                document IDs that defines the scope within which RAG operations\n                (remove, similarity search, and get all) occur. Defaults to\n                None, which means the operations will be performed across all\n                documents in the database.\n\n        \"\"\"\n        from langchain_community.embeddings import OllamaEmbeddings\n\n        self.model_name = model\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            base_url=base_url,\n            embeddings=OllamaEmbeddings(\n                base_url=base_url,\n                model=self.model_name,\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.OllamaDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='nomic-embed-text', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, base_url=None, documentids_workspace=None)</code>","text":"<p>Initialize with the specified configuration.</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\n    or tokens.\n\nseparators (Optional[list], optional): list of separators to use\n    when splitting by characters.\n\nn_results (int, optional): number of results to return from\n    similarity search.\n\nmodel (Optional[str], optional): name of model to use for\n    embeddings. Can be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\n    to use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\n    vector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\n    collection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\n    collection to store metadata in.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of\n    document IDs that defines the scope within which RAG operations\n    (remove, similarity search, and get all) occur. Defaults to\n    None, which means the operations will be performed across all\n    documents in the database.\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: list | None = None,\n    n_results: int = 3,\n    model: str | None = \"nomic-embed-text\",\n    vector_db_vendor: str | None = None,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n    base_url: str | None = None,\n    documentids_workspace: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize with the specified configuration.\n\n    Args:\n    ----\n        used (bool, optional): whether RAG has been used (frontend setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n        separators (Optional[list], optional): list of separators to use\n            when splitting by characters.\n\n        n_results (int, optional): number of results to return from\n            similarity search.\n\n        model (Optional[str], optional): name of model to use for\n            embeddings. Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of\n            document IDs that defines the scope within which RAG operations\n            (remove, similarity search, and get all) occur. Defaults to\n            None, which means the operations will be performed across all\n            documents in the database.\n\n    \"\"\"\n    from langchain_community.embeddings import OllamaEmbeddings\n\n    self.model_name = model\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        base_url=base_url,\n        embeddings=OllamaEmbeddings(\n            base_url=base_url,\n            model=self.model_name,\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder","title":"<code>XinferenceDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Extension of the DocumentEmbedder class to Xinference.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>class XinferenceDocumentEmbedder(DocumentEmbedder):\n    \"\"\"Extension of the DocumentEmbedder class to Xinference.\"\"\"\n\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: list | None = None,\n        n_results: int = 3,\n        model: str | None = \"auto\",\n        vector_db_vendor: str | None = None,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n        base_url: str | None = None,\n        documentids_workspace: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize with the specified configuration.\n\n        Args:\n        ----\n            used (bool, optional): whether RAG has been used (frontend setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens.\n\n            separators (Optional[list], optional): list of separators to use\n                when splitting by characters.\n\n            n_results (int, optional): number of results to return from\n                similarity search.\n\n            model (Optional[str], optional): name of model to use for\n                embeddings. Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n                collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n                collection to store metadata in.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of\n                document IDs that defines the scope within which RAG operations\n                (remove, similarity search, and get all) occur. Defaults to\n                None, which means the operations will be performed across all\n                documents in the database.\n\n        \"\"\"\n        from xinference.client import Client\n\n        self.model_name = model\n        self.client = Client(base_url=base_url)\n        self.models = {}\n        self.load_models()\n\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"embedding\")[0]\n        self.model_uid = self.models[self.model_name][\"id\"]\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            base_url=base_url,\n            embeddings=XinferenceEmbeddings(\n                server_url=base_url,\n                model_uid=self.model_uid,\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n\n    def load_models(self) -&gt; None:\n        \"\"\"Get all models that are currently available.\n\n        Connect to the Xinference server and write the running models to\n        `self.models`.\n        \"\"\"\n        for _id, model in self.client.list_models().items():\n            model[\"id\"] = _id\n            self.models[model[\"model_name\"]] = model\n\n    def list_models_by_type(self, model_type: str) -&gt; list[str]:\n        \"\"\"Return all models of a certain type.\n\n        Connect to the Xinference server and return all models of a certain\n        type.\n\n        Args:\n        ----\n            model_type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n        Returns:\n        -------\n            List[str]: list of model names\n\n        \"\"\"\n        names = []\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if model_type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == model_type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='auto', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, base_url=None, documentids_workspace=None)</code>","text":"<p>Initialize with the specified configuration.</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\n    or tokens.\n\nseparators (Optional[list], optional): list of separators to use\n    when splitting by characters.\n\nn_results (int, optional): number of results to return from\n    similarity search.\n\nmodel (Optional[str], optional): name of model to use for\n    embeddings. Can be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\n    to use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\n    vector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\n    collection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\n    collection to store metadata in.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of\n    document IDs that defines the scope within which RAG operations\n    (remove, similarity search, and get all) occur. Defaults to\n    None, which means the operations will be performed across all\n    documents in the database.\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: list | None = None,\n    n_results: int = 3,\n    model: str | None = \"auto\",\n    vector_db_vendor: str | None = None,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n    base_url: str | None = None,\n    documentids_workspace: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize with the specified configuration.\n\n    Args:\n    ----\n        used (bool, optional): whether RAG has been used (frontend setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n        separators (Optional[list], optional): list of separators to use\n            when splitting by characters.\n\n        n_results (int, optional): number of results to return from\n            similarity search.\n\n        model (Optional[str], optional): name of model to use for\n            embeddings. Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of\n            document IDs that defines the scope within which RAG operations\n            (remove, similarity search, and get all) occur. Defaults to\n            None, which means the operations will be performed across all\n            documents in the database.\n\n    \"\"\"\n    from xinference.client import Client\n\n    self.model_name = model\n    self.client = Client(base_url=base_url)\n    self.models = {}\n    self.load_models()\n\n    if self.model_name is None or self.model_name == \"auto\":\n        self.model_name = self.list_models_by_type(\"embedding\")[0]\n    self.model_uid = self.models[self.model_name][\"id\"]\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        base_url=base_url,\n        embeddings=XinferenceEmbeddings(\n            server_url=base_url,\n            model_uid=self.model_uid,\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder.list_models_by_type","title":"<code>list_models_by_type(model_type)</code>","text":"<p>Return all models of a certain type.</p> <p>Connect to the Xinference server and return all models of a certain type.</p> <pre><code>model_type (str): type of model to list (e.g. \"embedding\", \"chat\")\n</code></pre> <pre><code>List[str]: list of model names\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def list_models_by_type(self, model_type: str) -&gt; list[str]:\n    \"\"\"Return all models of a certain type.\n\n    Connect to the Xinference server and return all models of a certain\n    type.\n\n    Args:\n    ----\n        model_type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n    Returns:\n    -------\n        List[str]: list of model names\n\n    \"\"\"\n    names = []\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if model_type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == model_type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder.load_models","title":"<code>load_models()</code>","text":"<p>Get all models that are currently available.</p> <p>Connect to the Xinference server and write the running models to <code>self.models</code>.</p> Source code in <code>biochatter/biochatter/vectorstore.py</code> <pre><code>def load_models(self) -&gt; None:\n    \"\"\"Get all models that are currently available.\n\n    Connect to the Xinference server and write the running models to\n    `self.models`.\n    \"\"\"\n    for _id, model in self.client.list_models().items():\n        model[\"id\"] = _id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#vectorstore-agent","title":"Vectorstore Agent","text":""},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus","title":"<code>VectorDatabaseAgentMilvus</code>","text":"<p>The VectorDatabaseAgentMilvus class manages vector databases in a connected host database. It manages an embedding collection <code>_col_embeddings:langchain.vectorstores.Milvus</code>, which is the main information on the embedded text fragments and the basis for similarity search, and a metadata collection <code>_col_metadata:pymilvus.Collection</code>, which stores the metadata of the embedded text fragments. A typical workflow includes the following operations:</p> <ol> <li>connect to a host using <code>connect()</code></li> <li>get all documents in the active database using <code>get_all_documents()</code></li> <li>save a number of fragments, usually from a specific document, using     <code>store_embeddings()</code></li> <li>do similarity search among all fragments of the currently active database     using <code>similarity_search()</code></li> <li>remove a document from the currently active database using     <code>remove_document()</code></li> </ol> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>class VectorDatabaseAgentMilvus:\n    \"\"\"The VectorDatabaseAgentMilvus class manages vector databases in a connected\n    host database. It manages an embedding collection\n    `_col_embeddings:langchain.vectorstores.Milvus`, which is the main\n    information on the embedded text fragments and the basis for similarity\n    search, and a metadata collection `_col_metadata:pymilvus.Collection`, which\n    stores the metadata of the embedded text fragments. A typical workflow\n    includes the following operations:\n\n    1. connect to a host using `connect()`\n    2. get all documents in the active database using `get_all_documents()`\n    3. save a number of fragments, usually from a specific document, using\n        `store_embeddings()`\n    4. do similarity search among all fragments of the currently active database\n        using `similarity_search()`\n    5. remove a document from the currently active database using\n        `remove_document()`\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_func: OpenAIEmbeddings,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n    ):\n        \"\"\"Args:\n        ----\n            embedding_func OpenAIEmbeddings: Function used to embed the text\n\n            connection_args Optional dict: args to connect Vector Database\n\n            embedding_collection_name Optional str: exposed for test\n\n            metadata_collection_name Optional str: exposed for test\n\n        \"\"\"\n        self._embedding_func = embedding_func\n        self._col_embeddings: Milvus | None = None\n        self._col_metadata: Collection | None = None\n        self._connection_args = validate_connection_args(connection_args)\n        self._embedding_name = embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n        self._metadata_name = metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n\n    def connect(self) -&gt; None:\n        \"\"\"Connect to a host and read two document collections (the default names\n        are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n        database (default database name is `default`); if those document\n        collections don't exist, create the two collections.\n        \"\"\"\n        self._connect(**self._connection_args)\n        self._init_host()\n\n    def _connect(self, host: str, port: str, user: str, password: str) -&gt; None:\n        self.alias = self._create_connection_alias(host, port, user, password)\n\n    def _init_host(self) -&gt; None:\n        \"\"\"Initialize host. Will read/create document collection inside currently\n        active database.\n        \"\"\"\n        self._create_collections()\n\n    def _create_connection_alias(\n        self,\n        host: str,\n        port: str,\n        user: str,\n        password: str,\n    ) -&gt; str:\n        \"\"\"Connect to host and create a connection alias for metadata collection\n        using a random uuid.\n\n        Args:\n        ----\n            host (str): host ip address\n            port (str): host port\n\n        Returns:\n        -------\n            str: connection alias\n\n        \"\"\"\n        alias = uuid.uuid4().hex\n        try:\n            connections.connect(\n                host=host,\n                port=port,\n                user=user,\n                password=password,\n                alias=alias,\n            )\n            logger.debug(f\"Created new connection using: {alias}\")\n            return alias\n        except MilvusException as e:\n            logger.error(f\"Failed to create  new connection using: {alias}\")\n            raise e\n\n    def _create_collections(self) -&gt; None:\n        \"\"\"Create or load the embedding and metadata collections from the currently\n        active database.\n        \"\"\"\n        embedding_exists = utility.has_collection(\n            self._embedding_name,\n            using=self.alias,\n        )\n        meta_exists = utility.has_collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n\n        if embedding_exists:\n            self._load_embeddings_collection()\n        else:\n            self._create_embeddings_collection()\n\n        if meta_exists:\n            self._load_metadata_collection()\n        else:\n            self._create_metadata_collection()\n\n        self._create_metadata_collection_index()\n        self._col_metadata.load()\n\n    def _load_embeddings_collection(self) -&gt; None:\n        \"\"\"Load embeddings collection from currently active database.\"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to load embeddings collection {self._embedding_name}.\",\n            )\n            raise e\n\n    def _create_embeddings_collection(self) -&gt; None:\n        \"\"\"Create embedding collection.\n        All fields: \"meta_id\", \"vector\"\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to create embeddings collection {self._embedding_name}\",\n            )\n            raise e\n\n    def _load_metadata_collection(self) -&gt; None:\n        \"\"\"Load metadata collection from currently active database.\"\"\"\n        self._col_metadata = Collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n        self._col_metadata.load()\n\n    def _create_metadata_collection(self) -&gt; None:\n        \"\"\"Create metadata collection.\n\n        All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\",\n        \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\",\n        \"isDeleted\".\n\n        As the vector database requires a vector field, we will create a fake\n        vector \"embedding\". The field \"isDeleted\" is used to specify if the\n        document is deleted.\n        \"\"\"\n        MAX_LENGTH = 10000\n        doc_id = FieldSchema(\n            name=\"id\",\n            dtype=DataType.INT64,\n            is_primary=True,\n            auto_id=True,\n        )\n        doc_name = FieldSchema(\n            name=\"name\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_author = FieldSchema(\n            name=\"author\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_title = FieldSchema(\n            name=\"title\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_format = FieldSchema(\n            name=\"format\",\n            dtype=DataType.VARCHAR,\n            max_length=255,\n        )\n        doc_subject = FieldSchema(\n            name=\"subject\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_creator = FieldSchema(\n            name=\"creator\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_producer = FieldSchema(\n            name=\"producer\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_creationDate = FieldSchema(\n            name=\"creationDate\",\n            dtype=DataType.VARCHAR,\n            max_length=1024,\n        )\n        doc_modDate = FieldSchema(\n            name=\"modDate\",\n            dtype=DataType.VARCHAR,\n            max_length=1024,\n        )\n        doc_source = FieldSchema(\n            name=\"source\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        embedding = FieldSchema(\n            name=\"embedding\",\n            dtype=DataType.FLOAT_VECTOR,\n            dim=METADATA_VECTOR_DIM,\n        )\n        isDeleted = FieldSchema(\n            name=\"isDeleted\",\n            dtype=DataType.BOOL,\n        )\n        fields = [\n            doc_id,\n            doc_name,\n            doc_author,\n            doc_title,\n            doc_format,\n            doc_subject,\n            doc_creator,\n            doc_producer,\n            doc_creationDate,\n            doc_modDate,\n            doc_source,\n            embedding,\n            isDeleted,\n        ]\n        schema = CollectionSchema(fields=fields)\n        try:\n            self._col_metadata = Collection(\n                name=self._metadata_name,\n                schema=schema,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(f\"Failed to create collection {self._metadata_name}\")\n            raise e\n\n    def _create_metadata_collection_index(self) -&gt; None:\n        \"\"\"Create index for metadata collection in currently active database.\"\"\"\n        if not isinstance(self._col_metadata, Collection) or len(self._col_metadata.indexes) &gt; 0:\n            return\n\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 64},\n        }\n\n        try:\n            self._col_metadata.create_index(\n                field_name=\"embedding\",\n                index_params=index_params,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to create index for meta collection \" f\"{self._metadata_name}.\",\n            )\n            raise e\n\n    def _insert_data(self, documents: list[Document]) -&gt; str:\n        \"\"\"Insert documents into the currently active database.\n\n        Args:\n        ----\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n        -------\n            str: document id\n\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        metadata = [documents[0].metadata]\n        aligned_metadata = align_metadata(metadata)\n        try:\n            result = self._col_metadata.insert(aligned_metadata)\n            meta_id = str(result.primary_keys[0])\n            self._col_metadata.flush()\n        except MilvusException as e:\n            logger.error(\"Failed to insert meta data\")\n            raise e\n        aligned_docs = align_embeddings(documents, meta_id)\n        try:\n            # As we passed collection_name, documents will be added to existed collection\n            self._col_embeddings = Milvus.from_documents(\n                embedding=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n                documents=aligned_docs,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to insert data to embedding collection \" f\"{self._embedding_name}.\",\n            )\n            raise e\n        return meta_id\n\n    def store_embeddings(self, documents: list[Document]) -&gt; str:\n        \"\"\"Store documents in the currently active database.\n\n        Args:\n        ----\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n        -------\n            str: document id\n\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        return self._insert_data(documents)\n\n    def _build_embedding_search_expression(\n        self,\n        meta_ids: list[dict],\n    ) -&gt; str | None:\n        \"\"\"Build search expression for embedding collection. The generated\n        expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]\n\n        Args:\n        ----\n            meta_ids: the array of metadata id in metadata collection\n\n        Returns:\n        -------\n            str: search expression or None\n\n        \"\"\"\n        if len(meta_ids) == 0:\n            return \"meta_id in []\"\n        built_expr = \"\"\"meta_id in [\"\"\"\n        for item in meta_ids:\n            id = f'\"{item[\"id\"]}\",'\n            built_expr += id\n        built_expr = built_expr[:-1]\n        built_expr += \"\"\"]\"\"\"\n        return built_expr\n\n    def _join_embedding_and_metadata_results(\n        self,\n        result_embedding: list[Document],\n        result_meta: list[dict],\n    ) -&gt; list[Document]:\n        \"\"\"Join the search results of embedding collection and results of metadata.\n\n        Args:\n        ----\n            result_embedding (List[Document]): search result of embedding\n                collection\n\n            result_meta (List[Dict]): search result of metadata collection\n\n        Returns:\n        -------\n            List[Document]: combined results like\n                [{page_content: str, metadata: {...}}]\n\n        \"\"\"\n\n        def _find_metadata_by_id(\n            metadata: list[dict],\n            id: str,\n        ) -&gt; dict | None:\n            for d in metadata:\n                if str(d[\"id\"]) == id:\n                    return d\n            return None\n\n        joined_docs = []\n        for res in result_embedding:\n            found = _find_metadata_by_id(result_meta, res.metadata[\"meta_id\"])\n            if found is None:  # discard\n                logger.error(\n                    f\"Failed to join meta_id {res.metadata['meta_id']}\",\n                )\n                continue\n            joined_docs.append(\n                Document(page_content=res.page_content, metadata=found),\n            )\n        return joined_docs\n\n    @staticmethod\n    def _build_meta_col_query_expr_for_all_documents(\n        doc_ids: list[str] | None = None,\n    ) -&gt; str:\n        \"\"\"Build metadata collection query expression to obtain all documents.\n\n        Args:\n        ----\n            doc_ids: the list of document ids (metadata ids), if thie argument is None,\n                     that is, the query is to get all undeleted documents in metadata collection.\n                     Otherwise, the query is to getr all undeleted documents form provided doc_ids\n\n        Returns:\n        -------\n            query: str\n\n        \"\"\"\n        expr = f\"id in {doc_ids} and isDeleted == false\" if doc_ids is not None else \"isDeleted == false\"\n        return expr.replace('\"', \"\").replace(\"'\", \"\")\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = 3,\n        doc_ids: list[str] | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Perform similarity search insider the currently active database\n        according to the input query.\n\n        This method will:\n        1. get all non-deleted meta_id and build to search expression for\n            the currently active embedding collection\n        2. do similarity search in the embedding collection\n        3. combine metadata and embeddings\n\n        Args:\n        ----\n            query (str): query string\n\n            k (int): the number of results to return\n\n            doc_ids (Optional[list[str]]): the list of document ids, do\n                similarity search across the specified documents\n\n        Returns:\n        -------\n            List[Document]: search results\n\n        \"\"\"\n        result_metadata = []\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids,\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        expr = self._build_embedding_search_expression(result_metadata)\n        result_embedding = self._col_embeddings.similarity_search(\n            query=query,\n            k=k,\n            expr=expr,\n        )\n        return self._join_embedding_and_metadata_results(\n            result_embedding,\n            result_metadata,\n        )\n\n    def remove_document(\n        self,\n        doc_id: str,\n        doc_ids: list[str] | None = None,\n    ) -&gt; bool:\n        \"\"\"Remove the document include meta data and its embeddings.\n\n        Args:\n        ----\n            doc_id (str): the document to be deleted\n\n            doc_ids (Optional[list[str]]): the list of document ids, defines\n                documents scope within which remove operation occurs.\n\n        Returns:\n        -------\n            bool: True if the document is deleted, False otherwise\n\n        \"\"\"\n        if not self._col_metadata:\n            return False\n        if doc_ids is not None and (len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and doc_id not in doc_ids)):\n            return False\n        try:\n            expr = f\"id in [{doc_id}]\"\n            res = self._col_metadata.query(\n                expr=expr,\n                output_fields=METADATA_FIELDS,\n            )\n            if len(res) == 0:\n                return False\n            del_res = self._col_metadata.delete(expr)\n            self._col_metadata.flush()\n\n            res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n            if len(res) == 0:\n                return True\n            ids = [item[\"pk\"] for item in res]\n            embedding_expr = f\"pk in {ids}\"\n            del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n            self._col_embeddings.col.flush()\n            return True\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_all_documents(\n        self,\n        doc_ids: list[str] | None = None,\n    ) -&gt; list[dict]:\n        \"\"\"Get all non-deleted documents from the currently active database.\n\n        Args:\n        ----\n            doc_ids (List[str] optional): the list of document ids, defines\n                documents scope within which the operation of obtaining all\n                documents occurs\n\n        Returns:\n        -------\n            List[Dict]: the metadata of all non-deleted documents in the form\n                [{{id}, {author}, {source}, ...}]\n\n        \"\"\"\n        try:\n            expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n                doc_ids,\n            )\n            result_metadata = self._col_metadata.query(\n                expr=expr,\n                output_fields=METADATA_FIELDS,\n            )\n            return result_metadata\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_description(self, doc_ids: list[str] | None = None):\n        def get_name(meta: dict[str, str]):\n            name_col = [\"title\", \"name\", \"subject\", \"source\"]\n            for col in name_col:\n                if meta[col] is not None and len(meta[col]) &gt; 0:\n                    return meta[col]\n            return \"\"\n\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids,\n        )\n        result = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        names = list(map(get_name, result))\n        names_set = set(names)\n        desc = f\"This vector store contains the following articles: {names_set}\"\n        return desc[:MAX_AGENT_DESC_LENGTH]\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.__init__","title":"<code>__init__(embedding_func, connection_args=None, embedding_collection_name=None, metadata_collection_name=None)</code>","text":"<pre><code>embedding_func OpenAIEmbeddings: Function used to embed the text\n\nconnection_args Optional dict: args to connect Vector Database\n\nembedding_collection_name Optional str: exposed for test\n\nmetadata_collection_name Optional str: exposed for test\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def __init__(\n    self,\n    embedding_func: OpenAIEmbeddings,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n):\n    \"\"\"Args:\n    ----\n        embedding_func OpenAIEmbeddings: Function used to embed the text\n\n        connection_args Optional dict: args to connect Vector Database\n\n        embedding_collection_name Optional str: exposed for test\n\n        metadata_collection_name Optional str: exposed for test\n\n    \"\"\"\n    self._embedding_func = embedding_func\n    self._col_embeddings: Milvus | None = None\n    self._col_metadata: Collection | None = None\n    self._connection_args = validate_connection_args(connection_args)\n    self._embedding_name = embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n    self._metadata_name = metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._build_embedding_search_expression","title":"<code>_build_embedding_search_expression(meta_ids)</code>","text":"<p>Build search expression for embedding collection. The generated expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]</p> <pre><code>meta_ids: the array of metadata id in metadata collection\n</code></pre> <pre><code>str: search expression or None\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _build_embedding_search_expression(\n    self,\n    meta_ids: list[dict],\n) -&gt; str | None:\n    \"\"\"Build search expression for embedding collection. The generated\n    expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]\n\n    Args:\n    ----\n        meta_ids: the array of metadata id in metadata collection\n\n    Returns:\n    -------\n        str: search expression or None\n\n    \"\"\"\n    if len(meta_ids) == 0:\n        return \"meta_id in []\"\n    built_expr = \"\"\"meta_id in [\"\"\"\n    for item in meta_ids:\n        id = f'\"{item[\"id\"]}\",'\n        built_expr += id\n    built_expr = built_expr[:-1]\n    built_expr += \"\"\"]\"\"\"\n    return built_expr\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents","title":"<code>_build_meta_col_query_expr_for_all_documents(doc_ids=None)</code>  <code>staticmethod</code>","text":"<p>Build metadata collection query expression to obtain all documents.</p> <pre><code>doc_ids: the list of document ids (metadata ids), if thie argument is None,\n         that is, the query is to get all undeleted documents in metadata collection.\n         Otherwise, the query is to getr all undeleted documents form provided doc_ids\n</code></pre> <pre><code>query: str\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>@staticmethod\ndef _build_meta_col_query_expr_for_all_documents(\n    doc_ids: list[str] | None = None,\n) -&gt; str:\n    \"\"\"Build metadata collection query expression to obtain all documents.\n\n    Args:\n    ----\n        doc_ids: the list of document ids (metadata ids), if thie argument is None,\n                 that is, the query is to get all undeleted documents in metadata collection.\n                 Otherwise, the query is to getr all undeleted documents form provided doc_ids\n\n    Returns:\n    -------\n        query: str\n\n    \"\"\"\n    expr = f\"id in {doc_ids} and isDeleted == false\" if doc_ids is not None else \"isDeleted == false\"\n    return expr.replace('\"', \"\").replace(\"'\", \"\")\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._create_collections","title":"<code>_create_collections()</code>","text":"<p>Create or load the embedding and metadata collections from the currently active database.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _create_collections(self) -&gt; None:\n    \"\"\"Create or load the embedding and metadata collections from the currently\n    active database.\n    \"\"\"\n    embedding_exists = utility.has_collection(\n        self._embedding_name,\n        using=self.alias,\n    )\n    meta_exists = utility.has_collection(\n        self._metadata_name,\n        using=self.alias,\n    )\n\n    if embedding_exists:\n        self._load_embeddings_collection()\n    else:\n        self._create_embeddings_collection()\n\n    if meta_exists:\n        self._load_metadata_collection()\n    else:\n        self._create_metadata_collection()\n\n    self._create_metadata_collection_index()\n    self._col_metadata.load()\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._create_connection_alias","title":"<code>_create_connection_alias(host, port, user, password)</code>","text":"<p>Connect to host and create a connection alias for metadata collection using a random uuid.</p> <pre><code>host (str): host ip address\nport (str): host port\n</code></pre> <pre><code>str: connection alias\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _create_connection_alias(\n    self,\n    host: str,\n    port: str,\n    user: str,\n    password: str,\n) -&gt; str:\n    \"\"\"Connect to host and create a connection alias for metadata collection\n    using a random uuid.\n\n    Args:\n    ----\n        host (str): host ip address\n        port (str): host port\n\n    Returns:\n    -------\n        str: connection alias\n\n    \"\"\"\n    alias = uuid.uuid4().hex\n    try:\n        connections.connect(\n            host=host,\n            port=port,\n            user=user,\n            password=password,\n            alias=alias,\n        )\n        logger.debug(f\"Created new connection using: {alias}\")\n        return alias\n    except MilvusException as e:\n        logger.error(f\"Failed to create  new connection using: {alias}\")\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._create_embeddings_collection","title":"<code>_create_embeddings_collection()</code>","text":"<p>Create embedding collection. All fields: \"meta_id\", \"vector\"</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _create_embeddings_collection(self) -&gt; None:\n    \"\"\"Create embedding collection.\n    All fields: \"meta_id\", \"vector\"\n    \"\"\"\n    try:\n        self._col_embeddings = Milvus(\n            embedding_function=self._embedding_func,\n            collection_name=self._embedding_name,\n            connection_args=self._connection_args,\n        )\n    except MilvusException as e:\n        logger.error(\n            f\"Failed to create embeddings collection {self._embedding_name}\",\n        )\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._create_metadata_collection","title":"<code>_create_metadata_collection()</code>","text":"<p>Create metadata collection.</p> <p>All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\", \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\", \"isDeleted\".</p> <p>As the vector database requires a vector field, we will create a fake vector \"embedding\". The field \"isDeleted\" is used to specify if the document is deleted.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _create_metadata_collection(self) -&gt; None:\n    \"\"\"Create metadata collection.\n\n    All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\",\n    \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\",\n    \"isDeleted\".\n\n    As the vector database requires a vector field, we will create a fake\n    vector \"embedding\". The field \"isDeleted\" is used to specify if the\n    document is deleted.\n    \"\"\"\n    MAX_LENGTH = 10000\n    doc_id = FieldSchema(\n        name=\"id\",\n        dtype=DataType.INT64,\n        is_primary=True,\n        auto_id=True,\n    )\n    doc_name = FieldSchema(\n        name=\"name\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    doc_author = FieldSchema(\n        name=\"author\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    doc_title = FieldSchema(\n        name=\"title\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    doc_format = FieldSchema(\n        name=\"format\",\n        dtype=DataType.VARCHAR,\n        max_length=255,\n    )\n    doc_subject = FieldSchema(\n        name=\"subject\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    doc_creator = FieldSchema(\n        name=\"creator\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    doc_producer = FieldSchema(\n        name=\"producer\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    doc_creationDate = FieldSchema(\n        name=\"creationDate\",\n        dtype=DataType.VARCHAR,\n        max_length=1024,\n    )\n    doc_modDate = FieldSchema(\n        name=\"modDate\",\n        dtype=DataType.VARCHAR,\n        max_length=1024,\n    )\n    doc_source = FieldSchema(\n        name=\"source\",\n        dtype=DataType.VARCHAR,\n        max_length=MAX_LENGTH,\n    )\n    embedding = FieldSchema(\n        name=\"embedding\",\n        dtype=DataType.FLOAT_VECTOR,\n        dim=METADATA_VECTOR_DIM,\n    )\n    isDeleted = FieldSchema(\n        name=\"isDeleted\",\n        dtype=DataType.BOOL,\n    )\n    fields = [\n        doc_id,\n        doc_name,\n        doc_author,\n        doc_title,\n        doc_format,\n        doc_subject,\n        doc_creator,\n        doc_producer,\n        doc_creationDate,\n        doc_modDate,\n        doc_source,\n        embedding,\n        isDeleted,\n    ]\n    schema = CollectionSchema(fields=fields)\n    try:\n        self._col_metadata = Collection(\n            name=self._metadata_name,\n            schema=schema,\n            using=self.alias,\n        )\n    except MilvusException as e:\n        logger.error(f\"Failed to create collection {self._metadata_name}\")\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._create_metadata_collection_index","title":"<code>_create_metadata_collection_index()</code>","text":"<p>Create index for metadata collection in currently active database.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _create_metadata_collection_index(self) -&gt; None:\n    \"\"\"Create index for metadata collection in currently active database.\"\"\"\n    if not isinstance(self._col_metadata, Collection) or len(self._col_metadata.indexes) &gt; 0:\n        return\n\n    index_params = {\n        \"metric_type\": \"L2\",\n        \"index_type\": \"HNSW\",\n        \"params\": {\"M\": 8, \"efConstruction\": 64},\n    }\n\n    try:\n        self._col_metadata.create_index(\n            field_name=\"embedding\",\n            index_params=index_params,\n            using=self.alias,\n        )\n    except MilvusException as e:\n        logger.error(\n            \"Failed to create index for meta collection \" f\"{self._metadata_name}.\",\n        )\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._init_host","title":"<code>_init_host()</code>","text":"<p>Initialize host. Will read/create document collection inside currently active database.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _init_host(self) -&gt; None:\n    \"\"\"Initialize host. Will read/create document collection inside currently\n    active database.\n    \"\"\"\n    self._create_collections()\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._insert_data","title":"<code>_insert_data(documents)</code>","text":"<p>Insert documents into the currently active database.</p> <pre><code>documents (List[Documents]): documents array, usually from\n    DocumentReader.load_document, DocumentReader.document_from_pdf,\n    DocumentReader.document_from_txt\n</code></pre> <pre><code>str: document id\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _insert_data(self, documents: list[Document]) -&gt; str:\n    \"\"\"Insert documents into the currently active database.\n\n    Args:\n    ----\n        documents (List[Documents]): documents array, usually from\n            DocumentReader.load_document, DocumentReader.document_from_pdf,\n            DocumentReader.document_from_txt\n\n    Returns:\n    -------\n        str: document id\n\n    \"\"\"\n    if len(documents) == 0:\n        return None\n    metadata = [documents[0].metadata]\n    aligned_metadata = align_metadata(metadata)\n    try:\n        result = self._col_metadata.insert(aligned_metadata)\n        meta_id = str(result.primary_keys[0])\n        self._col_metadata.flush()\n    except MilvusException as e:\n        logger.error(\"Failed to insert meta data\")\n        raise e\n    aligned_docs = align_embeddings(documents, meta_id)\n    try:\n        # As we passed collection_name, documents will be added to existed collection\n        self._col_embeddings = Milvus.from_documents(\n            embedding=self._embedding_func,\n            collection_name=self._embedding_name,\n            connection_args=self._connection_args,\n            documents=aligned_docs,\n        )\n    except MilvusException as e:\n        logger.error(\n            \"Failed to insert data to embedding collection \" f\"{self._embedding_name}.\",\n        )\n        raise e\n    return meta_id\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._join_embedding_and_metadata_results","title":"<code>_join_embedding_and_metadata_results(result_embedding, result_meta)</code>","text":"<p>Join the search results of embedding collection and results of metadata.</p> <pre><code>result_embedding (List[Document]): search result of embedding\n    collection\n\nresult_meta (List[Dict]): search result of metadata collection\n</code></pre> <pre><code>List[Document]: combined results like\n    [{page_content: str, metadata: {...}}]\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _join_embedding_and_metadata_results(\n    self,\n    result_embedding: list[Document],\n    result_meta: list[dict],\n) -&gt; list[Document]:\n    \"\"\"Join the search results of embedding collection and results of metadata.\n\n    Args:\n    ----\n        result_embedding (List[Document]): search result of embedding\n            collection\n\n        result_meta (List[Dict]): search result of metadata collection\n\n    Returns:\n    -------\n        List[Document]: combined results like\n            [{page_content: str, metadata: {...}}]\n\n    \"\"\"\n\n    def _find_metadata_by_id(\n        metadata: list[dict],\n        id: str,\n    ) -&gt; dict | None:\n        for d in metadata:\n            if str(d[\"id\"]) == id:\n                return d\n        return None\n\n    joined_docs = []\n    for res in result_embedding:\n        found = _find_metadata_by_id(result_meta, res.metadata[\"meta_id\"])\n        if found is None:  # discard\n            logger.error(\n                f\"Failed to join meta_id {res.metadata['meta_id']}\",\n            )\n            continue\n        joined_docs.append(\n            Document(page_content=res.page_content, metadata=found),\n        )\n    return joined_docs\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._load_embeddings_collection","title":"<code>_load_embeddings_collection()</code>","text":"<p>Load embeddings collection from currently active database.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _load_embeddings_collection(self) -&gt; None:\n    \"\"\"Load embeddings collection from currently active database.\"\"\"\n    try:\n        self._col_embeddings = Milvus(\n            embedding_function=self._embedding_func,\n            collection_name=self._embedding_name,\n            connection_args=self._connection_args,\n        )\n    except MilvusException as e:\n        logger.error(\n            f\"Failed to load embeddings collection {self._embedding_name}.\",\n        )\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus._load_metadata_collection","title":"<code>_load_metadata_collection()</code>","text":"<p>Load metadata collection from currently active database.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def _load_metadata_collection(self) -&gt; None:\n    \"\"\"Load metadata collection from currently active database.\"\"\"\n    self._col_metadata = Collection(\n        self._metadata_name,\n        using=self.alias,\n    )\n    self._col_metadata.load()\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.connect","title":"<code>connect()</code>","text":"<p>Connect to a host and read two document collections (the default names are <code>DocumentEmbeddings</code> and <code>DocumentMetadata</code>) in the currently active database (default database name is <code>default</code>); if those document collections don't exist, create the two collections.</p> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to a host and read two document collections (the default names\n    are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n    database (default database name is `default`); if those document\n    collections don't exist, create the two collections.\n    \"\"\"\n    self._connect(**self._connection_args)\n    self._init_host()\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.get_all_documents","title":"<code>get_all_documents(doc_ids=None)</code>","text":"<p>Get all non-deleted documents from the currently active database.</p> <pre><code>doc_ids (List[str] optional): the list of document ids, defines\n    documents scope within which the operation of obtaining all\n    documents occurs\n</code></pre> <pre><code>List[Dict]: the metadata of all non-deleted documents in the form\n    [{{id}, {author}, {source}, ...}]\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def get_all_documents(\n    self,\n    doc_ids: list[str] | None = None,\n) -&gt; list[dict]:\n    \"\"\"Get all non-deleted documents from the currently active database.\n\n    Args:\n    ----\n        doc_ids (List[str] optional): the list of document ids, defines\n            documents scope within which the operation of obtaining all\n            documents occurs\n\n    Returns:\n    -------\n        List[Dict]: the metadata of all non-deleted documents in the form\n            [{{id}, {author}, {source}, ...}]\n\n    \"\"\"\n    try:\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids,\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        return result_metadata\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.remove_document","title":"<code>remove_document(doc_id, doc_ids=None)</code>","text":"<p>Remove the document include meta data and its embeddings.</p> <pre><code>doc_id (str): the document to be deleted\n\ndoc_ids (Optional[list[str]]): the list of document ids, defines\n    documents scope within which remove operation occurs.\n</code></pre> <pre><code>bool: True if the document is deleted, False otherwise\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def remove_document(\n    self,\n    doc_id: str,\n    doc_ids: list[str] | None = None,\n) -&gt; bool:\n    \"\"\"Remove the document include meta data and its embeddings.\n\n    Args:\n    ----\n        doc_id (str): the document to be deleted\n\n        doc_ids (Optional[list[str]]): the list of document ids, defines\n            documents scope within which remove operation occurs.\n\n    Returns:\n    -------\n        bool: True if the document is deleted, False otherwise\n\n    \"\"\"\n    if not self._col_metadata:\n        return False\n    if doc_ids is not None and (len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and doc_id not in doc_ids)):\n        return False\n    try:\n        expr = f\"id in [{doc_id}]\"\n        res = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        if len(res) == 0:\n            return False\n        del_res = self._col_metadata.delete(expr)\n        self._col_metadata.flush()\n\n        res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n        if len(res) == 0:\n            return True\n        ids = [item[\"pk\"] for item in res]\n        embedding_expr = f\"pk in {ids}\"\n        del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n        self._col_embeddings.col.flush()\n        return True\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.similarity_search","title":"<code>similarity_search(query, k=3, doc_ids=None)</code>","text":"<p>Perform similarity search insider the currently active database according to the input query.</p> <p>This method will: 1. get all non-deleted meta_id and build to search expression for     the currently active embedding collection 2. do similarity search in the embedding collection 3. combine metadata and embeddings</p> <pre><code>query (str): query string\n\nk (int): the number of results to return\n\ndoc_ids (Optional[list[str]]): the list of document ids, do\n    similarity search across the specified documents\n</code></pre> <pre><code>List[Document]: search results\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def similarity_search(\n    self,\n    query: str,\n    k: int = 3,\n    doc_ids: list[str] | None = None,\n) -&gt; list[Document]:\n    \"\"\"Perform similarity search insider the currently active database\n    according to the input query.\n\n    This method will:\n    1. get all non-deleted meta_id and build to search expression for\n        the currently active embedding collection\n    2. do similarity search in the embedding collection\n    3. combine metadata and embeddings\n\n    Args:\n    ----\n        query (str): query string\n\n        k (int): the number of results to return\n\n        doc_ids (Optional[list[str]]): the list of document ids, do\n            similarity search across the specified documents\n\n    Returns:\n    -------\n        List[Document]: search results\n\n    \"\"\"\n    result_metadata = []\n    expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n        doc_ids,\n    )\n    result_metadata = self._col_metadata.query(\n        expr=expr,\n        output_fields=METADATA_FIELDS,\n    )\n    expr = self._build_embedding_search_expression(result_metadata)\n    result_embedding = self._col_embeddings.similarity_search(\n        query=query,\n        k=k,\n        expr=expr,\n    )\n    return self._join_embedding_and_metadata_results(\n        result_embedding,\n        result_metadata,\n    )\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.store_embeddings","title":"<code>store_embeddings(documents)</code>","text":"<p>Store documents in the currently active database.</p> <pre><code>documents (List[Documents]): documents array, usually from\n    DocumentReader.load_document, DocumentReader.document_from_pdf,\n    DocumentReader.document_from_txt\n</code></pre> <pre><code>str: document id\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def store_embeddings(self, documents: list[Document]) -&gt; str:\n    \"\"\"Store documents in the currently active database.\n\n    Args:\n    ----\n        documents (List[Documents]): documents array, usually from\n            DocumentReader.load_document, DocumentReader.document_from_pdf,\n            DocumentReader.document_from_txt\n\n    Returns:\n    -------\n        str: document id\n\n    \"\"\"\n    if len(documents) == 0:\n        return None\n    return self._insert_data(documents)\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.align_embeddings","title":"<code>align_embeddings(docs, meta_id)</code>","text":"<p>Ensure that the metadata id is present in each document.</p> <pre><code>docs (List[Document]): List of documents\n\nmeta_id (int): Metadata id to assign to the documents\n</code></pre> <pre><code>List[Document]: List of documents, with each document having a metadata\n    id.\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def align_embeddings(docs: list[Document], meta_id: int) -&gt; list[Document]:\n    \"\"\"Ensure that the metadata id is present in each document.\n\n    Args:\n    ----\n        docs (List[Document]): List of documents\n\n        meta_id (int): Metadata id to assign to the documents\n\n    Returns:\n    -------\n        List[Document]: List of documents, with each document having a metadata\n            id.\n\n    \"\"\"\n    ret = []\n    for doc in docs:\n        ret.append(\n            Document(\n                page_content=doc.page_content,\n                metadata={\"meta_id\": meta_id},\n            ),\n        )\n    return ret\n</code></pre>"},{"location":"BioChatter/api-docs/vectorstore/#biochatter.vectorstore_agent.align_metadata","title":"<code>align_metadata(metadata, isDeleted=False)</code>","text":"<p>Ensure that specific metadata fields are present; if not provided, fill with \"unknown\". Also, add a random vector to each metadata item to simulate an embedding.</p> <pre><code>metadata (List[Dict]): List of metadata items\n\nisDeleted (Optional[bool], optional): Whether the document is deleted.\n    Defaults to False.\n</code></pre> <pre><code>List[List]: List of metadata items, with each item being a list of\n    metadata fields.\n</code></pre> Source code in <code>biochatter/biochatter/vectorstore_agent.py</code> <pre><code>def align_metadata(\n    metadata: list[dict],\n    isDeleted: bool | None = False,\n) -&gt; list[list]:\n    \"\"\"Ensure that specific metadata fields are present; if not provided, fill with\n    \"unknown\". Also, add a random vector to each metadata item to simulate an\n    embedding.\n\n    Args:\n    ----\n        metadata (List[Dict]): List of metadata items\n\n        isDeleted (Optional[bool], optional): Whether the document is deleted.\n            Defaults to False.\n\n    Returns:\n    -------\n        List[List]: List of metadata items, with each item being a list of\n            metadata fields.\n\n    \"\"\"\n    ret = []\n    fields = METADATA_FIELDS.copy()\n    fields.pop(0)\n    for ix, k in enumerate(fields):\n        ret.append([item[k] if k in item else \"unknown\" for item in metadata])\n\n    ret.append(\n        [[random.random() for _ in range(METADATA_VECTOR_DIM)] for _ in range(len(metadata))],\n    )\n    ret.append([isDeleted for _ in metadata])\n    return ret\n</code></pre>"},{"location":"BioChatter/benchmark/developer/","title":"Benchmark - Developer Notes","text":"<p>To understand the benchmarking procedure, you should be familiar with Pytest. The benchmark test matrix is executed via Pytest fixtures that iterate through the combinations of test parameters such as model name and size.  This basic setup happens in the <code>conftest.py</code> file in the <code>benchmark</code> directory.  The benchmark Pytest setup is distinct from the Pytest setup we use for our continuous integration (in the <code>test</code> folder).</p> <p>You can imagine the benchmark as a set of unit tests, with the only difference being that the test subject is not our codebase, but the behaviour and performance of the various LLMs, prompts, etc. These tests are defined in two parts: the data and the method. Data are collected in a set of YAML files in the <code>benchmark/data</code> directory, while the benchmark methods are implemented in the Pytest functions in the individual Python modules (file names starting with <code>test_</code>). We also have some Python modules for auxiliary functions, such as <code>load_dataset.py</code>.</p> <p>In the following, we will describe a walkthrough of how to implement your own tests according to our benchmark philosophy.</p>"},{"location":"BioChatter/benchmark/developer/#test-setup","title":"Test setup","text":"<p>Tests are collected in the typical Pytest manner at the start of the procedure. In <code>conftest.py</code>, we define the model combinations we want to use in the benchmark.  We distinguish between closed-source and open-source models, since open-source models offer more flexibility, e.g., by setting their size and quantisation. In contrast, for OpenAI models, all we need is the name.</p>"},{"location":"BioChatter/benchmark/developer/#quickstart","title":"Quickstart","text":"<p>For getting started with developing your own benchmark, OpenAI models offer the simplest way, only requiring an API key and an internet connection. If you don't want to run open-source models right away, which is tied to setting up an external service for deploying the models, we can remove the <code>XINFERENCE_MODEL_NAMES</code> from the list of models to be benchmarked in <code>conftest.py</code> (by deleting or commenting them out):</p> <pre><code>BENCHMARKED_MODELS = OPENAI_MODEL_NAMES\n</code></pre> <p>In addition, we can reduce the number of OpenAI models to call to one for development; <code>gpt-3.5-turbo-0125</code> is a well-performing and economical initial choice (in <code>conftest.py</code>).</p> <pre><code>OPENAI_MODEL_NAMES = [\n    \"gpt-3.5-turbo-0125\",\n]\n</code></pre> <p>The last thing to look out for when running the benchmark is to reduce the number of iterations for each test to one. We run iterations to account for stochasticity in LLM responses when we run the benchmark for real, but in development, this iteration brings no benefit and just increases computational cost. Set <code>N_ITERATIONS</code> to 1 in <code>conftest.py</code>.</p> <pre><code># how often should each benchmark be run?\nN_ITERATIONS = 1\n</code></pre> <p>This setup should allow you to run and debug a newly developed benchmark dataset or method effectively. For more explanation on how to do this, please read on.</p>"},{"location":"BioChatter/benchmark/developer/#debugging-the-benchmark","title":"Debugging the benchmark","text":"<p>You can get some insight into how the benchmark works by debugging the existing test cases and stepping through the code line-by-line. For this, it is necessary that you are familiar with the debugging procedure in your programming environment of choice, for instance, VSCode. You can set breakpoints in the initial setup (e.g., in <code>conftest.py</code> and <code>load_dataset.py</code>) as well as the test functions (e.g., <code>test_rag_interpretation.py</code>). Stepping through the code will give you insights into how the benchmark is designed and also how the LLMs respond in detail to each specific task. This is particularly helpful for ensuring that your newly developed benchmark test cases behave as expected and test accurately the functionality you aim to test.</p>"},{"location":"BioChatter/benchmark/developer/#creating-new-test-cases-for-existing-tests","title":"Creating new test cases for existing tests","text":"<p>Our test cases are collected in YAML files that follow a simple formalism for defining each test. These files are found in <code>benchmark/data</code> and need to end in <code>_data.yaml</code> in order to be loaded in the test procedure. They include test cases and auxiliary materials, such as knowledge graph schemata. A test case consists of</p> <ul> <li> <p>a descriptive name</p> </li> <li> <p>a set of input data to simulate a real-world task (e.g., the question of a user and some auxiliary information)</p> </li> <li> <p>a set of expected results to assess the performance of the model's response, akin to assert statements in regular unit tests</p> </li> </ul> <p>Here is a simple example test case:</p> <pre><code>rag_interpretation:\n# test simple relevance judgement\n  - case: explicit_relevance_yes\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        [\n          \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for  relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation  or justification! Here is the fragment: \",\n          \"The EGFR pathway is deregulated in a number of cancers.\",\n        ]\n    expected:\n      answer: \"yes\"\n</code></pre> <p>In this test, we benchmark the model's ability to judge the relevance of text fragments to be used in a Retrieval-Augmented Generation (RAG) scenario in an explicit fashion (i.e., we directly ask the model to judge the relevance of the given fragments). Thus, we choose the descriptive name (<code>case</code>) <code>explicit_relevance_yes</code>. The <code>input</code>s are a simulated user question (<code>prompt</code>) and the <code>system_messages</code> that simulate the prompt engineering and RAG fragments (that in the real application would be retrieved by some suitable mechanism). Finally, we define the <code>expected</code> output, which in this case is only the <code>answer</code> we expect from the LLM.</p> <p>The way that these test cases are evaluated is defined in the Pytest functions, which are tuned to the specific fields we define in the YAML. We can freely define test definitions and testing implementation by adjusting the YAML alongside the test Python code. The coordination between cases and Pytest functions is done by name of the test category, in this example <code>rag_interpretation</code> in the YAML and <code>test_rag_interpretation.py</code> in the <code>benchmark</code> directory.</p> <p>New tests can be arbitrarily complex as long as the test function is synchronised with the content of the YAML test dataset's fields (see below).</p>"},{"location":"BioChatter/benchmark/developer/#combinatorial-expansion-of-tests","title":"Combinatorial expansion of tests","text":"<p>Sometimes, it can be efficient to run the same test with slightly different variations of input data. For instance, LLMs are very sensitive to the prompt setup, and thus, we can run the same test with varying approaches to the prompt engineering. To simplify this, we can define input data as dictionaries (with keys being a descriptive name of the sub-test, and values being the content), which will lead to expansion of these test cases into full cases according to the definition. For instance, we can define a RAG interpretation test with prompts at three different levels of detail (<code>simple</code>, <code>more_explicit</code>, and <code>repeat_instruction</code>):</p> <pre><code>rag_interpretation:\n  # test simple irrelevance judgement\n  - case: explicit_relevance_no\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        simple:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        more_explicit:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        repeat_instruction:\n          [\n            \"You will receive a text fragment to help answer the user's question. You should only respond with 'yes' or 'no' without additional words. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n    expected:\n      answer: \"no\"\n</code></pre> <p>Upon instantiation of the test matrix, this definition will be expanded into three full tests, each with their respective prompt setup. You can define as many combinations as you like (for instance, you could also define a list of prompts in this example), but be aware that the number of tests will grow exponentially with the number of combinations.</p>"},{"location":"BioChatter/benchmark/developer/#setting-up-the-test-data-pipeline","title":"Setting up the test data pipeline","text":"<p>Test data are provided to the test functions via fixtures. The fixtures are defined in the <code>conftest.py</code> file and are used to load the test data from the YAML files. If you add a new test module or a function with a new kind of test data, you need to add the corresponding fixture to the <code>pytest_generate_tests</code> function in <code>conftest.py</code>. This function is responsible for loading the test data and providing appropriately named fixtures to the test functions. For the tests defined above, this equates to:</p> <pre><code>def pytest_generate_tests(metafunc):\n    data = BENCHMARK_DATASET\n    if \"test_data_rag_interpretation\" in metafunc.fixturenames:\n        metafunc.parametrize(\n            \"test_data_rag_interpretation\",\n            data[\"rag_interpretation\"],\n        )\n</code></pre> <p>We prepend the fixtures with <code>test_data_</code> for consistency and higher code readability. For more information, see the Pytest Documentation.</p>"},{"location":"BioChatter/benchmark/developer/#creating-new-test-procedures","title":"Creating new test procedures","text":"<p>If a new kind of test requires a bespoke procedure, such as evaluating a newly introduced functionality or calculating a score in a distinct way, we can introduce new methods to the test modules or even entire new modules. Following the layout of the existing tests, the newly created test functions should refer to fixtures for their data inputs.  Such a test function typically has as parameters:</p> <ul> <li> <p>the <code>model_name</code> fixture, to be able to record a model-specific benchmark metric;</p> </li> <li> <p>a <code>test_data</code> object that is generated from the benchmark dataset according to the name of the test module (e.g., <code>test_data_rag_interpretation</code>). This is the fixture you defined above in <code>conftest.py</code>;</p> </li> <li> <p>a <code>conversation</code> instance (the connection to the LLM to be tested);</p> </li> <li> <p>the <code>multiple_testing</code> fixture that implements running the test multiple times and averaging the results;</p> </li> <li> <p>any number of additional inputs that may be required for the tests.</p> </li> </ul> <p>For instance, the knowledge graph query generation tests acquire additional tests inputs from the YAML definition (the schema of the BioCypher knowledge graph underlying the test) and additional functionality from BioChatter (an instance of the prompt engine class that generates the knowledge graph query using the aforementioned schema).</p>"},{"location":"BioChatter/benchmark/developer/#running-the-benchmark","title":"Running the benchmark","text":"<p>If everything is set up correctly, you can run the benchmark by executing the following command in the root directory of the repository:</p> <pre><code>poetry run pytest benchmark\n</code></pre> <p>We need to specify the <code>benchmark</code> directory to run the benchmark tests, because we also have regular tests in the <code>test</code> directory. If you want to run only a specific test module, you can specify the file name (or use any other Pytest workflow).</p> <p>Skipping tests</p> <p>For efficiency reasons, we by default do not rerun tests that have already been executed for a given model and test case. For this purpose, we store the results in the <code>benchmark/results</code> directory, including, for every test case, an md5 hash of the input data. If you want to rerun a test, you can delete the corresponding line (or entire file) in the <code>results</code> directory.</p> <p>We re-run the benchmark automatically if a test case has changed (reflected in a different md5 hash) or if there is a new <code>biochatter</code> version (potentially introducing changes in the behaviour of the framework). If a test case has changed, the old result is automatically removed from the result files. You can also force a rerun of all tests by using the <code>--run-all</code> flag:</p> <pre><code>poetry run pytest benchmark --run-all\n</code></pre>"},{"location":"BioChatter/benchmark/developer/#processing-benchmark-results","title":"Processing benchmark results","text":"<p>The benchmark results are processed to generate visualizations and statistics that are displayed in the documentation. This processing used to happen during the documentation build but has been moved to a separate workflow for better efficiency:</p> <ol> <li> <p>The processing script is located at <code>docs/scripts/hooks.py</code> and can be run directly:</p> <pre><code>python docs/scripts/hooks.py\n</code></pre> </li> <li> <p>The processing is automated through a GitHub Action that runs when:</p> <ul> <li> <p>Changes are pushed to the <code>main</code> branch that affect benchmark results</p> </li> <li> <p>Pull requests targeting <code>main</code> include benchmark result changes</p> </li> <li> <p>Manual triggers via workflow dispatch in GitHub Actions</p> </li> </ul> </li> </ol> <p>This separation means that the heavy processing of benchmark results only happens when the results actually change, rather than on every documentation build.</p>"},{"location":"BioChatter/benchmark/developer/#running-open-source-models","title":"Running open-source models","text":"<p>To execute the benchmark on any of the open-source models in the test matrix, you need to deploy an Xorbits Inference server at an arbitrary IP, either via Docker (available on Linux machines with dedicated Nvidia GPU) or natively (e.g., on Apple machines). Please refer to the Xinference documentation for details.</p> <p>When you have deployed the Xinference server, you can point the benchmark to the server by setting the <code>BENCHMARK_URL</code> parameter in <code>conftest.py</code>:</p> <pre><code># Xinference IP and port\nBENCHMARK_URL = \"http://localhost:9997\"\n</code></pre>"},{"location":"BioChatter/benchmark/overview/","title":"Benchmark Results - Overview","text":"<p>Here we collect the results of the living BioChatter benchmark. For an explanation, see the benchmarking documentation and the developer docs for further reading.</p>"},{"location":"BioChatter/benchmark/overview/#scores-per-model","title":"Scores per model","text":"<p>Table sorted by median score in descending order. Click the column names to reorder.</p> Model name Size Median Accuracy SD claude-3-opus-20240229 Unknown 0.77 0.28 gpt-3.5-turbo-0125 175 0.77 0.19 gpt-3.5-turbo-0613 175 0.76 0.21 claude-3-5-sonnet-20240620 Unknown 0.76 0.28 gpt-4-0613 Unknown 0.73 0.17 gpt-4o-2024-08-06 Unknown 0.73 0.24 llama-3.1-instruct 70 0.73 0.29 llama-3.1-instruct 8 0.72 0.28 gpt-4-0125-preview Unknown 0.69 0.26 gpt-4o-mini-2024-07-18 Unknown 0.69 0.22 gpt-4-1106-preview Unknown 0.69 0.06 gpt-4o-2024-05-13 Unknown 0.68 0.31 gpt-4-turbo-2024-04-09 Unknown 0.68 0.27 llama-3-instruct 8 0.65 0.36 openhermes-2.5 7 0.6 0.3 gpt-4o-2024-11-20 Unknown 0.55 0.14 chatglm3 6 0.44 0.26 llama-2-chat 70 0.42 0.34 mistral-instruct-v0.2 7 0.4 0.33 code-llama-instruct 7 0.4 0.35 code-llama-instruct 34 0.38 0.35 code-llama-instruct 13 0.38 0.33 llama-2-chat 13 0.38 0.33 mixtral-instruct-v0.1 46,7 0.34 0.28 llama-2-chat 7 0.31 0.3 <p> </p>"},{"location":"BioChatter/benchmark/overview/#scores-per-quantisation","title":"Scores per quantisation","text":"<p>Table sorted by median score in descending order. Click the column names to reorder.</p> Model name Size Version Quantisation Median Accuracy SD claude-3-opus-20240229 Unknown nan nan 0.77 0.28 gpt-3.5-turbo-0125 175 nan nan 0.77 0.19 claude-3-5-sonnet-20240620 Unknown nan nan 0.76 0.28 gpt-3.5-turbo-0613 175 nan nan 0.76 0.21 llama-3.1-instruct 8 ggufv2 Q6_K 0.74 0.28 llama-3.1-instruct 70 ggufv2 IQ2_M 0.74 0.29 llama-3.1-instruct 8 ggufv2 Q5_K_M 0.74 0.28 gpt-4-0613 Unknown nan nan 0.73 0.17 llama-3.1-instruct 70 ggufv2 IQ4_XS 0.73 0.29 gpt-4o-2024-08-06 Unknown nan nan 0.73 0.24 llama-3.1-instruct 8 ggufv2 Q8_0 0.72 0.3 llama-3.1-instruct 8 ggufv2 Q3_K_L 0.71 0.28 llama-3.1-instruct 8 ggufv2 Q4_K_M 0.7 0.26 llama-3.1-instruct 8 ggufv2 IQ4_XS 0.69 0.28 gpt-4-1106-preview Unknown nan nan 0.69 0.06 gpt-4-0125-preview Unknown nan nan 0.69 0.26 gpt-4o-mini-2024-07-18 Unknown nan nan 0.69 0.22 gpt-4-turbo-2024-04-09 Unknown nan nan 0.68 0.27 gpt-4o-2024-05-13 Unknown nan nan 0.68 0.31 llama-3.1-instruct 70 ggufv2 Q3_K_S 0.67 0.28 llama-3-instruct 8 ggufv2 Q8_0 0.65 0.35 llama-3-instruct 8 ggufv2 Q4_K_M 0.65 0.38 llama-3-instruct 8 ggufv2 Q6_K 0.65 0.36 openhermes-2.5 7 ggufv2 Q6_K 0.62 0.3 llama-3-instruct 8 ggufv2 Q5_K_M 0.62 0.36 openhermes-2.5 7 ggufv2 Q5_K_M 0.6 0.29 openhermes-2.5 7 ggufv2 Q8_0 0.6 0.3 openhermes-2.5 7 ggufv2 Q4_K_M 0.6 0.3 openhermes-2.5 7 ggufv2 Q3_K_M 0.56 0.3 gpt-4o-2024-11-20 Unknown nan nan 0.55 0.14 code-llama-instruct 34 ggufv2 Q2_K 0.5 0.33 openhermes-2.5 7 ggufv2 Q2_K 0.5 0.28 code-llama-instruct 7 ggufv2 Q3_K_M 0.49 0.31 code-llama-instruct 7 ggufv2 Q4_K_M 0.47 0.39 mistral-instruct-v0.2 7 ggufv2 Q5_K_M 0.46 0.34 mistral-instruct-v0.2 7 ggufv2 Q6_K 0.45 0.34 code-llama-instruct 34 ggufv2 Q3_K_M 0.45 0.31 chatglm3 6 ggmlv3 q4_0 0.44 0.26 llama-2-chat 70 ggufv2 Q5_K_M 0.44 0.35 llama-2-chat 70 ggufv2 Q4_K_M 0.44 0.35 code-llama-instruct 13 ggufv2 Q6_K 0.44 0.35 code-llama-instruct 13 ggufv2 Q8_0 0.44 0.33 code-llama-instruct 13 ggufv2 Q5_K_M 0.43 0.32 llama-2-chat 70 ggufv2 Q3_K_M 0.41 0.33 mistral-instruct-v0.2 7 ggufv2 Q3_K_M 0.41 0.34 mistral-instruct-v0.2 7 ggufv2 Q8_0 0.4 0.33 llama-2-chat 13 ggufv2 Q8_0 0.4 0.34 code-llama-instruct 7 ggufv2 Q8_0 0.4 0.37 code-llama-instruct 7 ggufv2 Q5_K_M 0.39 0.34 llama-2-chat 13 ggufv2 Q3_K_M 0.39 0.33 llama-2-chat 13 ggufv2 Q5_K_M 0.39 0.33 code-llama-instruct 7 ggufv2 Q2_K 0.38 0.29 code-llama-instruct 34 ggufv2 Q4_K_M 0.38 0.35 code-llama-instruct 7 ggufv2 Q6_K 0.38 0.39 code-llama-instruct 34 ggufv2 Q5_K_M 0.38 0.38 llama-2-chat 70 ggufv2 Q2_K 0.38 0.35 llama-2-chat 13 ggufv2 Q6_K 0.37 0.34 code-llama-instruct 34 ggufv2 Q8_0 0.37 0.35 mistral-instruct-v0.2 7 ggufv2 Q2_K 0.37 0.29 mistral-instruct-v0.2 7 ggufv2 Q4_K_M 0.37 0.35 code-llama-instruct 34 ggufv2 Q6_K 0.37 0.36 llama-2-chat 13 ggufv2 Q4_K_M 0.36 0.34 mixtral-instruct-v0.1 46,7 ggufv2 Q4_K_M 0.35 0.3 mixtral-instruct-v0.1 46,7 ggufv2 Q5_K_M 0.34 0.31 mixtral-instruct-v0.1 46,7 ggufv2 Q6_K 0.34 0.29 llama-2-chat 7 ggufv2 Q5_K_M 0.34 0.29 mixtral-instruct-v0.1 46,7 ggufv2 Q3_K_M 0.33 0.28 llama-2-chat 7 ggufv2 Q2_K 0.33 0.32 code-llama-instruct 13 ggufv2 Q4_K_M 0.33 0.31 mixtral-instruct-v0.1 46,7 ggufv2 Q8_0 0.33 0.25 mixtral-instruct-v0.1 46,7 ggufv2 Q2_K 0.32 0.27 llama-2-chat 7 ggufv2 Q8_0 0.31 0.28 llama-2-chat 7 ggufv2 Q6_K 0.31 0.29 llama-2-chat 7 ggufv2 Q4_K_M 0.3 0.29 llama-2-chat 7 ggufv2 Q3_K_M 0.3 0.33 llama-2-chat 13 ggufv2 Q2_K 0.28 0.29 code-llama-instruct 13 ggufv2 Q2_K 0.17 0.34 code-llama-instruct 13 ggufv2 Q3_K_M 0.15 0.34 <p></p>"},{"location":"BioChatter/benchmark/overview/#scores-of-all-tasks","title":"Scores of all tasks","text":"<p>Wide table; you may need to scroll horizontally to see all columns. Table sorted by median score in descending order. Click the column names to reorder.</p> Full model name naive_query_generation_using_schema entity_selection sourcedata_info_extraction explicit_relevance_of_single_fragments property_selection relationship_selection api_calling query_generation medical_exam end_to_end_query_generation implicit_relevance_of_multiple_fragments multimodal_answer python_api_calling property_exists python_api_calling_reduced Mean Accuracy Median Accuracy SD claude-3-opus-20240229 0.733333 1 0.691235 0.833333 0.421875 0 nan 0.944444 0.805556 0.655556 1 nan nan 1 nan 0.73503 0.770293 0.276411 gpt-3.5-turbo-0125 0.514451 0.978261 0.510032 1 0.36747 1 0.746479 0.953757 0.671322 0.919075 0.9 nan 0.615789 0.789474 0.759615 0.766123 0.766123 0.191717 claude-3-5-sonnet-20240620 0.633333 1 0.756088 1 0.375 0 nan 0.966667 0.7737 0.733333 1 nan nan 0.866667 nan 0.736799 0.764894 0.283502 gpt-3.5-turbo-0613 0.5 0.888889 0.575381 1 0.3625 0.5 nan 0.946667 nan 0.833333 1 nan nan 0.755556 nan 0.736233 0.755556 0.211926 llama-3.1-instruct:8:ggufv2:Q6_K 0.633333 1 0.394469 1 0.46875 0 nan 0.955556 0.751748 0.733333 0.833333 nan nan 0.833333 nan 0.69126 0.742541 0.278015 llama-3.1-instruct:70:ggufv2:IQ2_M 0.633333 1 0.626498 1 0.328125 0 nan 0.955556 0.772881 0.6 1 nan nan 0.916667 nan 0.712096 0.742489 0.293676 llama-3.1-instruct:8:ggufv2:Q5_K_M 0.7 1 0.380477 1 0.4375 0 nan 0.933333 0.749117 0.733333 0.833333 nan nan 0.833333 nan 0.690948 0.741225 0.279278 gpt-4-0613 0.682081 0.920635 0.668903 1 0.38253 0.695652 0.619048 0.959538 0.730912 0.878613 1 nan 0.489474 0.666667 0.769231 0.747377 0.730912 0.174471 llama-3.1-instruct:70:ggufv2:IQ4_XS 0.566667 1 0.699238 1 0.375 0 nan 0.955556 0.822581 0.6 1 nan nan 0.75 nan 0.706276 0.728138 0.285226 gpt-4o-2024-08-06 0.528302 1 0.711185 1 0.425439 1 nan 0.874214 0.850211 0.830189 0.666667 nan 0.363158 0.179894 0.740385 0.705357 0.725785 0.243719 llama-3.1-instruct:8:ggufv2:Q8_0 0.660377 1 0.38907 1 0.565789 0.142857 nan 0.937107 0.765734 0.773585 1 nan nan 0.161765 nan 0.67239 0.719062 0.295232 llama-3.1-instruct:8:ggufv2:Q3_K_L 0.622642 1 0.360379 1 0.486842 0.142857 nan 0.943396 0.768421 0.811321 0.833333 nan nan 0.24 nan 0.655381 0.711901 0.280463 llama-3.1-instruct:8:ggufv2:Q4_K_M 0.660377 0.92 0.382027 1 0.513158 0.285714 nan 0.924528 0.741935 0.735849 0.833333 nan nan 0.150538 nan 0.649769 0.698113 0.257418 llama-3.1-instruct:8:ggufv2:IQ4_XS 0.646018 0.893939 0.414621 1 0.387255 0 nan 0.946903 0.756944 0.743363 0.833333 nan nan 0.439394 nan 0.641979 0.69469 0.276449 gpt-4-1106-preview nan nan nan nan nan nan nan nan nan nan nan nan 0.616216 nan 0.767677 0.691946 0.691946 0.0618335 gpt-4-0125-preview 0.456647 0.824561 0.689705 1 0.0301205 0.782609 0.793651 0.83815 0.777159 0.109827 0.5 nan 0.508772 0.619048 0.769231 0.621391 0.689705 0.259268 gpt-4o-mini-2024-07-18 0.537572 0.921053 0.684553 0.833333 0.388554 0.130435 0.714286 0.953757 0.840796 0.687861 0.5 0.98 0.578947 0.52439 0.730769 0.667087 0.686207 0.216242 gpt-4-turbo-2024-04-09 0.508671 1 0.650369 1 0.325301 0.130435 nan 0.83237 0.83701 0.635838 1 0.99 0.259649 0.657143 0.762821 0.684972 0.684972 0.270175 gpt-4o-2024-05-13 0.537572 1 0.653946 1 0.0301205 0.130435 0.809524 0.809249 0.763501 0.115607 0.7 0.96 nan 0.526316 nan 0.618175 0.676973 0.312259 llama-3.1-instruct:70:ggufv2:Q3_K_S 0.633333 1 0.642336 1 0.375 0 nan 0.966667 0.8 0.6 1 nan nan 0.625 nan 0.694758 0.668547 0.28438 llama-3-instruct:8:ggufv2:Q8_0 0.666667 0.875 0.188555 1 0.28125 0 nan 0.92 0.640669 0 1 nan nan 0.725 nan 0.572467 0.653668 0.35454 llama-3-instruct:8:ggufv2:Q4_K_M 0.666667 0.861111 0.116871 1 0.109375 0 nan 0.92 0.624884 0 1 nan nan 0.775 nan 0.552173 0.645775 0.376754 llama-3-instruct:8:ggufv2:Q6_K 0.666667 0.875 0.162657 1 0.28125 0 nan 0.926667 0.623955 0 1 nan nan 0.775 nan 0.573745 0.645311 0.359165 openhermes-2.5:7:ggufv2:Q6_K 0.557078 0.7675 0.619167 1 0.126404 0.896552 nan 0.890411 0.57423 0.273973 1 nan nan 0.113379 nan 0.619881 0.619524 0.300867 llama-3-instruct:8:ggufv2:Q5_K_M 0.6 0.875 0.166434 1 0.1875 0 nan 0.926667 0.635097 0 1 nan nan 0.65 nan 0.549154 0.617549 0.360565 openhermes-2.5:7:ggufv2:Q5_K_M 0.56621 0.758808 0.579916 1 0.196629 0.896552 nan 0.917808 0.571429 0.26484 1 nan nan 0.120988 nan 0.624835 0.602375 0.293641 openhermes-2.5:7:ggufv2:Q8_0 0.497717 0.628118 0.600829 1 0.196629 0.896552 nan 0.90411 0.577031 0.237443 1 nan nan 0.100629 nan 0.603551 0.60219 0.296591 openhermes-2.5:7:ggufv2:Q4_K_M 0.479452 0.66967 0.597281 1 0.126404 0.896552 nan 0.894977 0.586368 0.246575 1 nan nan 0.140921 nan 0.603473 0.600377 0.299216 openhermes-2.5:7:ggufv2:Q3_K_M 0.47032 1 0.554488 1 0.171348 1 nan 0.917808 0.563959 0.287671 0.5 nan nan 0.156805 nan 0.602036 0.559223 0.301462 gpt-4o-2024-11-20 nan nan nan nan nan nan nan nan nan nan nan nan 0.377193 nan 0.730769 0.553981 0.553981 0.144347 code-llama-instruct:34:ggufv2:Q2_K 0.566667 0 nan 0.5 0 0.5 nan 0.686667 nan 0 1 nan nan 0.75 nan 0.444815 0.5 0.328199 openhermes-2.5:7:ggufv2:Q2_K 0.420091 0.604444 0.444054 1 0.0168539 0.655172 nan 0.917808 0.539106 0.159817 0.5 nan nan 0.18638 nan 0.494884 0.497442 0.27656 code-llama-instruct:7:ggufv2:Q3_K_M 0.426667 0.5 nan 0.833333 0 0.25 nan 0.873333 nan 0 0.7 nan nan 0.8 nan 0.487037 0.493519 0.307716 code-llama-instruct:7:ggufv2:Q4_K_M 0.653333 0.333333 0.138732 1 0 0 nan 0.966667 nan 0 1 nan nan 0.6 nan 0.469207 0.469207 0.38731 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0.466667 0.444444 0.385754 1 0 0 nan 0.826667 0.364146 0 1 nan nan 0.688889 nan 0.470597 0.455556 0.34385 mistral-instruct-v0.2:7:ggufv2:Q6_K 0.433333 0.5 0.367412 1 0.046875 0 nan 0.833333 0.366947 0 1 nan nan 0.65 nan 0.472536 0.452935 0.337974 code-llama-instruct:34:ggufv2:Q3_K_M 0.6 0 nan 0.5 0 0.25 nan 0.786667 nan 0 0.5 nan nan 0.875 nan 0.390185 0.445093 0.306514 chatglm3:6:ggmlv3:q4_0 0.48 0.75 0.188284 0.733333 0.2875 0.4 nan 0.553333 0.426704 0 1 nan nan 0.275 nan 0.463105 0.444905 0.260423 llama-2-chat:70:ggufv2:Q5_K_M 0.36 0.444444 0.210166 1 0 0.25 nan 0.906667 nan 0 0.9 nan nan 0.777778 nan 0.484905 0.444444 0.346535 llama-2-chat:70:ggufv2:Q4_K_M 0.42 0.444444 0.240936 1 0 0.25 nan 0.92 nan 0 1 nan nan 0.755556 nan 0.503094 0.444444 0.354692 code-llama-instruct:13:ggufv2:Q6_K 0.54 0 nan 0.833333 0 0 nan 0.793333 nan 0 0.5 nan nan 0.825 nan 0.387963 0.443981 0.345581 code-llama-instruct:13:ggufv2:Q8_0 0.566667 0 nan 0.833333 0 0 nan 0.766667 nan 0 0.5 nan nan 0.75 nan 0.37963 0.439815 0.334971 code-llama-instruct:13:ggufv2:Q5_K_M 0.566667 0 nan 0.666667 0 0 nan 0.78 nan 0 0.5 nan nan 0.775 nan 0.36537 0.432685 0.320506 llama-2-chat:70:ggufv2:Q3_K_M 0.413333 0.333333 0.197898 1 0.171875 0 nan 0.906667 nan 0 0.5 nan nan 0.777778 nan 0.430088 0.413333 0.327267 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0.466667 0.333333 0.368974 1 0.046875 0 nan 0.773333 0.360411 0 1 nan nan 0.666667 nan 0.456024 0.412499 0.335885 mistral-instruct-v0.2:7:ggufv2:Q8_0 0.433333 0.333333 0.351684 1 0.0375 0 nan 0.846667 0.366947 0 0.9 nan nan 0.644444 nan 0.446719 0.40014 0.330107 llama-2-chat:13:ggufv2:Q8_0 0.48 0 0.0762457 1 0 0 nan 0.786667 0.431373 0 0.5 nan nan 0.711111 nan 0.362309 0.396841 0.335904 code-llama-instruct:7:ggufv2:Q8_0 0.4 0 nan 1 0 0 nan 0.96 nan 0 0.5 nan nan 0.666667 nan 0.391852 0.395926 0.37338 code-llama-instruct:7:ggufv2:Q5_K_M 0.4 0.111111 nan 0.833333 0 0 nan 0.96 nan 0 0.5 nan nan 0.688889 nan 0.388148 0.394074 0.340156 llama-2-chat:13:ggufv2:Q3_K_M 0.48 0 0.112631 1 0 0 nan 0.68 0.428571 0 0.5 nan nan 0.733333 nan 0.357685 0.393128 0.325419 llama-2-chat:13:ggufv2:Q5_K_M 0.433333 0 0.0766167 1 0 0 nan 0.746667 0.431373 0 0.5 nan nan 0.644444 nan 0.348403 0.389888 0.32518 code-llama-instruct:7:ggufv2:Q2_K 0.533333 0.25 nan 0.333333 0.0625 0.25 nan 0.92 nan 0 0.7 nan nan 0.8 nan 0.427685 0.380509 0.292686 code-llama-instruct:34:ggufv2:Q4_K_M 0.466667 0 nan 0.5 0 0 nan 0.906667 nan 0 0.4 nan nan 0.975 nan 0.360926 0.380463 0.350483 code-llama-instruct:7:ggufv2:Q6_K 0.333333 0 nan 0.833333 0 0 nan 0.96 nan 0 0.9 nan nan 0.775 nan 0.422407 0.37787 0.391629 code-llama-instruct:34:ggufv2:Q5_K_M 0.466667 0.125 nan 0.333333 0 0 nan 0.9 nan 0 1 nan nan 0.95 nan 0.419444 0.376389 0.384096 llama-2-chat:70:ggufv2:Q2_K 0.473333 0 0.215047 1 0 0 nan 0.9 nan 0 0.5 nan nan 0.666667 nan 0.375505 0.375505 0.352226 llama-2-chat:13:ggufv2:Q6_K 0.386667 0 0.0781337 1 0 0 nan 0.813333 0.428571 0 0.5 nan nan 0.775 nan 0.361973 0.37432 0.342819 code-llama-instruct:34:ggufv2:Q8_0 0.466667 0.25 nan 0.333333 0 0 nan 0.86 nan 0 0.9 nan nan 0.925 nan 0.415 0.374167 0.353285 mistral-instruct-v0.2:7:ggufv2:Q2_K 0.573333 0.222222 0.331261 1 0 0 nan 0.693333 0.352941 0 0.5 nan nan 0.6 nan 0.388463 0.370702 0.294881 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0.366667 0.333333 0.347025 1 0 0 nan 0.826667 0.365079 0 1 nan nan 0.688889 nan 0.447969 0.365873 0.348328 code-llama-instruct:34:ggufv2:Q6_K 0.473333 0.125 nan 0.333333 0 0 nan 0.853333 nan 0 0.9 nan nan 0.9 nan 0.398333 0.365833 0.356636 llama-2-chat:13:ggufv2:Q4_K_M 0.366667 0 0.0888675 1 0 0 nan 0.76 0.428571 0 0.5 nan nan 0.777778 nan 0.356535 0.361601 0.336686 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0.426667 0.333333 0.193786 0.166667 0.1625 0 nan 0.76 0.368814 0 1 nan nan 0.755556 nan 0.378848 0.351074 0.301567 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0.333333 0.422222 0.235659 0 0 0.25 nan 0.84 0.352941 0 1 nan nan 0.711111 nan 0.376842 0.343137 0.313874 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0.333333 0.475 0.225524 0 0 0.25 nan 0.826667 0.34267 0 0.7 nan nan 0.85 nan 0.363927 0.338002 0.289705 llama-2-chat:7:ggufv2:Q5_K_M 0.3379 0.57037 0.0697591 1 0.0337079 0 nan 0.547945 0.40056 0 0.6 nan nan 0.155556 nan 0.3378 0.33785 0.293711 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0.38 0.333333 0.229622 0 0.065625 0.25 nan 0.893333 nan 0 0.5 nan nan 0.777778 nan 0.342969 0.333333 0.278279 llama-2-chat:7:ggufv2:Q2_K 0.164384 0.410256 0.0361865 0.833333 0 0 nan 0.611872 0.369748 0 1 nan nan 0.324786 nan 0.340961 0.332873 0.320018 code-llama-instruct:13:ggufv2:Q4_K_M 0.533333 0 nan 0.333333 0 0 nan 0.833333 nan 0 0.5 nan nan 0.775 nan 0.330556 0.331944 0.30939 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0.386667 0.311111 0.189177 0.133333 0 0.25 nan 0.846667 0.358543 0 0.6 nan nan 0.666667 nan 0.340197 0.325654 0.248216 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0.48 0 0.157514 0.333333 0 0 nan 0.726667 0.329599 0 0.6 nan nan 0.733333 nan 0.305495 0.317547 0.269925 llama-2-chat:7:ggufv2:Q8_0 0.292237 0.481481 0.0847297 1 0 0.103448 nan 0.589041 0.40056 0 0.5 nan nan 0.163399 nan 0.328627 0.310432 0.278624 llama-2-chat:7:ggufv2:Q6_K 0.292237 0.553846 0.0614608 1 0 0 nan 0.561644 0.406162 0 0.5 nan nan 0.177778 nan 0.323012 0.307625 0.290181 llama-2-chat:7:ggufv2:Q4_K_M 0.273973 0.57037 0.0852494 1 0 0 nan 0.611872 0.40056 0 0.5 nan nan 0.251852 nan 0.335807 0.30489 0.291028 llama-2-chat:7:ggufv2:Q3_K_M 0.228311 0.435897 0.0650717 1 0.0898876 0.103448 nan 0.589041 0.394958 0 1 nan nan 0.207407 nan 0.374002 0.301156 0.326263 llama-2-chat:13:ggufv2:Q2_K 0.366667 0 0.0649389 1 0 0 nan 0.433333 0.414566 0 0.5 nan nan 0.288889 nan 0.278945 0.283917 0.285171 code-llama-instruct:13:ggufv2:Q2_K 0.566667 0 nan 0.0333333 0 0 nan 0.82 nan 0 0.4 nan nan 0.875 nan 0.299444 0.166389 0.336056 code-llama-instruct:13:ggufv2:Q3_K_M 0.533333 0.45 nan 0 0 0 nan 0.833333 nan 0 0 nan nan 0.85 nan 0.296296 0.148148 0.336707"},{"location":"BioChatter/benchmark/results/","title":"Benchmark - All Results","text":""},{"location":"BioChatter/benchmark/results/#biocypher-query-generation","title":"BioCypher query generation","text":"<p>In this set of tasks, we test LLM abilities to generate queries for a BioCypher Knowledge Graph using BioChatter. The <code>schema_config.yaml</code> of the BioCypher Knowledge Graph and a natural language query are passed to BioChatter.</p> <p>Individual steps of the query generation process are tested separately, as well as the end-to-end performance of the process.</p> Entity selectionRelationship selectionProperty selectionProperty confabulationNaive query generationBioChatter query generationEnd to end query generation Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 22 22 0 1 3 claude-3-opus-20240229 24 24 0 1 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 24 24 0 1 3 openhermes-2.5:7:ggufv2:Q3_K_M 279 279 0 1 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 24 24 0 1 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 24 24 0 1 3 llama-3.1-instruct:70:ggufv2:IQ2_M 24 24 0 1 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 132 132 0 1 3 llama-3.1-instruct:8:ggufv2:Q6_K 24 24 0 1 3 gpt-4o-2024-05-13 70 70 0 1 5 gpt-4-turbo-2024-04-09 64 64 0 1 5 gpt-4o-2024-08-06 42 42 0 1 3 llama-3.1-instruct:8:ggufv2:Q8_0 186 186 0 1 3 gpt-3.5-turbo-0125 45 46 0 0.978261 5 gpt-4o-mini-2024-07-18 70 76 0 0.921053 5 gpt-4-0613 58 63 0 0.920635 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 138 150 0 0.92 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 59 66 0 0.893939 3 gpt-3.5-turbo-0613 40 45 0 0.888889 5 llama-3-instruct:8:ggufv2:Q8_0 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q6_K 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q5_K_M 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q4_K_M 31 36 0 0.861111 5 gpt-4-0125-preview 47 57 0 0.824561 5 openhermes-2.5:7:ggufv2:Q6_K 307 400 0 0.7675 5 openhermes-2.5:7:ggufv2:Q5_K_M 280 369 0 0.758808 5 chatglm3:6:ggmlv3:q4_0 30 40 0 0.75 5 openhermes-2.5:7:ggufv2:Q4_K_M 223 333 0 0.66967 5 openhermes-2.5:7:ggufv2:Q8_0 277 441 0 0.628118 5 openhermes-2.5:7:ggufv2:Q2_K 136 225 0 0.604444 5 llama-2-chat:7:ggufv2:Q4_K_M 77 135 0 0.57037 5 llama-2-chat:7:ggufv2:Q5_K_M 77 135 0 0.57037 5 llama-2-chat:7:ggufv2:Q6_K 72 130 0 0.553846 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 20 40 0 0.5 5 code-llama-instruct:7:ggufv2:Q3_K_M 20 40 0 0.5 5 llama-2-chat:7:ggufv2:Q8_0 65 135 0 0.481481 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 19 40 0 0.475 5 code-llama-instruct:13:ggufv2:Q3_K_M 18 40 0 0.45 5 llama-2-chat:70:ggufv2:Q4_K_M 20 45 0 0.444444 5 llama-2-chat:70:ggufv2:Q5_K_M 20 45 0 0.444444 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 20 45 0 0.444444 5 llama-2-chat:7:ggufv2:Q3_K_M 51 117 0 0.435897 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 19 45 0 0.422222 5 llama-2-chat:7:ggufv2:Q2_K 48 117 0 0.410256 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 15 45 0 0.333333 5 llama-2-chat:70:ggufv2:Q3_K_M 15 45 0 0.333333 5 code-llama-instruct:7:ggufv2:Q4_K_M 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 14 45 0 0.311111 5 code-llama-instruct:34:ggufv2:Q8_0 10 40 0 0.25 5 code-llama-instruct:7:ggufv2:Q2_K 10 40 0 0.25 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 10 45 0 0.222222 5 code-llama-instruct:34:ggufv2:Q5_K_M 5 40 0 0.125 5 code-llama-instruct:34:ggufv2:Q6_K 5 40 0 0.125 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 45 0 0.111111 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 40 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 45 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 45 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 40 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 45 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 45 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 40 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 45 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 45 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 69 69 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 87 87 0 1 5 gpt-4o-2024-08-06 63 63 0 1 3 openhermes-2.5:7:ggufv2:Q5_K_M 78 87 0 0.896552 5 openhermes-2.5:7:ggufv2:Q4_K_M 78 87 0 0.896552 5 openhermes-2.5:7:ggufv2:Q8_0 78 87 0 0.896552 5 openhermes-2.5:7:ggufv2:Q6_K 78 87 0 0.896552 5 gpt-4-0125-preview 54 69 0 0.782609 5 gpt-4-0613 48 69 0 0.695652 5 openhermes-2.5:7:ggufv2:Q2_K 57 87 0 0.655172 5 code-llama-instruct:34:ggufv2:Q2_K 30 60 0 0.5 5 gpt-3.5-turbo-0613 30 60 0 0.5 5 chatglm3:6:ggmlv3:q4_0 24 60 0 0.4 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 18 63 0 0.285714 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 15 60 0 0.25 5 llama-2-chat:70:ggufv2:Q4_K_M 15 60 0 0.25 5 code-llama-instruct:7:ggufv2:Q2_K 15 60 0 0.25 5 code-llama-instruct:7:ggufv2:Q3_K_M 15 60 0 0.25 5 llama-2-chat:70:ggufv2:Q5_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 15 60 0 0.25 5 code-llama-instruct:34:ggufv2:Q3_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 15 60 0 0.25 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 9 63 0 0.142857 3 llama-3.1-instruct:8:ggufv2:Q8_0 9 63 0 0.142857 3 gpt-4-turbo-2024-04-09 9 69 0 0.130435 5 gpt-4o-2024-05-13 9 69 0 0.130435 5 gpt-4o-mini-2024-07-18 9 69 0 0.130435 5 llama-2-chat:7:ggufv2:Q8_0 9 87 0 0.103448 5 llama-2-chat:7:ggufv2:Q3_K_M 9 87 0 0.103448 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 60 0 0 5 claude-3-opus-20240229 0 36 0 0 3 code-llama-instruct:13:ggufv2:Q2_K 0 60 0 0 5 claude-3-5-sonnet-20240620 0 36 0 0 3 code-llama-instruct:13:ggufv2:Q8_0 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 87 0 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 87 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 87 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 87 0 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q8_0 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q4_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 60 0 0 5 llama-3.1-instruct:8:ggufv2:Q6_K 0 36 0 0 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 0 36 0 0 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 0 45 0 0 3 llama-3.1-instruct:70:ggufv2:IQ2_M 0 36 0 0 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 0 36 0 0 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 0 36 0 0 3 llama-3-instruct:8:ggufv2:Q6_K 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q5_K_M 0 60 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 60 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 60 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations llama-3.1-instruct:8:ggufv2:Q8_0 129 228 0 0.565789 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 117 228 0 0.513158 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 111 228 0 0.486842 3 llama-3.1-instruct:8:ggufv2:Q6_K 90 192 0 0.46875 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 84 192 0 0.4375 3 gpt-4o-2024-08-06 97 228 1.1547 0.425439 3 claude-3-opus-20240229 81 192 0 0.421875 3 gpt-4o-mini-2024-07-18 129 332 0.547723 0.388554 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 79 204 0 0.387255 3 gpt-4-0613 127 332 0 0.38253 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 72 192 0 0.375 3 claude-3-5-sonnet-20240620 72 192 0 0.375 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 72 192 0 0.375 3 gpt-3.5-turbo-0125 122 332 0 0.36747 5 gpt-3.5-turbo-0613 116 320 0 0.3625 5 llama-3.1-instruct:70:ggufv2:IQ2_M 63 192 0 0.328125 3 gpt-4-turbo-2024-04-09 108 332 0.894427 0.325301 5 chatglm3:6:ggmlv3:q4_0 92 320 0 0.2875 5 llama-3-instruct:8:ggufv2:Q8_0 90 320 0 0.28125 5 llama-3-instruct:8:ggufv2:Q6_K 90 320 0 0.28125 5 openhermes-2.5:7:ggufv2:Q8_0 70 356 0 0.196629 5 openhermes-2.5:7:ggufv2:Q5_K_M 70 356 0 0.196629 5 llama-3-instruct:8:ggufv2:Q5_K_M 60 320 0 0.1875 5 llama-2-chat:70:ggufv2:Q3_K_M 55 320 0 0.171875 5 openhermes-2.5:7:ggufv2:Q3_K_M 61 356 0 0.171348 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 52 320 0 0.1625 5 openhermes-2.5:7:ggufv2:Q6_K 45 356 0 0.126404 5 openhermes-2.5:7:ggufv2:Q4_K_M 45 356 0 0.126404 5 llama-3-instruct:8:ggufv2:Q4_K_M 35 320 0 0.109375 5 llama-2-chat:7:ggufv2:Q3_K_M 32 356 0 0.0898876 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 21 320 0 0.065625 5 code-llama-instruct:7:ggufv2:Q2_K 20 320 0 0.0625 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 15 320 0 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 15 320 0 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 12 320 0 0.0375 5 llama-2-chat:7:ggufv2:Q5_K_M 12 356 0 0.0337079 5 gpt-4o-2024-05-13 10 332 0 0.0301205 5 gpt-4-0125-preview 10 332 0 0.0301205 5 openhermes-2.5:7:ggufv2:Q2_K 6 356 0 0.0168539 5 code-llama-instruct:13:ggufv2:Q2_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 320 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 356 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 356 0 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 320 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 356 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 320 0 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 320 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 320 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 356 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 320 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 24 24 0 1 3 code-llama-instruct:34:ggufv2:Q4_K_M 39 40 0 0.975 5 code-llama-instruct:34:ggufv2:Q5_K_M 38 40 0 0.95 5 code-llama-instruct:34:ggufv2:Q8_0 37 40 0 0.925 5 llama-3.1-instruct:70:ggufv2:IQ2_M 22 24 0.57735 0.916667 3 code-llama-instruct:34:ggufv2:Q6_K 36 40 0 0.9 5 code-llama-instruct:34:ggufv2:Q3_K_M 35 40 0 0.875 5 code-llama-instruct:13:ggufv2:Q2_K 35 40 0 0.875 5 claude-3-5-sonnet-20240620 26 30 0.57735 0.866667 3 code-llama-instruct:13:ggufv2:Q3_K_M 34 40 0 0.85 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 34 40 0 0.85 5 llama-3.1-instruct:8:ggufv2:Q6_K 20 24 1.1547 0.833333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 20 24 1.1547 0.833333 3 code-llama-instruct:13:ggufv2:Q6_K 33 40 0 0.825 5 code-llama-instruct:7:ggufv2:Q3_K_M 36 45 0 0.8 5 code-llama-instruct:7:ggufv2:Q2_K 32 40 0 0.8 5 gpt-3.5-turbo-0125 45 57 0 0.789474 5 llama-2-chat:70:ggufv2:Q5_K_M 35 45 0 0.777778 5 llama-2-chat:70:ggufv2:Q3_K_M 35 45 0 0.777778 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 35 45 0 0.777778 5 llama-2-chat:13:ggufv2:Q4_K_M 35 45 0 0.777778 5 llama-3-instruct:8:ggufv2:Q4_K_M 31 40 0 0.775 5 code-llama-instruct:7:ggufv2:Q6_K 31 40 0 0.775 5 code-llama-instruct:13:ggufv2:Q4_K_M 31 40 0 0.775 5 code-llama-instruct:13:ggufv2:Q5_K_M 31 40 0 0.775 5 llama-3-instruct:8:ggufv2:Q6_K 31 40 0 0.775 5 llama-2-chat:13:ggufv2:Q6_K 31 40 0 0.775 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 34 45 0 0.755556 5 gpt-3.5-turbo-0613 34 45 0 0.755556 5 llama-2-chat:70:ggufv2:Q4_K_M 34 45 0 0.755556 5 code-llama-instruct:34:ggufv2:Q2_K 30 40 0 0.75 5 code-llama-instruct:13:ggufv2:Q8_0 30 40 0 0.75 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 18 24 0 0.75 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 33 45 0 0.733333 5 llama-2-chat:13:ggufv2:Q3_K_M 33 45 0 0.733333 5 llama-3-instruct:8:ggufv2:Q8_0 29 40 0 0.725 5 llama-2-chat:13:ggufv2:Q8_0 32 45 0 0.711111 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 32 45 0 0.711111 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 31 45 0 0.688889 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 31 45 0 0.688889 5 code-llama-instruct:7:ggufv2:Q5_K_M 31 45 0 0.688889 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 30 45 0 0.666667 5 code-llama-instruct:7:ggufv2:Q8_0 30 45 0 0.666667 5 gpt-4-0613 46 69 0 0.666667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 30 45 0 0.666667 5 llama-2-chat:70:ggufv2:Q2_K 30 45 0 0.666667 5 gpt-4-turbo-2024-04-09 46 70 0 0.657143 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 26 40 0 0.65 5 llama-3-instruct:8:ggufv2:Q5_K_M 26 40 0 0.65 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 29 45 0 0.644444 5 llama-2-chat:13:ggufv2:Q5_K_M 29 45 0 0.644444 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 15 24 0 0.625 3 gpt-4-0125-preview 39 63 0 0.619048 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 27 45 0 0.6 5 code-llama-instruct:7:ggufv2:Q4_K_M 27 45 0 0.6 5 gpt-4o-2024-05-13 40 76 0 0.526316 5 gpt-4o-mini-2024-07-18 43 82 0.547723 0.52439 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 29 66 0 0.439394 3 llama-2-chat:7:ggufv2:Q2_K 38 117 0.57735 0.324786 5 llama-2-chat:13:ggufv2:Q2_K 13 45 0 0.288889 5 chatglm3:6:ggmlv3:q4_0 11 40 0 0.275 5 llama-2-chat:7:ggufv2:Q4_K_M 34 135 1.1547 0.251852 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 36 150 2.88675 0.24 3 llama-2-chat:7:ggufv2:Q3_K_M 28 135 1.1547 0.207407 5 openhermes-2.5:7:ggufv2:Q2_K 52 279 0.57735 0.18638 5 gpt-4o-2024-08-06 34 189 0.57735 0.179894 3 llama-2-chat:7:ggufv2:Q6_K 24 135 0 0.177778 5 llama-2-chat:7:ggufv2:Q8_0 25 153 1 0.163399 5 llama-3.1-instruct:8:ggufv2:Q8_0 33 204 1.73205 0.161765 3 openhermes-2.5:7:ggufv2:Q3_K_M 53 338 0.57735 0.156805 5 llama-2-chat:7:ggufv2:Q5_K_M 21 135 0.57735 0.155556 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 28 186 0.57735 0.150538 3 openhermes-2.5:7:ggufv2:Q4_K_M 52 369 0 0.140921 5 openhermes-2.5:7:ggufv2:Q5_K_M 49 405 0.57735 0.120988 5 openhermes-2.5:7:ggufv2:Q6_K 50 441 0.57735 0.113379 5 openhermes-2.5:7:ggufv2:Q8_0 48 477 1.1547 0.100629 5 Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 66 90 0 0.733333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 63 90 0 0.7 3 gpt-4-0613 118 173 0 0.682081 5 llama-3-instruct:8:ggufv2:Q4_K_M 100 150 0 0.666667 5 llama-3-instruct:8:ggufv2:Q8_0 100 150 0 0.666667 5 llama-3-instruct:8:ggufv2:Q6_K 100 150 0 0.666667 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 105 159 0 0.660377 3 llama-3.1-instruct:8:ggufv2:Q8_0 105 159 0 0.660377 3 code-llama-instruct:7:ggufv2:Q4_K_M 98 150 0 0.653333 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 73 113 0 0.646018 3 llama-3.1-instruct:70:ggufv2:IQ2_M 57 90 0 0.633333 3 claude-3-5-sonnet-20240620 57 90 0 0.633333 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 57 90 0 0.633333 3 llama-3.1-instruct:8:ggufv2:Q6_K 57 90 0 0.633333 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 99 159 0 0.622642 3 code-llama-instruct:34:ggufv2:Q3_K_M 90 150 0 0.6 5 llama-3-instruct:8:ggufv2:Q5_K_M 90 150 0 0.6 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 86 150 0 0.573333 5 code-llama-instruct:13:ggufv2:Q2_K 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q8_0 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q5_K_M 85 150 0 0.566667 5 code-llama-instruct:34:ggufv2:Q2_K 85 150 0 0.566667 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 51 90 0 0.566667 3 openhermes-2.5:7:ggufv2:Q5_K_M 124 219 0 0.56621 5 openhermes-2.5:7:ggufv2:Q6_K 122 219 0 0.557078 5 code-llama-instruct:13:ggufv2:Q6_K 81 150 0 0.54 5 gpt-4o-2024-05-13 93 173 0 0.537572 5 gpt-4o-mini-2024-07-18 93 173 0 0.537572 5 code-llama-instruct:7:ggufv2:Q2_K 80 150 0 0.533333 5 code-llama-instruct:13:ggufv2:Q3_K_M 80 150 0 0.533333 5 code-llama-instruct:13:ggufv2:Q4_K_M 80 150 0 0.533333 5 gpt-4o-2024-08-06 84 159 0 0.528302 3 gpt-3.5-turbo-0125 89 173 0 0.514451 5 gpt-4-turbo-2024-04-09 88 173 0 0.508671 5 gpt-3.5-turbo-0613 75 150 0 0.5 5 openhermes-2.5:7:ggufv2:Q8_0 109 219 0 0.497717 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 72 150 0 0.48 5 chatglm3:6:ggmlv3:q4_0 72 150 0 0.48 5 llama-2-chat:13:ggufv2:Q8_0 72 150 0 0.48 5 llama-2-chat:13:ggufv2:Q3_K_M 72 150 0 0.48 5 openhermes-2.5:7:ggufv2:Q4_K_M 105 219 0.57735 0.479452 5 llama-2-chat:70:ggufv2:Q2_K 71 150 0 0.473333 5 code-llama-instruct:34:ggufv2:Q6_K 71 150 0 0.473333 5 openhermes-2.5:7:ggufv2:Q3_K_M 103 219 0 0.47032 5 code-llama-instruct:34:ggufv2:Q4_K_M 70 150 0 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 70 150 0 0.466667 5 code-llama-instruct:34:ggufv2:Q5_K_M 70 150 0 0.466667 5 code-llama-instruct:34:ggufv2:Q8_0 70 150 0 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 70 150 0 0.466667 5 gpt-4-0125-preview 79 173 0 0.456647 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 65 150 0 0.433333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 65 150 0 0.433333 5 llama-2-chat:13:ggufv2:Q5_K_M 65 150 0 0.433333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 64 150 0 0.426667 5 code-llama-instruct:7:ggufv2:Q3_K_M 64 150 0 0.426667 5 openhermes-2.5:7:ggufv2:Q2_K 92 219 0 0.420091 5 llama-2-chat:70:ggufv2:Q4_K_M 63 150 0 0.42 5 llama-2-chat:70:ggufv2:Q3_K_M 62 150 0 0.413333 5 code-llama-instruct:7:ggufv2:Q8_0 60 150 0 0.4 5 code-llama-instruct:7:ggufv2:Q5_K_M 60 150 0 0.4 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 58 150 0 0.386667 5 llama-2-chat:13:ggufv2:Q6_K 58 150 0 0.386667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 57 150 0 0.38 5 llama-2-chat:13:ggufv2:Q2_K 55 150 0 0.366667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 55 150 0 0.366667 5 llama-2-chat:13:ggufv2:Q4_K_M 55 150 0 0.366667 5 llama-2-chat:70:ggufv2:Q5_K_M 54 150 0 0.36 5 llama-2-chat:7:ggufv2:Q5_K_M 74 219 0 0.3379 5 code-llama-instruct:7:ggufv2:Q6_K 50 150 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 50 150 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 50 150 0 0.333333 5 llama-2-chat:7:ggufv2:Q6_K 64 219 0 0.292237 5 llama-2-chat:7:ggufv2:Q8_0 64 219 0 0.292237 5 llama-2-chat:7:ggufv2:Q4_K_M 60 219 0 0.273973 5 llama-2-chat:7:ggufv2:Q3_K_M 50 219 0 0.228311 5 llama-2-chat:7:ggufv2:Q2_K 36 219 0 0.164384 5 Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 87 90 0 0.966667 3 code-llama-instruct:7:ggufv2:Q4_K_M 145 150 0 0.966667 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 87 90 0 0.966667 3 code-llama-instruct:7:ggufv2:Q6_K 144 150 0 0.96 5 code-llama-instruct:7:ggufv2:Q5_K_M 144 150 0 0.96 5 code-llama-instruct:7:ggufv2:Q8_0 144 150 0 0.96 5 gpt-4-0613 166 173 0 0.959538 5 llama-3.1-instruct:70:ggufv2:IQ2_M 86 90 1.1547 0.955556 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 86 90 1.1547 0.955556 3 llama-3.1-instruct:8:ggufv2:Q6_K 86 90 0.57735 0.955556 3 gpt-3.5-turbo-0125 165 173 0 0.953757 5 gpt-4o-mini-2024-07-18 165 173 0 0.953757 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 107 113 0.57735 0.946903 3 gpt-3.5-turbo-0613 142 150 0 0.946667 5 claude-3-opus-20240229 85 90 0.57735 0.944444 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 150 159 0 0.943396 3 llama-3.1-instruct:8:ggufv2:Q8_0 149 159 0.57735 0.937107 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 84 90 0 0.933333 3 llama-3-instruct:8:ggufv2:Q5_K_M 139 150 0 0.926667 5 llama-3-instruct:8:ggufv2:Q6_K 139 150 0 0.926667 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 147 159 0 0.924528 3 code-llama-instruct:7:ggufv2:Q2_K 138 150 0 0.92 5 llama-3-instruct:8:ggufv2:Q4_K_M 138 150 0 0.92 5 llama-2-chat:70:ggufv2:Q4_K_M 138 150 0 0.92 5 llama-3-instruct:8:ggufv2:Q8_0 138 150 0 0.92 5 openhermes-2.5:7:ggufv2:Q5_K_M 201 219 0.57735 0.917808 5 openhermes-2.5:7:ggufv2:Q2_K 201 219 0 0.917808 5 openhermes-2.5:7:ggufv2:Q3_K_M 201 219 3 0.917808 5 llama-2-chat:70:ggufv2:Q5_K_M 136 150 0 0.906667 5 code-llama-instruct:34:ggufv2:Q4_K_M 136 150 0 0.906667 5 llama-2-chat:70:ggufv2:Q3_K_M 136 150 0 0.906667 5 openhermes-2.5:7:ggufv2:Q8_0 198 219 0 0.90411 5 llama-2-chat:70:ggufv2:Q2_K 135 150 0 0.9 5 code-llama-instruct:34:ggufv2:Q5_K_M 135 150 0 0.9 5 openhermes-2.5:7:ggufv2:Q4_K_M 196 219 0.57735 0.894977 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 134 150 0 0.893333 5 openhermes-2.5:7:ggufv2:Q6_K 195 219 0 0.890411 5 gpt-4o-2024-08-06 139 159 2.3094 0.874214 3 code-llama-instruct:7:ggufv2:Q3_K_M 131 150 0 0.873333 5 code-llama-instruct:34:ggufv2:Q8_0 129 150 0 0.86 5 code-llama-instruct:34:ggufv2:Q6_K 128 150 0 0.853333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 127 150 0 0.846667 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 127 150 0 0.846667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 126 150 0 0.84 5 gpt-4-0125-preview 145 173 0 0.83815 5 code-llama-instruct:13:ggufv2:Q3_K_M 125 150 0 0.833333 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 125 150 0 0.833333 5 code-llama-instruct:13:ggufv2:Q4_K_M 125 150 0 0.833333 5 gpt-4-turbo-2024-04-09 144 173 0.447214 0.83237 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 124 150 0 0.826667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 124 150 0 0.826667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 124 150 0 0.826667 5 code-llama-instruct:13:ggufv2:Q2_K 123 150 0 0.82 5 llama-2-chat:13:ggufv2:Q6_K 122 150 0 0.813333 5 gpt-4o-2024-05-13 140 173 0 0.809249 5 code-llama-instruct:13:ggufv2:Q6_K 119 150 0 0.793333 5 code-llama-instruct:34:ggufv2:Q3_K_M 118 150 0 0.786667 5 llama-2-chat:13:ggufv2:Q8_0 118 150 0 0.786667 5 code-llama-instruct:13:ggufv2:Q5_K_M 117 150 0 0.78 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 116 150 0 0.773333 5 code-llama-instruct:13:ggufv2:Q8_0 115 150 0 0.766667 5 llama-2-chat:13:ggufv2:Q4_K_M 114 150 0 0.76 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 114 150 0 0.76 5 llama-2-chat:13:ggufv2:Q5_K_M 112 150 0 0.746667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 109 150 0 0.726667 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 104 150 0 0.693333 5 code-llama-instruct:34:ggufv2:Q2_K 103 150 0 0.686667 5 llama-2-chat:13:ggufv2:Q3_K_M 102 150 0 0.68 5 llama-2-chat:7:ggufv2:Q2_K 134 219 1.1547 0.611872 5 llama-2-chat:7:ggufv2:Q4_K_M 134 219 2.68223 0.611872 5 llama-2-chat:7:ggufv2:Q8_0 129 219 0 0.589041 5 llama-2-chat:7:ggufv2:Q3_K_M 129 219 1.1547 0.589041 5 llama-2-chat:7:ggufv2:Q6_K 123 219 0 0.561644 5 chatglm3:6:ggmlv3:q4_0 83 150 0 0.553333 5 llama-2-chat:7:ggufv2:Q5_K_M 120 219 0.57735 0.547945 5 llama-2-chat:13:ggufv2:Q2_K 65 150 0 0.433333 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 159 173 0 0.919075 5 gpt-4-0613 152 173 0 0.878613 5 gpt-3.5-turbo-0613 125 150 0 0.833333 5 gpt-4o-2024-08-06 132 159 1.1547 0.830189 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 129 159 0 0.811321 3 llama-3.1-instruct:8:ggufv2:Q8_0 123 159 0 0.773585 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 84 113 0 0.743363 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 117 159 0 0.735849 3 claude-3-5-sonnet-20240620 66 90 0 0.733333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 66 90 0 0.733333 3 llama-3.1-instruct:8:ggufv2:Q6_K 66 90 0 0.733333 3 gpt-4o-mini-2024-07-18 119 173 1.54266 0.687861 5 claude-3-opus-20240229 59 90 3.21455 0.655556 3 gpt-4-turbo-2024-04-09 110 173 0 0.635838 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 54 90 3.4641 0.6 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 54 90 0 0.6 3 llama-3.1-instruct:70:ggufv2:IQ2_M 54 90 0 0.6 3 openhermes-2.5:7:ggufv2:Q3_K_M 63 219 1 0.287671 5 openhermes-2.5:7:ggufv2:Q6_K 60 219 1.73205 0.273973 5 openhermes-2.5:7:ggufv2:Q5_K_M 58 219 0.57735 0.26484 5 openhermes-2.5:7:ggufv2:Q4_K_M 54 219 1.1547 0.246575 5 openhermes-2.5:7:ggufv2:Q8_0 52 219 1.1547 0.237443 5 openhermes-2.5:7:ggufv2:Q2_K 35 219 2.3094 0.159817 5 gpt-4o-2024-05-13 20 173 0 0.115607 5 gpt-4-0125-preview 19 173 0 0.109827 5 chatglm3:6:ggmlv3:q4_0 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 219 0 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 219 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 219 0 0 5 llama-2-chat:7:ggufv2:Q3_K_M 0 219 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 219 0 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 150 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 150 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 150 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q8_0 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 219 0 0 5 llama-3-instruct:8:ggufv2:Q6_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q5_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 150 0 0 5"},{"location":"BioChatter/benchmark/results/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>In this set of tasks, we test LLM abilities to generate answers to a given question using a RAG agent, or to judge the relevance of a RAG fragment to a given question. Instructions can be explicit (\"is this fragment relevant to the question?\") or implicit (just asking the question without instructions and evaluating whether the model responds with 'not enough information given').</p> Explicit relevance of single fragmentsImplicit relevance of single fragments Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 18 18 0 1 3 llama-2-chat:13:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:70:ggufv2:Q2_K 30 30 0 1 5 gpt-4o-2024-08-06 18 18 0 1 3 code-llama-instruct:7:ggufv2:Q8_0 30 30 0 1 5 gpt-3.5-turbo-0125 30 30 0 1 5 gpt-3.5-turbo-0613 30 30 0 1 5 gpt-4-0125-preview 30 30 0 1 5 gpt-4-0613 30 30 0 1 5 gpt-4-turbo-2024-04-09 30 30 0 1 5 gpt-4o-2024-05-13 30 30 0 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q2_K 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 18 18 0 1 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 18 18 0 1 3 llama-2-chat:13:ggufv2:Q8_0 30 30 0 1 5 llama-2-chat:13:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q2_K 30 30 0 1 5 llama-2-chat:7:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:7:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q3_K_M 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 18 18 0 1 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 18 18 0 1 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 18 18 0 1 3 llama-3.1-instruct:70:ggufv2:IQ2_M 18 18 0 1 3 llama-3-instruct:8:ggufv2:Q8_0 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q6_K 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q5_K_M 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q8_0 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:Q8_0 18 18 0 1 3 mistral-instruct-v0.2:7:ggufv2:Q2_K 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:Q5_K_M 18 18 0 1 3 llama-3.1-instruct:8:ggufv2:Q6_K 18 18 0 1 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 30 30 0 1 5 code-llama-instruct:7:ggufv2:Q6_K 25 30 0 0.833333 5 code-llama-instruct:7:ggufv2:Q5_K_M 25 30 0 0.833333 5 code-llama-instruct:13:ggufv2:Q6_K 25 30 0 0.833333 5 code-llama-instruct:13:ggufv2:Q8_0 25 30 0 0.833333 5 claude-3-opus-20240229 15 18 0 0.833333 3 code-llama-instruct:7:ggufv2:Q3_K_M 25 30 0 0.833333 5 llama-2-chat:7:ggufv2:Q2_K 25 30 0 0.833333 5 gpt-4o-mini-2024-07-18 25 30 0 0.833333 5 chatglm3:6:ggmlv3:q4_0 22 30 0 0.733333 5 code-llama-instruct:13:ggufv2:Q5_K_M 20 30 0 0.666667 5 code-llama-instruct:34:ggufv2:Q2_K 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q4_K_M 15 30 0 0.5 5 code-llama-instruct:13:ggufv2:Q4_K_M 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q5_K_M 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q6_K 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q8_0 10 30 0 0.333333 5 code-llama-instruct:7:ggufv2:Q2_K 10 30 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 10 30 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 5 30 0 0.166667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 4 30 0 0.133333 5 code-llama-instruct:13:ggufv2:Q2_K 1 30 0 0.0333333 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 30 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations chatglm3:6:ggmlv3:q4_0 10 10 0 1 5 claude-3-5-sonnet-20240620 6 6 0 1 3 claude-3-opus-20240229 6 6 0 1 3 code-llama-instruct:34:ggufv2:Q2_K 10 10 0 1 5 llama-2-chat:7:ggufv2:Q2_K 10 10 0 1 5 llama-2-chat:7:ggufv2:Q3_K_M 10 10 0 1 5 llama-2-chat:70:ggufv2:Q4_K_M 10 10 0 1 5 gpt-4-turbo-2024-04-09 10 10 0 1 5 gpt-3.5-turbo-0613 10 10 0 1 5 gpt-4-0613 10 10 0 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 10 10 0 1 5 code-llama-instruct:34:ggufv2:Q5_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 10 10 0 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 10 10 0 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 10 10 0 1 5 llama-3.1-instruct:8:ggufv2:Q8_0 6 6 0 1 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 10 10 0 1 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 6 6 0 1 3 llama-3.1-instruct:70:ggufv2:IQ2_M 6 6 0 1 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 6 6 0 1 3 llama-3-instruct:8:ggufv2:Q8_0 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q6_K 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q5_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q4_K_M 10 10 0 1 5 gpt-3.5-turbo-0125 9 10 0 0.9 5 code-llama-instruct:7:ggufv2:Q6_K 9 10 0 0.9 5 llama-2-chat:70:ggufv2:Q5_K_M 9 10 0 0.9 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 9 10 0 0.9 5 code-llama-instruct:34:ggufv2:Q8_0 9 10 0 0.9 5 code-llama-instruct:34:ggufv2:Q6_K 9 10 0 0.9 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:Q6_K 5 6 0.57735 0.833333 3 code-llama-instruct:7:ggufv2:Q3_K_M 7 10 0 0.7 5 gpt-4o-2024-05-13 7 10 0 0.7 5 code-llama-instruct:7:ggufv2:Q2_K 7 10 0 0.7 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 7 10 0 0.7 5 gpt-4o-2024-08-06 4 6 0.57735 0.666667 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 6 10 0 0.6 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 6 10 0 0.6 5 llama-2-chat:7:ggufv2:Q5_K_M 6 10 0 0.6 5 code-llama-instruct:13:ggufv2:Q6_K 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q4_K_M 5 10 0 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q6_K 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q4_K_M 5 10 0 0.5 5 llama-2-chat:70:ggufv2:Q3_K_M 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q8_0 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q5_K_M 5 10 0 0.5 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 10 0 0.5 5 gpt-4-0125-preview 5 10 0 0.5 5 code-llama-instruct:7:ggufv2:Q8_0 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q2_K 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q3_K_M 5 10 0 0.5 5 gpt-4o-mini-2024-07-18 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q4_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q5_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q6_K 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q8_0 5 10 0 0.5 5 llama-2-chat:70:ggufv2:Q2_K 5 10 0 0.5 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 5 10 0 0.5 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 5 10 0 0.5 5 openhermes-2.5:7:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q8_0 5 10 0 0.5 5 openhermes-2.5:7:ggufv2:Q2_K 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q2_K 4 10 0 0.4 5 code-llama-instruct:34:ggufv2:Q4_K_M 4 10 0 0.4 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 10 0 0 5"},{"location":"BioChatter/benchmark/results/#text-extraction","title":"Text Extraction","text":"<p>In this set of tasks, we test LLM abilities to extract text from a given document.</p> Overall PerformanceAssay extractionChemical extractionContext extractionDisease extractionEntity extractionExperiment extraction (yes or no)Hypothesis extractionIntervention extractionGene identifier mappingSignificance extractionStatistics extractionImage caption yes/no Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 224.558 297 0.0766981 0.756088 3 gpt-4o-2024-08-06 211.222 297 1.31903 0.711185 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 207.674 297 1.29175e-15 0.699238 3 claude-3-opus-20240229 205.297 297 0.441227 0.691235 3 gpt-4-0125-preview 341.404 495 0 0.689705 5 gpt-4o-mini-2024-07-18 338.854 495 1.74807 0.684553 5 gpt-4-0613 331.107 495 0 0.668903 5 gpt-4o-2024-05-13 323.703 495 0 0.653946 5 gpt-4-turbo-2024-04-09 321.933 495 4.39682 0.650369 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 190.774 297 9.26323e-16 0.642336 3 llama-3.1-instruct:70:ggufv2:IQ2_M 186.07 297 8.83831e-16 0.626498 3 openhermes-2.5:7:ggufv2:Q6_K 306.488 495 0 0.619167 5 openhermes-2.5:7:ggufv2:Q8_0 297.41 495 0 0.600829 5 openhermes-2.5:7:ggufv2:Q4_K_M 295.654 495 0 0.597281 5 openhermes-2.5:7:ggufv2:Q5_K_M 287.059 495 0 0.579916 5 gpt-3.5-turbo-0613 284.814 495 0 0.575381 5 openhermes-2.5:7:ggufv2:Q3_K_M 274.471 495 0 0.554488 5 gpt-3.5-turbo-0125 252.466 495 0 0.510032 5 openhermes-2.5:7:ggufv2:Q2_K 219.807 495 0 0.444054 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 123.142 297 5.69391e-16 0.414621 3 llama-3.1-instruct:8:ggufv2:Q6_K 117.157 297 3.73928e-16 0.394469 3 llama-3.1-instruct:8:ggufv2:Q8_0 115.554 297 4.18545e-16 0.38907 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 190.948 495 0 0.385754 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 113.462 297 0.300229 0.382027 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 113.002 297 0.00031952 0.380477 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 182.642 495 0 0.368974 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 181.869 495 0 0.367412 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 107.033 297 5.50801e-16 0.360379 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 174.084 495 0 0.351684 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 171.777 495 0 0.347025 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 163.974 495 0 0.331261 5 llama-2-chat:70:ggufv2:Q4_K_M 119.263 495 0 0.240936 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 116.651 495 0 0.235659 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 113.663 495 0 0.229622 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 111.634 495 0 0.225524 5 llama-2-chat:70:ggufv2:Q2_K 106.448 495 0 0.215047 5 llama-2-chat:70:ggufv2:Q5_K_M 104.032 495 0 0.210166 5 llama-2-chat:70:ggufv2:Q3_K_M 97.9593 495 0 0.197898 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 95.9243 495 0 0.193786 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 93.6428 495 0 0.189177 5 llama-3-instruct:8:ggufv2:Q8_0 93.3345 495 0 0.188555 5 chatglm3:6:ggmlv3:q4_0 93.2008 495 0 0.188284 5 llama-3-instruct:8:ggufv2:Q5_K_M 82.3847 495 0 0.166434 5 llama-3-instruct:8:ggufv2:Q6_K 80.5152 495 0 0.162657 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 77.9693 495 0 0.157514 5 code-llama-instruct:7:ggufv2:Q4_K_M 68.6724 495 0 0.138732 5 llama-3-instruct:8:ggufv2:Q4_K_M 57.8514 495 0 0.116871 5 llama-2-chat:13:ggufv2:Q3_K_M 55.7521 495 0 0.112631 5 llama-2-chat:13:ggufv2:Q4_K_M 43.9894 495 0 0.0888675 5 llama-2-chat:7:ggufv2:Q4_K_M 42.1985 495 0 0.0852494 5 llama-2-chat:7:ggufv2:Q8_0 25.1647 297 1.46597e-16 0.0847297 3 llama-2-chat:13:ggufv2:Q6_K 23.2057 297 0.00246731 0.0781337 3 llama-2-chat:13:ggufv2:Q5_K_M 37.9252 495 0 0.0766167 5 llama-2-chat:13:ggufv2:Q8_0 37.7416 495 0 0.0762457 5 llama-2-chat:7:ggufv2:Q5_K_M 34.5308 495 0 0.0697591 5 llama-2-chat:7:ggufv2:Q3_K_M 32.2105 495 0 0.0650717 5 llama-2-chat:13:ggufv2:Q2_K 32.1447 495 0 0.0649389 5 llama-2-chat:7:ggufv2:Q6_K 18.2539 297 2.57076e-16 0.0614608 3 llama-2-chat:7:ggufv2:Q2_K 17.9123 495 0 0.0361865 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 assay 23.4242 27 0 0.867565 3 llama-3.1-instruct:70:ggufv2:IQ2_M assay 22.1667 27 0 0.820988 3 llama-3.1-instruct:70:ggufv2:Q3_K_S assay 22.1667 27 0 0.820988 3 claude-3-opus-20240229 assay 21.4909 27 1.51082e-17 0.79596 3 gpt-4o-2024-08-06 assay 20.3897 27 3.02164e-17 0.755176 3 llama-3.1-instruct:70:ggufv2:IQ4_XS assay 20.3238 27 3.02164e-17 0.752734 3 gpt-4o-mini-2024-07-18 assay 33.0217 45 0.00559644 0.733816 5 gpt-4-turbo-2024-04-09 assay 26.4233 45 0.0114637 0.587184 5 llama-3.1-instruct:8:ggufv2:Q3_K_L assay 12.9241 27 1.60525e-17 0.478671 3 llama-3.1-instruct:8:ggufv2:IQ4_XS assay 12.4373 27 8.49837e-18 0.460639 3 llama-3.1-instruct:8:ggufv2:Q4_K_M assay 12.4264 27 7.55411e-18 0.460235 3 llama-3.1-instruct:8:ggufv2:Q6_K assay 12.1618 27 8.49837e-18 0.450438 3 llama-3.1-instruct:8:ggufv2:Q5_K_M assay 11.2618 27 0 0.417103 3 llama-3.1-instruct:8:ggufv2:Q8_0 assay 10.4603 27 1.51082e-17 0.387417 3 gpt-4o-2024-05-13 assay 6.67307 45 0 0.148291 5 gpt-4-0125-preview assay 6.60264 45 0 0.146725 5 openhermes-2.5:7:ggufv2:Q6_K assay 6.45354 45 0 0.143412 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M assay 6.42156 45 0 0.142701 5 openhermes-2.5:7:ggufv2:Q8_0 assay 6.24141 45 0 0.138698 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 assay 5.8662 45 0 0.13036 5 mistral-instruct-v0.2:7:ggufv2:Q2_K assay 5.84165 45 0 0.129814 5 mistral-instruct-v0.2:7:ggufv2:Q6_K assay 5.83272 45 0 0.129616 5 openhermes-2.5:7:ggufv2:Q5_K_M assay 5.77475 45 0 0.128328 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M assay 5.72421 45 0 0.127205 5 gpt-3.5-turbo-0613 assay 5.71717 45 0 0.127048 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M assay 5.66084 45 0 0.125797 5 gpt-3.5-turbo-0125 assay 5.48324 45 0 0.12185 5 gpt-4-0613 assay 5.47238 45 0 0.121608 5 openhermes-2.5:7:ggufv2:Q4_K_M assay 5.40473 45 0 0.120105 5 openhermes-2.5:7:ggufv2:Q3_K_M assay 4.99329 45 0 0.110962 5 openhermes-2.5:7:ggufv2:Q2_K assay 4.35689 45 0 0.0968198 5 llama-2-chat:7:ggufv2:Q6_K assay 2.34166 27 7.55411e-18 0.0867281 3 llama-2-chat:13:ggufv2:Q6_K assay 2.19772 27 3.77706e-18 0.081397 3 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M assay 3.17543 45 0 0.070565 5 llama-2-chat:7:ggufv2:Q8_0 assay 1.62311 27 0 0.0601152 3 llama-2-chat:70:ggufv2:Q4_K_M assay 1.8509 45 0 0.041131 5 llama-2-chat:70:ggufv2:Q5_K_M assay 1.81844 45 0 0.0404097 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K assay 1.68419 45 0 0.0374265 5 chatglm3:6:ggmlv3:q4_0 assay 1.61672 45 0 0.0359271 5 code-llama-instruct:7:ggufv2:Q4_K_M assay 1.53778 45 0 0.0341728 5 llama-3-instruct:8:ggufv2:Q6_K assay 1.48103 45 0 0.0329118 5 llama-3-instruct:8:ggufv2:Q8_0 assay 1.37088 45 0 0.0304641 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 assay 1.16327 45 0 0.0258505 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M assay 1.15926 45 0 0.0257612 5 llama-2-chat:70:ggufv2:Q2_K assay 1.15095 45 0 0.0255768 5 llama-2-chat:70:ggufv2:Q3_K_M assay 1.07788 45 0 0.023953 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M assay 1.05347 45 0 0.0234104 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K assay 1.02909 45 0 0.0228686 5 llama-2-chat:13:ggufv2:Q2_K assay 0.974441 45 0 0.0216542 5 llama-3-instruct:8:ggufv2:Q5_K_M assay 0.922706 45 0 0.0205046 5 llama-2-chat:7:ggufv2:Q5_K_M assay 0.919259 45 0 0.020428 5 llama-2-chat:13:ggufv2:Q5_K_M assay 0.836349 45 0 0.0185855 5 llama-2-chat:13:ggufv2:Q8_0 assay 0.756302 45 0 0.0168067 5 llama-2-chat:13:ggufv2:Q3_K_M assay 0.750557 45 0 0.016679 5 llama-2-chat:13:ggufv2:Q4_K_M assay 0.647223 45 0 0.0143827 5 llama-2-chat:7:ggufv2:Q4_K_M assay 0.604799 45 0 0.01344 5 llama-3-instruct:8:ggufv2:Q4_K_M assay 0.522273 45 0 0.0116061 5 llama-2-chat:7:ggufv2:Q3_K_M assay 0.455699 45 0 0.0101266 5 llama-2-chat:7:ggufv2:Q2_K assay 0.233824 45 0 0.00519608 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 chemical 24 27 0 0.888889 3 claude-3-5-sonnet-20240620 chemical 21.6667 27 0 0.802469 3 llama-3.1-instruct:70:ggufv2:IQ4_XS chemical 20 27 0 0.740741 3 gpt-4o-2024-08-06 chemical 18.6667 27 0 0.691358 3 llama-3.1-instruct:70:ggufv2:Q3_K_S chemical 18 27 0 0.666667 3 llama-3.1-instruct:70:ggufv2:IQ2_M chemical 18 27 0 0.666667 3 gpt-4-turbo-2024-04-09 chemical 29.188 45 0 0.648623 5 gpt-4o-mini-2024-07-18 chemical 27.7778 45 0 0.617284 5 llama-3.1-instruct:8:ggufv2:IQ4_XS chemical 12.3451 27 0 0.457227 3 llama-3.1-instruct:8:ggufv2:Q4_K_M chemical 12.0531 27 0 0.446411 3 llama-3.1-instruct:8:ggufv2:Q6_K chemical 11.0168 27 0 0.408029 3 llama-3.1-instruct:8:ggufv2:Q3_K_L chemical 10.8547 27 2.36066e-19 0.402026 3 llama-3.1-instruct:8:ggufv2:Q8_0 chemical 9.13698 27 0 0.338407 3 llama-3.1-instruct:8:ggufv2:Q5_K_M chemical 8.35802 27 0 0.309556 3 gpt-4-0613 chemical 6.38889 45 0 0.141975 5 gpt-4-0125-preview chemical 6.22222 45 0 0.138272 5 openhermes-2.5:7:ggufv2:Q6_K chemical 6.16667 45 0 0.137037 5 gpt-4o-2024-05-13 chemical 5.55556 45 0 0.123457 5 gpt-3.5-turbo-0613 chemical 5.44444 45 0 0.120988 5 openhermes-2.5:7:ggufv2:Q3_K_M chemical 5.23309 45 0 0.116291 5 openhermes-2.5:7:ggufv2:Q8_0 chemical 5.16667 45 0 0.114815 5 openhermes-2.5:7:ggufv2:Q5_K_M chemical 5.06667 45 0 0.112593 5 gpt-3.5-turbo-0125 chemical 5.06444 45 0 0.112543 5 openhermes-2.5:7:ggufv2:Q4_K_M chemical 4.95556 45 0 0.110123 5 openhermes-2.5:7:ggufv2:Q2_K chemical 4.66667 45 0 0.103704 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M chemical 4.02332 45 0 0.0894072 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M chemical 3.69824 45 0 0.0821832 5 mistral-instruct-v0.2:7:ggufv2:Q6_K chemical 3.5588 45 0 0.0790845 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M chemical 3.23175 45 0 0.0718166 5 mistral-instruct-v0.2:7:ggufv2:Q2_K chemical 2.9648 45 0 0.0658845 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M chemical 2.85926 45 0 0.0635392 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 chemical 2.80214 45 0 0.0622698 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K chemical 2.28839 45 0 0.050853 5 llama-2-chat:13:ggufv2:Q6_K chemical 1.33748 27 0 0.0495362 3 llama-3-instruct:8:ggufv2:Q6_K chemical 1.99259 45 0 0.0442798 5 llama-3-instruct:8:ggufv2:Q5_K_M chemical 1.98451 45 0 0.0441003 5 llama-3-instruct:8:ggufv2:Q8_0 chemical 1.98451 45 0 0.0441003 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M chemical 1.92687 45 0 0.0428194 5 llama-2-chat:70:ggufv2:Q2_K chemical 1.92403 45 0 0.0427562 5 llama-2-chat:70:ggufv2:Q4_K_M chemical 1.86594 45 0 0.0414653 5 llama-2-chat:7:ggufv2:Q8_0 chemical 1.11429 27 3.77706e-18 0.0412698 3 llama-2-chat:70:ggufv2:Q5_K_M chemical 1.7972 45 0 0.0399378 5 llama-2-chat:70:ggufv2:Q3_K_M chemical 1.65417 45 0 0.0367593 5 llama-2-chat:13:ggufv2:Q4_K_M chemical 1.60885 45 0 0.0357522 5 llama-2-chat:7:ggufv2:Q6_K chemical 0.85 27 1.88853e-18 0.0314815 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K chemical 1.37178 45 0 0.030484 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 chemical 1.02473 45 0 0.0227718 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M chemical 0.993896 45 0 0.0220866 5 llama-3-instruct:8:ggufv2:Q4_K_M chemical 0.920791 45 0 0.020462 5 chatglm3:6:ggmlv3:q4_0 chemical 0.839293 45 0 0.018651 5 llama-2-chat:7:ggufv2:Q5_K_M chemical 0.580952 45 0 0.0129101 5 llama-2-chat:13:ggufv2:Q5_K_M chemical 0.473978 45 0 0.0105328 5 llama-2-chat:13:ggufv2:Q8_0 chemical 0.473978 45 0 0.0105328 5 llama-2-chat:13:ggufv2:Q3_K_M chemical 0.447004 45 0 0.00993343 5 code-llama-instruct:7:ggufv2:Q4_K_M chemical 0.44189 45 0 0.00981978 5 llama-2-chat:13:ggufv2:Q2_K chemical 0.429118 45 0 0.00953595 5 llama-2-chat:7:ggufv2:Q4_K_M chemical 0.416702 45 0 0.00926004 5 llama-2-chat:7:ggufv2:Q3_K_M chemical 0.270151 45 0 0.00600336 5 llama-2-chat:7:ggufv2:Q2_K chemical 0.264943 45 0 0.00588762 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations llama-3.1-instruct:70:ggufv2:IQ2_M context 25.2426 27 1.51082e-17 0.93491 3 llama-3.1-instruct:70:ggufv2:IQ4_XS context 25.2195 27 3.02164e-17 0.934057 3 claude-3-5-sonnet-20240620 context 24.5401 27 6.04329e-17 0.908892 3 gpt-4o-2024-08-06 context 23.6991 27 0.00396564 0.877746 3 gpt-4-turbo-2024-04-09 context 38.9656 45 0.0255706 0.865903 5 claude-3-opus-20240229 context 23.2287 27 3.02164e-17 0.860323 3 gpt-4o-mini-2024-07-18 context 38.476 45 0.00134967 0.855023 5 llama-3.1-instruct:70:ggufv2:Q3_K_S context 22.2637 27 1.51082e-17 0.82458 3 llama-3.1-instruct:8:ggufv2:Q8_0 context 16.8902 27 9.44264e-19 0.625563 3 llama-3.1-instruct:8:ggufv2:IQ4_XS context 16.8016 27 1.51082e-17 0.622281 3 llama-3.1-instruct:8:ggufv2:Q4_K_M context 16.4994 27 2.83279e-18 0.611089 3 llama-3.1-instruct:8:ggufv2:Q6_K context 16.4093 27 1.88853e-18 0.607753 3 llama-3.1-instruct:8:ggufv2:Q5_K_M context 16.1817 27 1.88853e-18 0.599324 3 llama-3.1-instruct:8:ggufv2:Q3_K_L context 15.6028 27 1.88853e-18 0.57788 3 llama-2-chat:7:ggufv2:Q8_0 context 5.70797 27 0 0.211406 3 llama-2-chat:13:ggufv2:Q6_K context 4.88293 27 1.69967e-17 0.180849 3 gpt-4-0613 context 7.90663 45 0 0.175703 5 gpt-4-0125-preview context 7.85253 45 0 0.174501 5 gpt-4o-2024-05-13 context 7.82965 45 0 0.173992 5 gpt-3.5-turbo-0125 context 6.89247 45 0 0.153166 5 openhermes-2.5:7:ggufv2:Q4_K_M context 6.89055 45 0 0.153123 5 openhermes-2.5:7:ggufv2:Q6_K context 6.79989 45 0 0.151109 5 openhermes-2.5:7:ggufv2:Q3_K_M context 6.77271 45 0 0.150505 5 openhermes-2.5:7:ggufv2:Q8_0 context 6.67749 45 0 0.148389 5 gpt-3.5-turbo-0613 context 6.50472 45 0 0.144549 5 openhermes-2.5:7:ggufv2:Q5_K_M context 6.44769 45 0 0.143282 5 llama-2-chat:7:ggufv2:Q6_K context 3.73057 27 0 0.138169 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 context 5.16754 45 0 0.114834 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M context 5.12599 45 0 0.113911 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M context 5.02844 45 0 0.111743 5 mistral-instruct-v0.2:7:ggufv2:Q6_K context 5.0158 45 0 0.111462 5 mistral-instruct-v0.2:7:ggufv2:Q2_K context 4.99362 45 0 0.110969 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K context 4.51314 45 0 0.100292 5 llama-2-chat:70:ggufv2:Q3_K_M context 4.22332 45 0 0.0938516 5 llama-2-chat:70:ggufv2:Q4_K_M context 4.10284 45 0 0.0911743 5 llama-2-chat:70:ggufv2:Q2_K context 4.08979 45 0 0.0908843 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M context 4.06318 45 0 0.090293 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M context 4.01117 45 0 0.0891372 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M context 3.90982 45 0 0.0868849 5 openhermes-2.5:7:ggufv2:Q2_K context 3.86897 45 0 0.0859772 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M context 3.79416 45 0 0.0843146 5 llama-2-chat:70:ggufv2:Q5_K_M context 3.74591 45 0 0.0832424 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 context 3.70126 45 0 0.0822502 5 code-llama-instruct:7:ggufv2:Q4_K_M context 3.32657 45 0 0.0739237 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K context 3.1452 45 0 0.0698933 5 chatglm3:6:ggmlv3:q4_0 context 2.85636 45 0 0.0634747 5 llama-2-chat:7:ggufv2:Q3_K_M context 2.10857 45 0 0.046857 5 llama-2-chat:7:ggufv2:Q4_K_M context 1.89605 45 0 0.0421345 5 llama-2-chat:13:ggufv2:Q3_K_M context 1.78868 45 0 0.0397484 5 llama-2-chat:13:ggufv2:Q5_K_M context 1.78618 45 0 0.0396929 5 llama-2-chat:13:ggufv2:Q4_K_M context 1.77351 45 0 0.0394113 5 llama-3-instruct:8:ggufv2:Q8_0 context 1.67334 45 0 0.0371853 5 llama-3-instruct:8:ggufv2:Q5_K_M context 1.64821 45 0 0.0366268 5 llama-2-chat:13:ggufv2:Q8_0 context 1.58821 45 0 0.0352936 5 llama-3-instruct:8:ggufv2:Q4_K_M context 1.57169 45 0 0.0349264 5 llama-2-chat:13:ggufv2:Q2_K context 1.34289 45 0 0.0298419 5 llama-2-chat:7:ggufv2:Q5_K_M context 1.23881 45 0 0.0275291 5 llama-2-chat:7:ggufv2:Q2_K context 1.12335 45 0 0.0249632 5 llama-3-instruct:8:ggufv2:Q6_K context 1.10292 45 0 0.0245094 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 disease 20.4 27 1.51082e-17 0.755556 3 gpt-4o-mini-2024-07-18 disease 32.3333 45 0 0.718519 5 gpt-4o-2024-08-06 disease 19.4 27 1.51082e-17 0.718519 3 gpt-4-turbo-2024-04-09 disease 32.2667 45 0.00331269 0.717037 5 llama-3.1-instruct:70:ggufv2:IQ4_XS disease 17.2 27 7.55411e-18 0.637037 3 llama-3.1-instruct:70:ggufv2:IQ2_M disease 13.2 27 7.55411e-18 0.488889 3 llama-3.1-instruct:70:ggufv2:Q3_K_S disease 13.2 27 7.55411e-18 0.488889 3 llama-3.1-instruct:8:ggufv2:Q5_K_M disease 11.7851 27 7.55411e-18 0.436484 3 llama-3.1-instruct:8:ggufv2:Q3_K_L disease 11.5883 27 2.36066e-19 0.429196 3 llama-3.1-instruct:8:ggufv2:Q6_K disease 11.5414 27 1.51082e-17 0.42746 3 llama-3.1-instruct:8:ggufv2:IQ4_XS disease 11.5414 27 1.51082e-17 0.42746 3 llama-3.1-instruct:8:ggufv2:Q4_K_M disease 10.2531 27 2.36066e-19 0.379746 3 claude-3-opus-20240229 disease 10 27 0 0.37037 3 llama-3.1-instruct:8:ggufv2:Q8_0 disease 9.31558 27 2.36066e-19 0.345022 3 openhermes-2.5:7:ggufv2:Q3_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q4_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q5_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q6_K disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q8_0 disease 6.46667 45 0 0.143704 5 gpt-4-0125-preview disease 6.21333 45 0 0.138074 5 gpt-4o-2024-05-13 disease 6.2 45 0 0.137778 5 gpt-4-0613 disease 6.13333 45 0 0.136296 5 gpt-3.5-turbo-0613 disease 6.06667 45 0 0.134815 5 gpt-3.5-turbo-0125 disease 4.75238 45 0 0.105608 5 openhermes-2.5:7:ggufv2:Q2_K disease 4.32493 45 0 0.0961096 5 mistral-instruct-v0.2:7:ggufv2:Q2_K disease 4.20708 45 0 0.0934906 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K disease 4.14674 45 0 0.0921497 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M disease 4.02927 45 0 0.0895392 5 mistral-instruct-v0.2:7:ggufv2:Q6_K disease 4.01581 45 0 0.0892402 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 disease 3.47244 45 0 0.0771654 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M disease 3.04532 45 0 0.0676737 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M disease 2.92854 45 0 0.0650787 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M disease 2.65437 45 0 0.0589859 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 disease 2.57657 45 0 0.057257 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M disease 2.44785 45 0 0.0543966 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M disease 2.29171 45 0 0.0509269 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K disease 2.29094 45 0 0.0509099 5 llama-3-instruct:8:ggufv2:Q8_0 disease 1.73452 45 0 0.0385449 5 llama-3-instruct:8:ggufv2:Q6_K disease 1.73452 45 0 0.0385449 5 llama-3-instruct:8:ggufv2:Q5_K_M disease 1.73452 45 0 0.0385449 5 llama-2-chat:13:ggufv2:Q6_K disease 0.827524 27 0 0.030649 3 code-llama-instruct:7:ggufv2:Q4_K_M disease 1.33093 45 0 0.0295762 5 chatglm3:6:ggmlv3:q4_0 disease 1.21669 45 0 0.0270376 5 llama-3-instruct:8:ggufv2:Q4_K_M disease 0.995894 45 0 0.022131 5 llama-2-chat:7:ggufv2:Q8_0 disease 0.444887 27 2.36066e-19 0.0164773 3 llama-2-chat:7:ggufv2:Q6_K disease 0.439254 27 0 0.0162687 3 llama-2-chat:13:ggufv2:Q5_K_M disease 0.306386 45 0 0.00680858 5 llama-2-chat:13:ggufv2:Q8_0 disease 0.26663 45 0 0.00592511 5 llama-2-chat:13:ggufv2:Q4_K_M disease 0.250053 45 0 0.00555673 5 llama-2-chat:70:ggufv2:Q5_K_M disease 0.235648 45 0 0.00523663 5 llama-2-chat:7:ggufv2:Q3_K_M disease 0.185035 45 0 0.0041119 5 llama-2-chat:70:ggufv2:Q2_K disease 0.182046 45 0 0.00404548 5 llama-2-chat:70:ggufv2:Q4_K_M disease 0.179398 45 0 0.00398663 5 llama-2-chat:7:ggufv2:Q5_K_M disease 0.150208 45 0 0.00333795 5 llama-2-chat:70:ggufv2:Q3_K_M disease 0.142957 45 0 0.00317683 5 llama-2-chat:13:ggufv2:Q3_K_M disease 0.103277 45 0 0.00229505 5 llama-2-chat:7:ggufv2:Q4_K_M disease 0.0898052 45 0 0.00199567 5 llama-2-chat:13:ggufv2:Q2_K disease 0.0874203 45 0 0.00194267 5 llama-2-chat:7:ggufv2:Q2_K disease 0.0587138 45 0 0.00130475 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-08-06 entity 17.9286 27 0 0.664021 3 llama-3.1-instruct:70:ggufv2:Q3_K_S entity 17.8096 27 1.51082e-17 0.659615 3 llama-3.1-instruct:70:ggufv2:IQ4_XS entity 17.6096 27 7.55411e-18 0.652208 3 claude-3-opus-20240229 entity 16.325 27 4.53247e-17 0.60463 3 llama-3.1-instruct:70:ggufv2:IQ2_M entity 16.225 27 1.51082e-17 0.600926 3 claude-3-5-sonnet-20240620 entity 15.625 27 1.51082e-17 0.578704 3 gpt-4o-mini-2024-07-18 entity 24.5545 45 0.0661682 0.545656 5 gpt-4-turbo-2024-04-09 entity 19.2455 45 0.0562119 0.427677 5 llama-3.1-instruct:8:ggufv2:Q8_0 entity 8.02772 27 0 0.297323 3 llama-3.1-instruct:8:ggufv2:Q6_K entity 7.84022 27 0 0.290379 3 llama-3.1-instruct:8:ggufv2:IQ4_XS entity 6.80681 27 1.51082e-17 0.252104 3 llama-3.1-instruct:8:ggufv2:Q3_K_L entity 6.16922 27 1.69967e-17 0.22849 3 llama-3.1-instruct:8:ggufv2:Q4_K_M entity 5.96677 27 7.55411e-18 0.220991 3 llama-3.1-instruct:8:ggufv2:Q5_K_M entity 5.58394 27 0 0.206813 3 gpt-4o-2024-05-13 entity 5.9909 45 0 0.133131 5 gpt-4-0125-preview entity 4.59502 45 0 0.102112 5 gpt-3.5-turbo-0613 entity 4.57972 45 0 0.101772 5 openhermes-2.5:7:ggufv2:Q4_K_M entity 4.22461 45 0 0.0938803 5 openhermes-2.5:7:ggufv2:Q8_0 entity 4.1344 45 0 0.0918755 5 gpt-4-0613 entity 4.12852 45 0 0.0917448 5 openhermes-2.5:7:ggufv2:Q6_K entity 4.09333 45 0 0.0909629 5 openhermes-2.5:7:ggufv2:Q5_K_M entity 4.02016 45 0 0.0893369 5 gpt-3.5-turbo-0125 entity 3.71195 45 0 0.0824877 5 openhermes-2.5:7:ggufv2:Q3_K_M entity 3.65819 45 0 0.0812932 5 llama-2-chat:13:ggufv2:Q6_K entity 2.14189 27 0 0.0793293 3 llama-2-chat:7:ggufv2:Q6_K entity 2.07106 27 9.44264e-19 0.0767059 3 llama-2-chat:7:ggufv2:Q8_0 entity 1.79733 27 0 0.0665678 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M entity 2.42313 45 0 0.0538473 5 openhermes-2.5:7:ggufv2:Q2_K entity 2.33413 45 0 0.0518696 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M entity 2.30597 45 0 0.0512437 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M entity 2.20283 45 0 0.0489518 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K entity 2.10077 45 0 0.0466838 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M entity 2.0607 45 0 0.0457934 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 entity 2.00802 45 0 0.0446226 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M entity 1.99809 45 0 0.044402 5 mistral-instruct-v0.2:7:ggufv2:Q6_K entity 1.99214 45 0 0.0442699 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 entity 1.79999 45 0 0.0399998 5 mistral-instruct-v0.2:7:ggufv2:Q2_K entity 1.77563 45 0 0.0394584 5 chatglm3:6:ggmlv3:q4_0 entity 1.22227 45 0 0.0271617 5 llama-2-chat:70:ggufv2:Q3_K_M entity 1.20851 45 0 0.0268558 5 llama-2-chat:70:ggufv2:Q2_K entity 1.16189 45 0 0.0258197 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M entity 1.10007 45 0 0.0244461 5 llama-2-chat:70:ggufv2:Q4_K_M entity 1.01555 45 0 0.0225677 5 code-llama-instruct:7:ggufv2:Q4_K_M entity 0.948961 45 0 0.021088 5 llama-2-chat:70:ggufv2:Q5_K_M entity 0.903324 45 0 0.0200739 5 llama-2-chat:13:ggufv2:Q2_K entity 0.807379 45 0 0.0179418 5 llama-2-chat:13:ggufv2:Q4_K_M entity 0.785233 45 0 0.0174496 5 llama-3-instruct:8:ggufv2:Q5_K_M entity 0.75253 45 0 0.0167229 5 llama-3-instruct:8:ggufv2:Q6_K entity 0.749495 45 0 0.0166554 5 llama-2-chat:7:ggufv2:Q3_K_M entity 0.699988 45 0 0.0155553 5 llama-3-instruct:8:ggufv2:Q8_0 entity 0.695524 45 0 0.0154561 5 llama-3-instruct:8:ggufv2:Q4_K_M entity 0.694377 45 0 0.0154306 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K entity 0.685368 45 0 0.0152304 5 llama-2-chat:7:ggufv2:Q4_K_M entity 0.685027 45 0 0.0152228 5 llama-2-chat:13:ggufv2:Q8_0 entity 0.629764 45 0 0.0139947 5 llama-2-chat:7:ggufv2:Q5_K_M entity 0.623851 45 0 0.0138634 5 llama-2-chat:13:ggufv2:Q5_K_M entity 0.623813 45 0 0.0138625 5 llama-2-chat:13:ggufv2:Q3_K_M entity 0.56502 45 0 0.012556 5 llama-2-chat:7:ggufv2:Q2_K entity 0.318196 45 0 0.00707101 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 experiment_yes_or_no 27 27 0 1 3 gpt-4-turbo-2024-04-09 experiment_yes_or_no 45 45 0 1 5 llama-3.1-instruct:70:ggufv2:IQ4_XS experiment_yes_or_no 27 27 0 1 3 llama-3.1-instruct:70:ggufv2:Q3_K_S experiment_yes_or_no 27 27 0 1 3 claude-3-5-sonnet-20240620 experiment_yes_or_no 27 27 0 1 3 gpt-4o-2024-08-06 experiment_yes_or_no 25 27 0 0.925926 3 gpt-4o-mini-2024-07-18 experiment_yes_or_no 40 45 0 0.888889 5 llama-3.1-instruct:70:ggufv2:IQ2_M experiment_yes_or_no 22 27 0 0.814815 3 llama-3.1-instruct:8:ggufv2:Q6_K experiment_yes_or_no 18.0146 27 0 0.667206 3 llama-3.1-instruct:8:ggufv2:Q4_K_M experiment_yes_or_no 18.006 27 0 0.666888 3 llama-3.1-instruct:8:ggufv2:IQ4_XS experiment_yes_or_no 18.0059 27 0 0.666887 3 llama-3.1-instruct:8:ggufv2:Q8_0 experiment_yes_or_no 18.0059 27 0 0.666886 3 llama-3.1-instruct:8:ggufv2:Q5_K_M experiment_yes_or_no 18 27 0 0.666667 3 llama-3.1-instruct:8:ggufv2:Q3_K_L experiment_yes_or_no 18 27 0 0.666667 3 openhermes-2.5:7:ggufv2:Q2_K experiment_yes_or_no 9 45 0 0.2 5 gpt-4-0125-preview experiment_yes_or_no 9 45 0 0.2 5 llama-2-chat:70:ggufv2:Q4_K_M experiment_yes_or_no 9 45 0 0.2 5 chatglm3:6:ggmlv3:q4_0 experiment_yes_or_no 8.6 45 0 0.191111 5 openhermes-2.5:7:ggufv2:Q5_K_M experiment_yes_or_no 8.33333 45 0 0.185185 5 openhermes-2.5:7:ggufv2:Q6_K experiment_yes_or_no 8.33333 45 0 0.185185 5 openhermes-2.5:7:ggufv2:Q4_K_M experiment_yes_or_no 8.33333 45 0 0.185185 5 llama-2-chat:70:ggufv2:Q5_K_M experiment_yes_or_no 8.025 45 0 0.178333 5 openhermes-2.5:7:ggufv2:Q8_0 experiment_yes_or_no 8 45 0 0.177778 5 gpt-3.5-turbo-0613 experiment_yes_or_no 8 45 0 0.177778 5 gpt-4-0613 experiment_yes_or_no 8 45 0 0.177778 5 openhermes-2.5:7:ggufv2:Q3_K_M experiment_yes_or_no 8 45 0 0.177778 5 gpt-4o-2024-05-13 experiment_yes_or_no 8 45 0 0.177778 5 llama-2-chat:7:ggufv2:Q8_0 experiment_yes_or_no 4.67535 27 0 0.173161 3 llama-2-chat:70:ggufv2:Q2_K experiment_yes_or_no 7.05061 45 0 0.15668 5 llama-2-chat:70:ggufv2:Q3_K_M experiment_yes_or_no 6.07336 45 0 0.134964 5 gpt-3.5-turbo-0125 experiment_yes_or_no 6.03333 45 0 0.134074 5 llama-2-chat:13:ggufv2:Q6_K experiment_yes_or_no 3.25916 27 9.44264e-19 0.12071 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M experiment_yes_or_no 5.23564 45 0 0.116348 5 llama-2-chat:13:ggufv2:Q3_K_M experiment_yes_or_no 5.16593 45 0 0.114799 5 llama-3-instruct:8:ggufv2:Q8_0 experiment_yes_or_no 3.7 45 0 0.0822222 5 llama-3-instruct:8:ggufv2:Q5_K_M experiment_yes_or_no 3.68182 45 0 0.0818182 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 experiment_yes_or_no 3.32028 45 0 0.073784 5 llama-2-chat:7:ggufv2:Q6_K experiment_yes_or_no 1.97565 27 7.08198e-19 0.0731722 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M experiment_yes_or_no 3.26963 45 0 0.0726584 5 code-llama-instruct:7:ggufv2:Q4_K_M experiment_yes_or_no 3.0913 45 0 0.0686956 5 llama-3-instruct:8:ggufv2:Q6_K experiment_yes_or_no 2.36364 45 0 0.0525253 5 mistral-instruct-v0.2:7:ggufv2:Q6_K experiment_yes_or_no 2.36015 45 0 0.0524479 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M experiment_yes_or_no 2.2851 45 0 0.05078 5 mistral-instruct-v0.2:7:ggufv2:Q2_K experiment_yes_or_no 2.2802 45 0 0.0506711 5 llama-2-chat:7:ggufv2:Q4_K_M experiment_yes_or_no 2.06817 45 0 0.0459593 5 llama-3-instruct:8:ggufv2:Q4_K_M experiment_yes_or_no 1.89935 45 0 0.0422078 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M experiment_yes_or_no 1.45686 45 0 0.0323746 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M experiment_yes_or_no 1.29991 45 0 0.0288868 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M experiment_yes_or_no 1.1661 45 0 0.0259134 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K experiment_yes_or_no 1.15184 45 0 0.0255965 5 llama-2-chat:13:ggufv2:Q8_0 experiment_yes_or_no 1.06643 45 0 0.0236984 5 llama-2-chat:13:ggufv2:Q5_K_M experiment_yes_or_no 1.03147 45 0 0.0229215 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K experiment_yes_or_no 0.785587 45 0 0.0174575 5 llama-2-chat:7:ggufv2:Q3_K_M experiment_yes_or_no 0.726745 45 0 0.0161499 5 llama-2-chat:7:ggufv2:Q5_K_M experiment_yes_or_no 0.618798 45 0 0.0137511 5 llama-2-chat:13:ggufv2:Q4_K_M experiment_yes_or_no 0.468722 45 0 0.010416 5 llama-2-chat:13:ggufv2:Q2_K experiment_yes_or_no 0.267272 45 0 0.00593938 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 experiment_yes_or_no 0.201489 45 0 0.00447753 5 llama-2-chat:7:ggufv2:Q2_K experiment_yes_or_no 0.130285 45 0 0.00289522 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-mini-2024-07-18 hypothesis 17.2242 45 0.0552888 0.382759 5 gpt-4-turbo-2024-04-09 hypothesis 16.3215 45 0.154249 0.3627 5 gpt-4o-2024-08-06 hypothesis 8.73977 27 0.0621005 0.323695 3 llama-3.1-instruct:70:ggufv2:IQ4_XS hypothesis 6.99218 27 0 0.25897 3 claude-3-opus-20240229 hypothesis 6.74202 27 0.0355843 0.249704 3 llama-3.1-instruct:70:ggufv2:Q3_K_S hypothesis 5.51964 27 4.72132e-18 0.204431 3 claude-3-5-sonnet-20240620 hypothesis 4.76919 27 0.000375971 0.176637 3 llama-3.1-instruct:8:ggufv2:IQ4_XS hypothesis 4.4134 27 0 0.163459 3 llama-3.1-instruct:70:ggufv2:IQ2_M hypothesis 4.27846 27 0 0.158462 3 llama-3.1-instruct:8:ggufv2:Q3_K_L hypothesis 4.14868 27 0 0.153655 3 llama-3.1-instruct:8:ggufv2:Q5_K_M hypothesis 3.48866 27 3.55023e-05 0.129209 3 llama-2-chat:7:ggufv2:Q8_0 hypothesis 2.854 27 3.77706e-18 0.105704 3 llama-3.1-instruct:8:ggufv2:Q4_K_M hypothesis 2.74519 27 1.88853e-18 0.101674 3 llama-3.1-instruct:8:ggufv2:Q8_0 hypothesis 2.70116 27 0 0.100043 3 llama-3.1-instruct:8:ggufv2:Q6_K hypothesis 2.64133 27 9.44264e-19 0.097827 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M hypothesis 3.67339 45 0 0.0816309 5 llama-2-chat:7:ggufv2:Q6_K hypothesis 2.01944 27 5.19345e-18 0.074794 3 mistral-instruct-v0.2:7:ggufv2:Q6_K hypothesis 3.33681 45 0 0.0741512 5 gpt-4-0613 hypothesis 3.29696 45 0 0.0732657 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 hypothesis 2.9272 45 0 0.0650489 5 gpt-4o-2024-05-13 hypothesis 2.89512 45 0 0.064336 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M hypothesis 2.75585 45 0 0.0612411 5 gpt-3.5-turbo-0125 hypothesis 2.72775 45 0 0.0606168 5 llama-2-chat:13:ggufv2:Q6_K hypothesis 1.61253 27 1.88853e-18 0.0597233 3 gpt-3.5-turbo-0613 hypothesis 2.64497 45 0 0.0587771 5 openhermes-2.5:7:ggufv2:Q4_K_M hypothesis 2.57382 45 0 0.0571961 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M hypothesis 2.47292 45 0 0.0549539 5 openhermes-2.5:7:ggufv2:Q8_0 hypothesis 2.37196 45 0 0.0527103 5 gpt-4-0125-preview hypothesis 2.33518 45 0 0.051893 5 openhermes-2.5:7:ggufv2:Q6_K hypothesis 2.29085 45 0 0.0509077 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M hypothesis 2.23255 45 0 0.0496122 5 openhermes-2.5:7:ggufv2:Q3_K_M hypothesis 2.09626 45 0 0.0465835 5 mistral-instruct-v0.2:7:ggufv2:Q2_K hypothesis 2.05375 45 0 0.045639 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M hypothesis 1.87442 45 0 0.0416537 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 hypothesis 1.83735 45 0 0.04083 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M hypothesis 1.71557 45 0 0.0381237 5 openhermes-2.5:7:ggufv2:Q5_K_M hypothesis 1.52181 45 0 0.033818 5 openhermes-2.5:7:ggufv2:Q2_K hypothesis 1.4915 45 0 0.0331444 5 llama-2-chat:70:ggufv2:Q3_K_M hypothesis 1.44143 45 0 0.0320317 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K hypothesis 1.44009 45 0 0.032002 5 llama-2-chat:70:ggufv2:Q2_K hypothesis 1.4389 45 0 0.0319755 5 llama-2-chat:70:ggufv2:Q4_K_M hypothesis 1.41421 45 0 0.0314268 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K hypothesis 1.39565 45 0 0.0310144 5 llama-3-instruct:8:ggufv2:Q4_K_M hypothesis 1.13596 45 0 0.0252436 5 chatglm3:6:ggmlv3:q4_0 hypothesis 0.98676 45 0 0.021928 5 llama-3-instruct:8:ggufv2:Q8_0 hypothesis 0.878406 45 0 0.0195201 5 llama-3-instruct:8:ggufv2:Q6_K hypothesis 0.876219 45 0 0.0194715 5 llama-2-chat:7:ggufv2:Q5_K_M hypothesis 0.68638 45 0 0.0152529 5 llama-2-chat:70:ggufv2:Q5_K_M hypothesis 0.623758 45 0 0.0138613 5 llama-2-chat:7:ggufv2:Q4_K_M hypothesis 0.62053 45 0 0.0137896 5 llama-3-instruct:8:ggufv2:Q5_K_M hypothesis 0.604423 45 0 0.0134316 5 code-llama-instruct:7:ggufv2:Q4_K_M hypothesis 0.572369 45 0 0.0127193 5 llama-2-chat:13:ggufv2:Q8_0 hypothesis 0.55524 45 0 0.0123387 5 llama-2-chat:7:ggufv2:Q2_K hypothesis 0.520453 45 0 0.0115656 5 llama-2-chat:13:ggufv2:Q2_K hypothesis 0.49279 45 0 0.0109509 5 llama-2-chat:13:ggufv2:Q3_K_M hypothesis 0.424638 45 0 0.00943639 5 llama-2-chat:13:ggufv2:Q5_K_M hypothesis 0.408017 45 0 0.00906704 5 llama-2-chat:7:ggufv2:Q3_K_M hypothesis 0.402337 45 0 0.00894082 5 llama-2-chat:13:ggufv2:Q4_K_M hypothesis 0.366299 45 0 0.00813997 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations llama-3.1-instruct:70:ggufv2:Q3_K_S intervention 21.7143 27 0 0.804233 3 llama-3.1-instruct:70:ggufv2:IQ4_XS intervention 20.4286 27 0 0.756614 3 llama-3.1-instruct:70:ggufv2:IQ2_M intervention 20.4286 27 0 0.756614 3 gpt-4o-mini-2024-07-18 intervention 30.0404 45 0.0608581 0.667565 5 gpt-4o-2024-08-06 intervention 17.3762 27 0.0687322 0.643563 3 llama-3.1-instruct:8:ggufv2:Q8_0 intervention 14.9465 27 0 0.553575 3 claude-3-opus-20240229 intervention 14.8366 27 0 0.549502 3 claude-3-5-sonnet-20240620 intervention 14.533 27 0.00814604 0.538261 3 gpt-4-turbo-2024-04-09 intervention 23.5746 45 0.0579382 0.52388 5 llama-3.1-instruct:8:ggufv2:Q5_K_M intervention 13.2322 27 0 0.490083 3 llama-3.1-instruct:8:ggufv2:IQ4_XS intervention 12.6108 27 0 0.467065 3 llama-3.1-instruct:8:ggufv2:Q6_K intervention 11.4946 27 0 0.425726 3 llama-3.1-instruct:8:ggufv2:Q4_K_M intervention 11.4467 27 0 0.423953 3 llama-3.1-instruct:8:ggufv2:Q3_K_L intervention 7.23948 27 1.88853e-18 0.268129 3 gpt-4o-2024-05-13 intervention 5.34631 45 0 0.118807 5 openhermes-2.5:7:ggufv2:Q4_K_M intervention 4.9841 45 0 0.110758 5 gpt-4-0125-preview intervention 4.92171 45 0 0.109371 5 gpt-4-0613 intervention 4.72253 45 0 0.104945 5 openhermes-2.5:7:ggufv2:Q6_K intervention 4.71449 45 0 0.104767 5 openhermes-2.5:7:ggufv2:Q8_0 intervention 4.44465 45 0 0.09877 5 gpt-3.5-turbo-0613 intervention 4.27143 45 0 0.0949206 5 openhermes-2.5:7:ggufv2:Q5_K_M intervention 4.00021 45 0 0.0888935 5 gpt-3.5-turbo-0125 intervention 3.75141 45 0 0.0833647 5 openhermes-2.5:7:ggufv2:Q3_K_M intervention 3.55238 45 0 0.0789418 5 openhermes-2.5:7:ggufv2:Q2_K intervention 2.92766 45 0 0.0650591 5 llama-2-chat:7:ggufv2:Q8_0 intervention 1.56417 27 0 0.0579323 3 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K intervention 2.23683 45 0 0.0497073 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M intervention 2.23319 45 0 0.0496264 5 llama-2-chat:13:ggufv2:Q6_K intervention 1.13241 27 0.000274145 0.0419412 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M intervention 1.66677 45 0 0.0370393 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M intervention 1.23412 45 0 0.0274249 5 code-llama-instruct:7:ggufv2:Q4_K_M intervention 1.17173 45 0 0.0260384 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M intervention 1.15754 45 0 0.025723 5 llama-2-chat:13:ggufv2:Q4_K_M intervention 1.02157 45 0 0.0227015 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 intervention 0.987919 45 0 0.0219538 5 chatglm3:6:ggmlv3:q4_0 intervention 0.881806 45 0 0.0195957 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 intervention 0.879646 45 0 0.0195477 5 llama-2-chat:7:ggufv2:Q6_K intervention 0.514286 27 3.77706e-18 0.0190476 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M intervention 0.723791 45 0 0.0160842 5 mistral-instruct-v0.2:7:ggufv2:Q2_K intervention 0.680182 45 0 0.0151152 5 llama-2-chat:70:ggufv2:Q2_K intervention 0.668995 45 0 0.0148666 5 mistral-instruct-v0.2:7:ggufv2:Q6_K intervention 0.640258 45 0 0.0142279 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M intervention 0.550643 45 0 0.0122365 5 llama-2-chat:70:ggufv2:Q5_K_M intervention 0.542302 45 0 0.0120512 5 llama-2-chat:13:ggufv2:Q2_K intervention 0.502722 45 0 0.0111716 5 llama-2-chat:70:ggufv2:Q4_K_M intervention 0.417501 45 0 0.00927779 5 llama-2-chat:7:ggufv2:Q3_K_M intervention 0.416756 45 0 0.00926124 5 llama-3-instruct:8:ggufv2:Q5_K_M intervention 0.410888 45 0 0.00913085 5 llama-2-chat:70:ggufv2:Q3_K_M intervention 0.402319 45 0 0.00894042 5 llama-3-instruct:8:ggufv2:Q4_K_M intervention 0.37923 45 0 0.00842733 5 llama-2-chat:13:ggufv2:Q5_K_M intervention 0.339683 45 0 0.0075485 5 llama-3-instruct:8:ggufv2:Q6_K intervention 0.327257 45 0 0.00727237 5 llama-3-instruct:8:ggufv2:Q8_0 intervention 0.319187 45 0 0.00709304 5 llama-2-chat:13:ggufv2:Q3_K_M intervention 0.265476 45 0 0.00589947 5 llama-2-chat:7:ggufv2:Q5_K_M intervention 0.24986 45 0 0.00555244 5 llama-2-chat:13:ggufv2:Q8_0 intervention 0.244444 45 0 0.0054321 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K intervention 0.2273 45 0 0.0050511 5 llama-2-chat:7:ggufv2:Q2_K intervention 0.118691 45 0 0.00263758 5 llama-2-chat:7:ggufv2:Q4_K_M intervention 0.0769231 45 0 0.0017094 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 ncbi_link 24 27 0 0.888889 3 claude-3-opus-20240229 ncbi_link 20.88 27 1.51082e-17 0.773333 3 gpt-4o-2024-08-06 ncbi_link 15.6389 27 0.00890973 0.579218 3 llama-3.1-instruct:70:ggufv2:IQ4_XS ncbi_link 15.3 27 0 0.566667 3 gpt-4-turbo-2024-04-09 ncbi_link 23.3333 45 0.132508 0.518519 5 llama-3.1-instruct:70:ggufv2:IQ2_M ncbi_link 13.5 27 0 0.5 3 gpt-4o-mini-2024-07-18 ncbi_link 16.8619 45 0.00496904 0.374709 5 llama-3.1-instruct:70:ggufv2:Q3_K_S ncbi_link 9.5 27 0 0.351852 3 gpt-4-0125-preview ncbi_link 6.48768 45 0 0.144171 5 llama-3.1-instruct:8:ggufv2:IQ4_XS ncbi_link 3.86536 27 0 0.143161 3 gpt-4-0613 ncbi_link 6.05933 45 0 0.134652 5 llama-3.1-instruct:8:ggufv2:Q8_0 ncbi_link 2.32755 27 7.55411e-18 0.0862055 3 openhermes-2.5:7:ggufv2:Q6_K ncbi_link 3.5303 45 0 0.0784512 5 openhermes-2.5:7:ggufv2:Q8_0 ncbi_link 3.5303 45 0 0.0784512 5 gpt-4o-2024-05-13 ncbi_link 3.51302 45 0 0.078067 5 openhermes-2.5:7:ggufv2:Q5_K_M ncbi_link 3.47436 45 0 0.077208 5 llama-3.1-instruct:8:ggufv2:Q4_K_M ncbi_link 1.90181 27 7.91155e-07 0.0704374 3 openhermes-2.5:7:ggufv2:Q4_K_M ncbi_link 3.11111 45 0 0.0691358 5 llama-3.1-instruct:8:ggufv2:Q5_K_M ncbi_link 1.59127 27 0 0.0589359 3 openhermes-2.5:7:ggufv2:Q3_K_M ncbi_link 2.37436 45 0 0.0527635 5 llama-3.1-instruct:8:ggufv2:Q6_K ncbi_link 1.31 27 0 0.0485184 3 gpt-3.5-turbo-0613 ncbi_link 2.16667 45 0 0.0481481 5 gpt-3.5-turbo-0125 ncbi_link 1.42925 45 0 0.031761 5 llama-2-chat:13:ggufv2:Q6_K ncbi_link 0.690904 27 0 0.0255891 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M ncbi_link 1.03429 45 0 0.0229841 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M ncbi_link 0.884957 45 0 0.0196657 5 mistral-instruct-v0.2:7:ggufv2:Q2_K ncbi_link 0.881705 45 0 0.0195934 5 llama-2-chat:7:ggufv2:Q6_K ncbi_link 0.507313 27 9.44264e-19 0.0187894 3 llama-3.1-instruct:8:ggufv2:Q3_K_L ncbi_link 0.486291 27 2.95082e-19 0.0180108 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M ncbi_link 0.710989 45 0 0.0157998 5 llama-2-chat:7:ggufv2:Q8_0 ncbi_link 0.410766 27 9.44264e-19 0.0152135 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K ncbi_link 0.656812 45 0 0.0145958 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M ncbi_link 0.615714 45 0 0.0136825 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 ncbi_link 0.596131 45 0 0.0132474 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M ncbi_link 0.574422 45 0 0.0127649 5 mistral-instruct-v0.2:7:ggufv2:Q6_K ncbi_link 0.558824 45 0 0.0124183 5 openhermes-2.5:7:ggufv2:Q2_K ncbi_link 0.505458 45 0 0.0112324 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 ncbi_link 0.429927 45 0 0.00955394 5 code-llama-instruct:7:ggufv2:Q4_K_M ncbi_link 0.328564 45 0 0.00730142 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K ncbi_link 0.271548 45 0 0.0060344 5 llama-2-chat:13:ggufv2:Q8_0 ncbi_link 0.255217 45 0 0.00567148 5 llama-2-chat:70:ggufv2:Q2_K ncbi_link 0.253735 45 0 0.00563856 5 llama-2-chat:13:ggufv2:Q4_K_M ncbi_link 0.246231 45 0 0.00547179 5 llama-2-chat:70:ggufv2:Q4_K_M ncbi_link 0.241357 45 0 0.00536348 5 llama-2-chat:13:ggufv2:Q5_K_M ncbi_link 0.236802 45 0 0.00526226 5 llama-3-instruct:8:ggufv2:Q4_K_M ncbi_link 0.233815 45 0 0.00519589 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M ncbi_link 0.230909 45 0 0.00513131 5 llama-2-chat:7:ggufv2:Q4_K_M ncbi_link 0.216341 45 0 0.00480757 5 llama-2-chat:70:ggufv2:Q5_K_M ncbi_link 0.196981 45 0 0.00437735 5 llama-2-chat:13:ggufv2:Q2_K ncbi_link 0.192574 45 0 0.00427942 5 llama-3-instruct:8:ggufv2:Q8_0 ncbi_link 0.179211 45 0 0.00398247 5 llama-2-chat:7:ggufv2:Q3_K_M ncbi_link 0.177339 45 0 0.00394087 5 llama-3-instruct:8:ggufv2:Q6_K ncbi_link 0.173014 45 0 0.00384476 5 llama-2-chat:7:ggufv2:Q5_K_M ncbi_link 0.170952 45 0 0.00379894 5 llama-2-chat:70:ggufv2:Q3_K_M ncbi_link 0.166777 45 0 0.00370615 5 llama-3-instruct:8:ggufv2:Q5_K_M ncbi_link 0.166614 45 0 0.00370254 5 llama-2-chat:7:ggufv2:Q2_K ncbi_link 0.15271 45 0 0.00339354 5 llama-2-chat:13:ggufv2:Q3_K_M ncbi_link 0.150011 45 0 0.00333359 5 chatglm3:6:ggmlv3:q4_0 ncbi_link 0.122857 45 0 0.00273017 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 significance 21.6 27 6.04329e-17 0.8 3 gpt-4o-mini-2024-07-18 significance 36 45 0 0.8 5 gpt-4o-2024-08-06 significance 17.8444 27 0.00285111 0.660905 3 llama-3.1-instruct:70:ggufv2:Q3_K_S significance 15.6 27 6.04329e-17 0.577778 3 claude-3-opus-20240229 significance 13.7935 27 0.013441 0.51087 3 llama-3.1-instruct:70:ggufv2:IQ4_XS significance 13.6 27 6.7987e-17 0.503704 3 gpt-4-turbo-2024-04-09 significance 22.6141 45 0.0472812 0.502536 5 llama-3.1-instruct:70:ggufv2:IQ2_M significance 13.0286 27 6.04329e-17 0.48254 3 llama-3.1-instruct:8:ggufv2:Q6_K significance 6.06508 27 1.51082e-17 0.224633 3 llama-3.1-instruct:8:ggufv2:IQ4_XS significance 6.00361 27 7.55411e-18 0.222356 3 llama-3.1-instruct:8:ggufv2:Q5_K_M significance 5.21313 27 1.51082e-17 0.193079 3 llama-3.1-instruct:8:ggufv2:Q8_0 significance 5.0798 27 2.26623e-17 0.188141 3 llama-3.1-instruct:8:ggufv2:Q4_K_M significance 4.94646 27 3.02164e-17 0.183202 3 llama-3.1-instruct:8:ggufv2:Q3_K_L significance 4.31237 27 2.36066e-17 0.159717 3 gpt-4-0613 significance 5.6 45 0 0.124444 5 gpt-4-0125-preview significance 5.18384 45 0 0.115196 5 gpt-4o-2024-05-13 significance 4.22424 45 0 0.0938721 5 openhermes-2.5:7:ggufv2:Q4_K_M significance 3.92996 45 0 0.0873325 5 openhermes-2.5:7:ggufv2:Q8_0 significance 3.78182 45 0 0.0840404 5 openhermes-2.5:7:ggufv2:Q6_K significance 3.78182 45 0 0.0840404 5 openhermes-2.5:7:ggufv2:Q5_K_M significance 3.77787 45 0 0.0839526 5 openhermes-2.5:7:ggufv2:Q3_K_M significance 3.69091 45 0 0.0820202 5 gpt-3.5-turbo-0613 significance 3.58562 45 0 0.0796804 5 gpt-3.5-turbo-0125 significance 3.51717 45 0 0.0781594 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M significance 2.93833 45 0 0.0652963 5 mistral-instruct-v0.2:7:ggufv2:Q6_K significance 2.87928 45 0 0.063984 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M significance 2.79423 45 0 0.062094 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 significance 2.62296 45 0 0.0582881 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M significance 2.56724 45 0 0.0570498 5 openhermes-2.5:7:ggufv2:Q2_K significance 2.48514 45 0 0.0552254 5 mistral-instruct-v0.2:7:ggufv2:Q2_K significance 2.4813 45 0 0.05514 5 llama-2-chat:7:ggufv2:Q8_0 significance 1.10159 27 0 0.0407996 3 llama-2-chat:13:ggufv2:Q6_K significance 1.07015 27 1.0623e-18 0.0396352 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 significance 1.50696 45 0 0.033488 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K significance 1.34869 45 0 0.0299709 5 llama-2-chat:7:ggufv2:Q6_K significance 0.806474 27 0 0.0298694 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M significance 1.31454 45 0 0.0292119 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M significance 1.2312 45 0 0.0273599 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M significance 1.01129 45 0 0.0224731 5 llama-3-instruct:8:ggufv2:Q6_K significance 0.994971 45 0 0.0221105 5 llama-3-instruct:8:ggufv2:Q8_0 significance 0.957259 45 0 0.0212724 5 llama-2-chat:70:ggufv2:Q3_K_M significance 0.758379 45 0 0.0168529 5 llama-2-chat:70:ggufv2:Q2_K significance 0.716547 45 0 0.0159233 5 llama-2-chat:70:ggufv2:Q4_K_M significance 0.68386 45 0 0.0151969 5 llama-3-instruct:8:ggufv2:Q5_K_M significance 0.636128 45 0 0.0141362 5 llama-2-chat:70:ggufv2:Q5_K_M significance 0.518572 45 0 0.0115238 5 llama-2-chat:7:ggufv2:Q4_K_M significance 0.329457 45 0 0.00732127 5 llama-2-chat:13:ggufv2:Q8_0 significance 0.326026 45 0 0.00724502 5 llama-2-chat:7:ggufv2:Q5_K_M significance 0.281188 45 0 0.00624862 5 llama-3-instruct:8:ggufv2:Q4_K_M significance 0.228461 45 0 0.00507691 5 llama-2-chat:13:ggufv2:Q4_K_M significance 0.213246 45 0 0.0047388 5 llama-2-chat:13:ggufv2:Q2_K significance 0.207957 45 0 0.00462127 5 llama-2-chat:13:ggufv2:Q5_K_M significance 0.205271 45 0 0.00456158 5 llama-2-chat:7:ggufv2:Q3_K_M significance 0.194946 45 0 0.00433214 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K significance 0.178078 45 0 0.00395728 5 llama-2-chat:13:ggufv2:Q3_K_M significance 0.131484 45 0 0.00292186 5 code-llama-instruct:7:ggufv2:Q4_K_M significance 0.123914 45 0 0.00275365 5 chatglm3:6:ggmlv3:q4_0 significance 0.118153 45 0 0.00262562 5 llama-2-chat:7:ggufv2:Q2_K significance 0.103278 45 0 0.00229507 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 stats 27 27 0 1 3 claude-3-opus-20240229 stats 27 27 0 1 3 gpt-4-turbo-2024-04-09 stats 45 45 0 1 5 gpt-4o-2024-08-06 stats 26.5385 27 0 0.982906 3 gpt-4o-mini-2024-07-18 stats 42.5641 45 0 0.945869 5 llama-3.1-instruct:70:ggufv2:IQ4_XS stats 24 27 0 0.888889 3 llama-3.1-instruct:8:ggufv2:Q8_0 stats 18.6622 27 0 0.691194 3 llama-3.1-instruct:8:ggufv2:Q6_K stats 18.6622 27 0 0.691194 3 llama-3.1-instruct:8:ggufv2:IQ4_XS stats 18.3112 27 1.88853e-18 0.678192 3 llama-3.1-instruct:8:ggufv2:Q5_K_M stats 18.3058 27 0 0.677993 3 llama-3.1-instruct:70:ggufv2:Q3_K_S stats 18 27 0 0.666667 3 llama-3.1-instruct:70:ggufv2:IQ2_M stats 18 27 0 0.666667 3 llama-3.1-instruct:8:ggufv2:Q4_K_M stats 17.2171 27 0.033358 0.637672 3 llama-3.1-instruct:8:ggufv2:Q3_K_L stats 15.7067 27 0 0.581731 3 gpt-4-0125-preview stats 8.86667 45 0 0.197037 5 openhermes-2.5:7:ggufv2:Q8_0 stats 8.66667 45 0 0.192593 5 openhermes-2.5:7:ggufv2:Q6_K stats 8.66667 45 0 0.192593 5 openhermes-2.5:7:ggufv2:Q5_K_M stats 8.52821 45 0 0.189516 5 gpt-4-0613 stats 8.51282 45 0 0.189174 5 gpt-4o-2024-05-13 stats 8.51282 45 0 0.189174 5 openhermes-2.5:7:ggufv2:Q4_K_M stats 8.25641 45 0 0.183476 5 openhermes-2.5:7:ggufv2:Q3_K_M stats 8.05641 45 0 0.179031 5 openhermes-2.5:7:ggufv2:Q2_K stats 8 45 0 0.177778 5 gpt-3.5-turbo-0613 stats 7.98135 45 0 0.177363 5 gpt-3.5-turbo-0125 stats 7.12976 45 0 0.158439 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M stats 6.89091 45 0 0.153131 5 llama-2-chat:13:ggufv2:Q6_K stats 4.05299 27 2.26623e-17 0.150111 3 llama-2-chat:7:ggufv2:Q8_0 stats 3.87128 27 7.55411e-18 0.143381 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M stats 6.29908 45 0 0.13998 5 mistral-instruct-v0.2:7:ggufv2:Q6_K stats 6.18322 45 0 0.137405 5 llama-3-instruct:8:ggufv2:Q8_0 stats 5.17406 45 0 0.114979 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M stats 5.1041 45 0 0.113424 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 stats 5.04591 45 0 0.112131 5 llama-2-chat:7:ggufv2:Q6_K stats 2.99816 27 7.55411e-18 0.111043 3 mistral-instruct-v0.2:7:ggufv2:Q2_K stats 4.63496 45 0 0.102999 5 llama-3-instruct:8:ggufv2:Q6_K stats 4.30739 45 0 0.0957198 5 llama-3-instruct:8:ggufv2:Q5_K_M stats 3.9346 45 0 0.0874356 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 stats 3.60737 45 0 0.0801638 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M stats 3.58841 45 0 0.0797425 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K stats 3.21213 45 0 0.0713807 5 llama-2-chat:70:ggufv2:Q4_K_M stats 3.08109 45 0 0.0684688 5 llama-3-instruct:8:ggufv2:Q4_K_M stats 2.98843 45 0 0.0664096 5 llama-2-chat:70:ggufv2:Q2_K stats 2.65216 45 0 0.0589368 5 llama-2-chat:70:ggufv2:Q3_K_M stats 2.44276 45 0 0.0542835 5 llama-2-chat:70:ggufv2:Q5_K_M stats 2.3993 45 0 0.0533177 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M stats 2.21549 45 0 0.049233 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M stats 1.96241 45 0 0.0436091 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K stats 1.76057 45 0 0.0391237 5 llama-2-chat:7:ggufv2:Q4_K_M stats 1.43589 45 0 0.0319086 5 llama-2-chat:13:ggufv2:Q4_K_M stats 1.41695 45 0 0.0314878 5 llama-2-chat:13:ggufv2:Q8_0 stats 1.38608 45 0 0.0308019 5 llama-2-chat:7:ggufv2:Q5_K_M stats 1.3859 45 0 0.0307977 5 llama-2-chat:13:ggufv2:Q3_K_M stats 1.35834 45 0 0.0301854 5 llama-2-chat:13:ggufv2:Q5_K_M stats 1.3371 45 0 0.0297134 5 llama-2-chat:13:ggufv2:Q2_K stats 1.12439 45 0 0.0249865 5 code-llama-instruct:7:ggufv2:Q4_K_M stats 0.860471 45 0 0.0191216 5 llama-2-chat:7:ggufv2:Q3_K_M stats 0.804538 45 0 0.0178786 5 llama-2-chat:7:ggufv2:Q2_K stats 0.558031 45 0 0.0124007 5 chatglm3:6:ggmlv3:q4_0 stats 0.17925 45 0 0.00398332 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4-turbo-2024-04-09 99 100 0.447214 0.99 5 gpt-4o-mini-2024-07-18 98 100 0.547723 0.98 5 gpt-4o-2024-05-13 96 100 0.83666 0.96 5 <p></p>"},{"location":"BioChatter/benchmark/results/#medical-exam-question-answering","title":"Medical Exam Question Answering","text":"<p>In this set of tasks, we test LLM abilities to answer medical exam questions.</p> Overall Performance Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-08-06 806 948 2.73205 0.850211 3 gpt-4o-mini-2024-07-18 1352 1608 1.44215 0.840796 5 gpt-4-turbo-2024-04-09 1366 1632 3.231 0.83701 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 765 930 0 0.822581 3 claude-3-opus-20240229 754 936 2.88675 0.805556 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 732 915 0 0.8 3 gpt-4-0125-preview 837 1077 4.04145 0.777159 3 claude-3-5-sonnet-20240620 759 981 0 0.7737 3 llama-3.1-instruct:70:ggufv2:IQ2_M 684 885 0 0.772881 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 657 855 0 0.768421 3 llama-3.1-instruct:8:ggufv2:Q8_0 657 858 0 0.765734 3 gpt-4o-2024-05-13 820 1074 4.04145 0.763501 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 654 864 0 0.756944 3 llama-3.1-instruct:8:ggufv2:Q6_K 645 858 0 0.751748 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 636 849 0 0.749117 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 621 837 0 0.741935 3 gpt-4-0613 785 1074 3.4641 0.730912 3 gpt-3.5-turbo-0125 721 1074 7.50555 0.671322 3 llama-3-instruct:8:ggufv2:Q8_0 690 1077 0 0.640669 3 llama-3-instruct:8:ggufv2:Q5_K_M 684 1077 0 0.635097 3 llama-3-instruct:8:ggufv2:Q4_K_M 673 1077 0.57735 0.624884 3 llama-3-instruct:8:ggufv2:Q6_K 672 1077 0 0.623955 3 openhermes-2.5:7:ggufv2:Q4_K_M 628 1071 1.73205 0.586368 3 openhermes-2.5:7:ggufv2:Q8_0 618 1071 0 0.577031 3 openhermes-2.5:7:ggufv2:Q6_K 615 1071 0 0.57423 3 openhermes-2.5:7:ggufv2:Q5_K_M 612 1071 0 0.571429 3 openhermes-2.5:7:ggufv2:Q3_K_M 604 1071 1.73205 0.563959 3 openhermes-2.5:7:ggufv2:Q2_K 579 1074 0 0.539106 3 llama-2-chat:13:ggufv2:Q8_0 462 1071 0 0.431373 3 llama-2-chat:13:ggufv2:Q5_K_M 462 1071 0 0.431373 3 llama-2-chat:13:ggufv2:Q4_K_M 459 1071 0 0.428571 3 llama-2-chat:13:ggufv2:Q6_K 459 1071 0 0.428571 3 llama-2-chat:13:ggufv2:Q3_K_M 459 1071 0 0.428571 3 chatglm3:6:ggmlv3:q4_0 457 1071 21.6162 0.426704 3 llama-2-chat:13:ggufv2:Q2_K 444 1071 0 0.414566 3 llama-2-chat:7:ggufv2:Q6_K 435 1071 0 0.406162 3 llama-2-chat:7:ggufv2:Q8_0 429 1071 0 0.40056 3 llama-2-chat:7:ggufv2:Q5_K_M 429 1071 0 0.40056 3 llama-2-chat:7:ggufv2:Q4_K_M 429 1071 0 0.40056 3 llama-2-chat:7:ggufv2:Q3_K_M 423 1071 0 0.394958 3 llama-2-chat:7:ggufv2:Q2_K 396 1071 0 0.369748 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 395 1071 0.57735 0.368814 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 393 1071 0 0.366947 3 mistral-instruct-v0.2:7:ggufv2:Q6_K 393 1071 0 0.366947 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 391 1071 0.57735 0.365079 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 390 1071 0 0.364146 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 386 1071 0.57735 0.360411 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 384 1071 0 0.358543 3 mistral-instruct-v0.2:7:ggufv2:Q2_K 378 1071 0 0.352941 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 378 1071 0 0.352941 3 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 367 1071 0.57735 0.34267 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 353 1071 0.57735 0.329599 3"},{"location":"BioChatter/features/","title":"Features","text":"<p>Here we describe the features of the BioChatter framework and their standard usage scenarios. For individual components, please check the side bar navigation.</p>"},{"location":"BioChatter/features/api/","title":"API Calling","text":""},{"location":"BioChatter/features/api/#overview","title":"Overview","text":"<p>BioChatter provides access to biological databases through chat via the API Agent. It is designed to interact with various external APIs and provides a structured approach to generating queries, fetching results, and interpreting the responses from different API services.</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant APIAgent\n    participant External Software\n\n    External Software -&gt;&gt; APIAgent: API definition\n    User/Primary Agent -&gt;&gt; APIAgent: question\n    APIAgent -&gt;&gt; APIAgent: parameterise API\n    APIAgent -&gt;&gt; APIAgent: generate API query\n    APIAgent -&gt;&gt; External Software: submit query (optional)\n    APIAgent -&gt;&gt; External Software: fetch result\n    External Software -&gt;&gt; APIAgent: return result\n    APIAgent -&gt;&gt; APIAgent: summarise / interpret (optional)\n    APIAgent -&gt;&gt; User/Primary Agent: return results</code></pre>"},{"location":"BioChatter/features/api/#example-oncokb-integration","title":"Example: OncoKB Integration","text":"<p>The OncoKB API integration allows users to query oncologically relevant genomic information. Here's a minimal working example:</p> <pre><code>from biochatter.llm_connect import GptConversation\nfrom biochatter.api_agent.base.api_agent import APIAgent\nfrom biochatter.api_agent.web.oncokb import OncoKBQueryBuilder, OncoKBFetcher, OncoKBInterpreter\n\n# Set up a conversation factory (you might need to adjust this based on your setup)\ndef conversation_factory():\n    return GptConversation(model_name=\"gpt-4\", prompts={}, correct=False)\n\n# Create an API agent for OncoKB\noncokb_agent = APIAgent(\n    conversation_factory=conversation_factory,\n    query_builder=OncoKBQueryBuilder(),\n    fetcher=OncoKBFetcher(),\n    interpreter=OncoKBInterpreter()\n)\n\n# Execute a query\nquestion = \"What is the oncogenic potential of BRAF V600E mutation?\"\nresult = oncokb_agent.execute(question)\n\nprint(result)\n</code></pre> <p>This example demonstrates how to set up and use the OncoKB API agent to query genomic information. The execute method handles the entire process of query generation, API interaction, and result interpretation.</p>"},{"location":"BioChatter/features/api/#core-components","title":"Core components","text":"<p>The API agent module is organized into three main submodules:</p> <ul> <li><code>base</code>: Contains core abstractions and base classes</li> <li><code>web</code>: Implementations for web-based APIs (e.g., OncoKB, BLAST)</li> <li><code>python</code>: Implementations for Python package APIs (e.g., Scanpy)</li> </ul>"},{"location":"BioChatter/features/api/#api-agent","title":"API Agent","text":"<p>The main class that orchestrates the API interaction process, located in <code>biochatter.api_agent.base.api_agent</code>.</p> <p>Key Methods:</p> <ul> <li> <p><code>execute(question: str) -&gt; Optional[str]</code>: Executes the full API interaction pipeline.</p> </li> <li> <p><code>parameterise_query(question: str) -&gt; Optional[BaseModel]</code>: Generates a structured query based on the input question.</p> </li> <li> <p><code>fetch_results(query_model: \"BaseModel\") -&gt; Optional[str]</code>: Retrieves results from the API.</p> </li> <li> <p><code>summarise_results(question: str, response_text: str) -&gt; Optional[str]</code>: Interprets and summarizes the API response with respect to the question.</p> </li> </ul>"},{"location":"BioChatter/features/api/#queryparameters","title":"QueryParameters","text":"<p>The <code>QueryParameters</code> class is a Pydantic model that defines the structure and validation rules for API query parameters. It serves several important purposes:</p> <ul> <li> <p>Structure Definition: It clearly defines all the parameters required for an API call, including their types and default values.</p> </li> <li> <p>Validation: Pydantic automatically validates the data, ensuring that all required fields are present and that the data types are correct.</p> </li> <li> <p>Documentation: Each field can include a description, providing clear documentation for developers and assisting the LLM in understanding how to use each parameter.</p> </li> <li> <p>Type Hinting: It provides type hints, improving code readability and enabling better IDE support.</p> </li> <li> <p>Serialization: The Pydantic model can easily be serialized to and deserialized from JSON, facilitating data transfer.</p> </li> </ul>"},{"location":"BioChatter/features/api/#base-classes","title":"Base Classes","text":"<p>Located in <code>biochatter.api_agent.base.agent_abc</code>:</p>"},{"location":"BioChatter/features/api/#basequerybuilder","title":"BaseQueryBuilder","text":"<p>Abstract base class for creating query builders specific to different APIs.</p> <p>Key Methods:</p> <ul> <li> <p><code>create_runnable(query_parameters: BaseModel, conversation: Conversation) -&gt; Callable</code>: Creates runnable object for executing queries.</p> </li> <li> <p><code>parameterise_query(question: str, conversation: Conversation) -&gt; BaseModel</code>: Generates a parameterized query object based on the input question.</p> </li> </ul>"},{"location":"BioChatter/features/api/#basefetcher","title":"BaseFetcher","text":"<p>An abstract base class for implementing API-specific data fetching logic.</p> <p>Key Method:</p> <ul> <li><code>fetch_results(query_model)</code>: Submits the query to the API and retrieves the results.</li> </ul>"},{"location":"BioChatter/features/api/#baseinterpreter","title":"BaseInterpreter","text":"<p>An abstract base class for implementing API-specific response interpretation logic.</p> <p>Key Method:</p> <ul> <li><code>summarise_results(question: str, conversation_factory: Callable, response_text: str) -&gt; str</code>: Summarizes and interprets the API response.</li> </ul>"},{"location":"BioChatter/features/api/#setting-up-a-new-api-integration","title":"Setting up a new API integration","text":""},{"location":"BioChatter/features/api/#overview_1","title":"Overview","text":"<p>Create specific implementations of QueryParameters, BaseQueryBuilder, BaseFetcher, and BaseInterpreter for your target API and design prompts. Instantiate an APIAgent with these components. Use the execute method of APIAgent to process questions and interact with the API.</p>"},{"location":"BioChatter/features/api/#1-specific-queryparameters","title":"1. Specific QueryParameters","text":"<p>For each field in your API call a Pydantic field is created. The description must be clear and concise to be understandable by a LLM so that it will fill the field with the appropriate arguments.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass NewAPIQueryParameters(BaseModel):\n    endpoint: str = Field(..., description=\"API endpoint\")\n    param1: str = Field(..., description=\"Description of param1\")\n    param2: int = Field(default=0, description=\"Description of param2\")\n    # Add more parameters as needed\n</code></pre>"},{"location":"BioChatter/features/api/#2-prompt-design","title":"2. Prompt design","text":""},{"location":"BioChatter/features/api/#query_prompt-instructions-for-structured-output-to-write-newapi-call","title":"QUERY_PROMPT: instructions for structured output to write NewAPI call.","text":"<p>Create a prompt that will allow the LLM to obtain sufficient context to fill the Fields in the NewAPIQueryParameters class. Always keep the first two sentences. Secondly, provide instructions on how to interpret questions. Finally, for LLMs with large context windows the entire API documentation can be copy pasted inside of the prompt. Examples of API calls to retrieve the relevant information to a question should also be provided.</p> <pre><code>NewAPI_QUERY_PROMPT = \"\"\"\nYou are a world class algorithm for creating queries in structured formats. Your task is to use OncoKB Web APIs to answer genomic questions.\n\nAPI DOCUMENTATION AND EXAMPLES\"\"\"\n</code></pre>"},{"location":"BioChatter/features/api/#summary_prompt-instructions-on-how-to-interpret-and-answer-user-questions-based-on-retrieved-information","title":"SUMMARY_PROMPT: Instructions on how to interpret and answer user questions based on retrieved information","text":"<p>Follow prompt design below, replace NewAPI accordingly. If results are not satisfactory, engineer the prompts further.</p> <pre><code>NewAPI_SUMMARY_PROMPT = \"\"\"\nYou have to answer this question in a clear and concise manner: {question} Be factual!\\n\\\nYou are a world leading NewAPI domain specialist who knows everything about NewAPI results.\\n\\\nDo not make up information, only use the provided information and mention how relevant the found information is based on your knowledge about NewAPI\\n\\\nHere is the information relevant to the question found on NewAPI:\\n\\\n{context}\n\"\"\"\n</code></pre>"},{"location":"BioChatter/features/api/#3-implement-querybuilder","title":"3. Implement QueryBuilder","text":"<p>Create a class that inherits from BaseQueryBuilder:</p> <pre><code>from biochatter.api_agent.base.agent_abc import BaseQueryBuilder\n\nclass NewAPIQueryBuilder(BaseQueryBuilder):\n    def create_runnable(self,\n        query_parameters: NewAPIQueryParameters,\n        conversation: Conversation\n        ) -&gt; Callable:\n        # Implement method to create a runnable query object\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n    def parameterise_query(self,\n        question: str,\n        conversation: Conversation\n        ) -&gt; NewAPIQueryParameters:\n        # Implement method to generate API parameters from a question\n        runnable = self.create_runnable(\n            query_parameters=NewAPIQueryParameters,\n            conversation=conversation,\n        )\n        NewAPI_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {NewAPI_QUERY_PROMPT}\"}\n        )\n       NewAPI_call_obj.question_uuid = str(uuid.uuid4())\n        return NewAPI_call_obj\n</code></pre>"},{"location":"BioChatter/features/api/#4-implement-fetcher","title":"4. Implement Fetcher","text":"<p>Create a class that inherits from BaseFetcher. Adapt request header in initiation if specific API tokens are required, and implement a logic to build the full URL required for the API call from NewAPIQueryParameters, and execute it.</p> <p>The <code>fetch_results</code> method should return the response text from the API. If this is a two-stage process (e.g., submitting a query and then retrieving the results), you should implement the necessary logic to handle this. You can look at the BLAST fetcher for an example (<code>blast.py</code>).</p> <p>It may be useful to tune the retries parameter to handle network issues or, as in the case of BLAST, the computational time required to process the query.</p> <p>NOTE: if the response is too large for your LLM context window you may have to reduce its size in some way.</p> <pre><code>from biochatter.api_agent.base.agent_abc import BaseFetcher\n\nclass NewAPIFetcher(BaseFetcher):\n        def __init__(self,):\n        self.headers = {\n        }\n        self.base_url = \"https://api.new.org/api/\"\n\n    def fetch_results(\n        self,\n        query_model: NewAPIQueryParameters,\n        retries: int = 3,\n    ) -&gt; str:\n    #implement your logic here\n    return results_response.text\n</code></pre>"},{"location":"BioChatter/features/api/#5-implement-interpreter","title":"5. Implement Interpreter","text":"<p>Create a class that inherits from BaseInterpreter and adapt the system prompt to NewAPI.</p> <pre><code>from biochatter.api_agent.base.agent_abc import BaseInterpreter\n\nclass NewAPIInterpreter(BaseInterpreter):\n    def summarise_results(self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str) -&gt; str:\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world NewAPI specialist who knows everything about NewAPI topic and its results.\",\n                ),\n                (\"user\", \"{input}\"),\n            ]\n        )\n        summary_prompt = NewAPI_SUMMARY_PROMPT.format(\n            question=question, context=response_text\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        answer = chain.invoke({\"input\": {summary_prompt}})\n        return answer\n</code></pre>"},{"location":"BioChatter/features/api/#6-integrate-with-apiagent","title":"6. Integrate with APIAgent","text":"<p>Once you have implemented all components, you can use them with the APIAgent:</p> <pre><code>from biochatter.api_agent.base.api_agent import APIAgent\n\nnew_api_agent = APIAgent(\n    conversation_factory=your_conversation_factory,\n    query_builder=NewAPIQueryBuilder(),\n    fetcher=NewAPIFetcher(),\n    interpreter=NewAPIInterpreter()\n)\n\nresult = new_api_agent.execute(\"Your question here\")\n</code></pre>"},{"location":"BioChatter/features/benchmark/","title":"The BioChatter Living Benchmark","text":"<p>For trustworthy application of LLMs to real-world and biomedical problems, it is imperative to understand their performance and limitations. We need to constantly evaluate the multitude of combinations of individual models and versions, their parameters (e.g., temperature), prompt sets, databases and vector databases, and diverse application scenarios. To this end, we are maintaining a living benchmarking framework that allows us to continuously compare the performance of different models and configurations on a variety of tasks. The procedure of the living benchmark follows the idea of test-driven development, in that test cases are added with practical reference to a desired feature or behaviour. For cases where a model does not yield a perfect response out of the box, which will be most cases, modifications to the components of the framework, such as prompts or functions, are made to improve the model's performance. By tracking model performance on these tests over time, we can gain insights into the reliability of the framework and identify the gaps to be addressed.</p>"},{"location":"BioChatter/features/benchmark/#running-the-benchmark","title":"Running the benchmark","text":"<p>The benchmark uses the pytest framework to orchestrate the evaluation of a number of models on a number of tasks. The benchmark is run on a regular basis, and the results are published in the benchmark section. The benchmarking suite can be found in the <code>benchmark</code> directory of the BioChatter repository. It can be executed using standard pytest syntax, e.g., <code>poetry run pytest benchmark</code>. As default behavior it checks, which test cases have already been executed and only executes the tests that have not been executed yet. To run all benchmarks again, use <code>poetry run pytest benchmark --run-all</code>. If you want to develop and debug the benchmark instead of just running it, please refer to the developer docs.</p> <p>To allow flexible extension of the benchmark, we have implemeted a modular test framework that uses pytest fixtures to allow easy addition of new models and tasks. All setup is done in the <code>conftest.py</code> file in the <code>benchmark</code> directory. The benchmarks are organised by module, task, and subtask. Module names (e.g., <code>biocypher_query_generation</code> or <code>rag_interpretation</code>) group tasks by use case and are used to create benchmark dataset objects for the tests. Tasks (e.g., <code>entity_selection</code> or <code>query_generation</code>) are the actual tests that are executed in each module. The name of the test (minus the <code>test_</code> prefix) is used to generate the result file name. Subtasks (e.g., <code>single_word</code> or <code>multi_word</code>) are used to parametrise the tests and track the results of different subtasks in the result files. In addition, we generate md5 hashes of the test data for each subtask and use them to skip tests that have already been executed. Exemplary tests are defined in a YAML file in the <code>benchmark/data</code> directory, while the actual test dataset is encrypted and only available to the benchmarking pipeline. The result files are simple CSVs whose file names are generated from the name of the test function; they can be found in <code>benchmark/results</code> and contain scores for all executed combination of parameters.</p> <p>To achieve modularity, we use pytest fixtures and parametrization. For instance, to add a new model, we can modify the <code>OPENAI_MODEL_NAMES</code> and <code>XINFERENCE_MODELS</code> dictionary in <code>conftest.py</code>. The latter carries model names, sizes, format, and quantisations. The environment that runs the benchmark needs to make available all prerequisites for the different modules. For instance, the tasks requiring connection to an LLM need to provide the necessary credentials and API keys, or a connection to a self-hosted model. Likewise, the benchmarks of retrieval-augmented generation (RAG) processes require a connection to the RAG agent, e.g., a vector database.</p>"},{"location":"BioChatter/features/benchmark/#calibration","title":"Calibration","text":"<p>To ensure valid assessment of LLM performance, we need to ensure calibration and technical validity of the benchmarking framework. More recent LLMs in particular may be problematic when using publicly available benchmark datasets, since they could have been used for training the model. This is particularly relevant in closed-source (e.g., OpenAI) models. Thus, we need to consider strategies for avoiding contamination, such as hand-crafting datasets, carefully testing for contamination, or using perturbation strategies to generate new datasets from existing ones. Advanced scenarios could utilise LLMs as \"examiners,\" allowing more flexible test design and free-form answers. There is much research into these phenomena, all of which should be considered in the maintenance of this testing framework.</p>"},{"location":"BioChatter/features/benchmark/#aspects-of-benchmarking","title":"Aspects of benchmarking","text":"<p>In the following, we will detail the different aspects of benchmarking that we are currently testing. This is a living document that will be updated as we add new tests and test modules.</p>"},{"location":"BioChatter/features/benchmark/#models","title":"Models","text":"<p>Naturally the biggest impact on BioChatter performance comes with the model used. However, model versions can have a significant impact, which can be obfuscated by the fact that model names are often not unique. For instance, OpenAI's GPT models often have versions with significantly diverging capabilities and performance. Open-source models even come in a range of sizes and quantisations, which can have a significant impact on performance.</p>"},{"location":"BioChatter/features/benchmark/#prompts","title":"Prompts","text":"<p>As has been recently studied extensively, prompt engineering can make or break the performance of a model on a given task. As such, it is important to test the default prompts we commonly use, as well as a range of variations to determine factors of prompt performance and robustness. As an added complexity, LLMs are often used to generate prompts, which theoretically allows for procedural generation of an infinite number of prompts, as long as time and resources allow.</p>"},{"location":"BioChatter/features/benchmark/#model-parameters","title":"Model parameters","text":"<p>The parameters of the model can have a significant impact on the performance of the model. We often set model temperature to 0 to provide consistent results, but some applications may benefit from a higher temperature. In testing, we mostly rely on a temperature of 0 due to the complexity of testing highly variable results in most cases.</p>"},{"location":"BioChatter/features/benchmark/#databases","title":"Databases","text":"<p>An important facet of BioChatter and BioCypher is their combination in querying databases. This helps to ameliorate the limitations of LLMs by providing structured and validated knowledge to counteract hallucinations. To ensure the seamless interaction of BioChatter and BioCypher, we need to test the performance of BioChatter on a variety of databases.</p>"},{"location":"BioChatter/features/benchmark/#vector-databases","title":"Vector databases","text":"<p>Similarly to regular databases, vector databases are an important tool to provide validated knowledge to LLMs. Vector databases bring their own set of parameters and application scenarios, which likewise need to be tested. For instance, the length and overlap of fragments, the embedding algorithms, as well as the semantic search algorithms applied can have an impact on LLM conversation performance.</p>"},{"location":"BioChatter/features/benchmark/#tasks","title":"Tasks","text":"<p>There is a wide range of tasks that are potentially useful to BioChatter users. To cover most scenarios of research and development use, as well as clinical applications, we test a variety of tasks and LLM personas.</p>"},{"location":"BioChatter/features/chat/","title":"Basic Usage: Chat","text":"<p>BioChatter provides access to chat functionality via the <code>Conversation</code> class, which is implemented in several child classes (in the <code>llm_connect.py</code> module) to account for differences in APIs of the LLMs.</p>"},{"location":"BioChatter/features/chat/#setting-up-the-conversation","title":"Setting up the conversation","text":"<p>To start a conversation, we can initialise the Conversation class (here exemplified by GPT):</p> <pre><code>from biochatter.llm_connect import GptConversation\n\nconversation = GptConversation(\n    model_name=\"gpt-3.5-turbo\",\n    prompts={},\n)\nconversation.set_api_key(api_key=\"sk-...\")\n</code></pre> <p>The <code>set_api_key</code> method is needed in order to initialise the conversation for those models that require an API key (which is true for GPT).</p> <p>It is possible to supply a dictionary of prompts to the conversation from the outset, which is formatted in a way to correspond to the different roles of the conversation, i.e., primary and correcting models. Prompts with the <code>primary_model_prompts</code> key will be appended to the System Messages of the primary model, and <code>correcting_agent_prompts</code> will be appended to the System Messages of the correction model at setup. If we pass a dictionary without these keys (or an empty one), there will be no system messages appended to the models. They can however be introduced later by using the following method:</p> <pre><code>conversation.append_system_message(\"System Message\")\n</code></pre> <p>Similarly, the user queries (<code>HumanMessage</code>) are passed to the conversation using <code>conversation.append_user_message(\"User Message\")</code>. For purposes of keeping track of the conversation history, we can also append the model's responses as <code>AIMessage</code> using <code>conversation.append_ai_message</code>.</p>"},{"location":"BioChatter/features/chat/#querying-the-model","title":"Querying the model","text":"<p>After setting up the conversation in this way, for instance by establishing a flattery component (e.g. 'You are an assistant to a researcher ...'), the model can be queried using the <code>query</code> function.</p> <pre><code>msg, token_usage, correction = conversation.query('Question here')\n</code></pre> <p>Note that a query will automatically append a user message to the message history, so there is no need to call <code>append_user_message()</code> again. The query function returns the actual answer of the model (<code>msg</code>), the token usage statistics reported by the API (<code>token_usage</code>), and an optional <code>correction</code> that contains the opinion of the corrective agent.</p>"},{"location":"BioChatter/features/chat/#using-openai-models","title":"Using OpenAI models","text":"<p>Using an OpenAI model via the API is generally the easiest way to get started, but requires the provision of an API key to the OpenAI API. To do this, you can designate the <code>OPENAI_API_KEY</code> variable in your environment directly (<code>export OPENAI_API_KEY=sk-...</code>) by adding it to your shell configuration (e.g., the <code>zshrc</code>).</p>"},{"location":"BioChatter/features/chat/#using-anthropic-models-claude","title":"Using Anthropic models (Claude)","text":"<p>Similarly, to use an Anthropic model, you need a billable account with Anthropic API access, and to set the <code>ANTHROPIC_API_KEY</code> variable in your environment.</p> <pre><code>from biochatter.llm_connect import AnthropicConversation\n\nconversation = AnthropicConversation(\n    model_name=\"claude-3-5-sonnet-20240620\",\n    prompts={},\n)\n</code></pre>"},{"location":"BioChatter/features/chat/#multimodal-models-text-and-image","title":"Multimodal models - Text and image","text":"<p>We support multimodal queries in models that offer these capabilities after the blueprint of the OpenAI API. We can either add an image-containing message to the conversation using the <code>append_image_message</code> method, or we can pass an image URL directly to the <code>query</code> method:</p> <pre><code># Either: Append image message\nconversation.append_image_message(\n    message=\"Here is an attached image\",\n    image_url=\"https://example.com/image.jpg\"\n)\n\n# Or: Query with image included\nmsg, token_usage, correction = conversation.query(\n    \"What's in this image?\",\n    image_url=\"https://example.com/image.jpg\"\n)\n</code></pre>"},{"location":"BioChatter/features/chat/#using-local-images","title":"Using local images","text":"<p>Following the recommendations by OpenAI, we can pass local images as base64-encoded strings. We allow this by setting the <code>local</code> flag to <code>True</code> in the <code>append_image_message</code> method:</p> <pre><code>conversation.append_image_message(\n    message=\"Here is an attached image\",\n    image_url=\"my/local/image.jpg\",\n    local=True\n)\n</code></pre> <p>We also support the use of local images in the <code>query</code> method by detecting the netloc of the image URL. If the netloc is empty, we assume that the image is local and read it as a base64-encoded string:</p> <pre><code>msg, token_usage, correction = conversation.query(\n    \"What's in this image?\",\n    image_url=\"my/local/image.jpg\"\n)\n</code></pre>"},{"location":"BioChatter/features/chat/#open-source-multimodal-models","title":"Open-source multimodal models","text":"<p>While OpenAI models work seamlessly, open-source multimodal models can be buggy or incompatible with certain hardware. We have experienced mixed success with open models and, while they are technically supported by BioChatter, their outputs currently may be unreliable.</p>"},{"location":"BioChatter/features/open-llm/","title":"Open-source and Local LLMs","text":"<p>There are two self-hosted/local LLM solutions that BioChatter currently supports out-of-the-box.</p> <ul> <li>Xorbits Inference</li> <li>Ollama</li> </ul> <p>Below, we provide installation and usage instructions for both of them.</p>"},{"location":"BioChatter/features/open-llm/#xorbits-inference-xinference","title":"Xorbits Inference (Xinference)","text":"<p>Xorbits Inference is an open-source toolkit for running open-source models, particularly language models. To support BioChatter applications in local and protected contexts, we provide API access via BioChatter classes in a unified way. Briefly, this module allows to connect to any open-source model supported by Xinference via the state-of-the-art and easy-to-use OpenAI API. This allows local and remote access to essentially all relevant open-source models, including these builtin models, at very little setup cost.</p>"},{"location":"BioChatter/features/open-llm/#usage","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>XinferenceConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"BioChatter/features/open-llm/#connecting-to-the-model-from-biochatter","title":"Connecting to the model from BioChatter","text":"<p>All that remains once Xinference has started your model is to tell BioChatter the API endpoint of your deployed model via the <code>base_url</code> parameter of the <code>XinferenceConversation</code> class. For instance:</p> <pre><code>from biochatter.llm_connect import XinferenceConversation\n\nconversation = XinferenceConversation(\n    base_url=\"http://localhost:9997\",\n    prompts={},\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"BioChatter/features/open-llm/#deploying-locally-via-docker","title":"Deploying locally via Docker","text":"<p>We have created a Docker workflow that allows the deployment of builtin Xinference models, here. It will soon be available via Dockerhub. There is another workflow that allows mounting (potentially) any compatible model from HuggingFace, here. Note that, due to graphics driver limitations, this currently only works for Linux machines with dedicated Nvidia graphics cards. If you have a different setup, please check below for deploying Xinference without the Docker workflow.</p>"},{"location":"BioChatter/features/open-llm/#deploying-locally-without-docker","title":"Deploying locally without Docker","text":""},{"location":"BioChatter/features/open-llm/#installation","title":"Installation","text":"<p>To run Xinference locally on your computer or a workstation available on your network, follow the official instructions for your type of hardware. Briefly, this includes installing the <code>xinference</code> and <code>ctransformers</code> Python libraries into an environment of your choice, as well as a hardware-specific installation of the <code>llama-ccp-python</code> library.</p>"},{"location":"BioChatter/features/open-llm/#deploying-your-model","title":"Deploying your model","text":"<p>After installation, you can run the model (locally using <code>xinference</code> or in a distributed fashion. After startup, you can visit the local server address in your browser (standard is <code>http://localhost:9997</code>) and select and start your desired model. There is a large selection of predefined models to choose from, as well as the possibility to add your own favourite models to the framework. You will see your running models in the <code>Running Models</code> tab, once they have started.</p> <p>Alternatively, you can deploy (and query) your model via the Xinference Python client:</p> <pre><code>from xinference.client import Client\n\nclient = Client(\"http://localhost:9997\")\nmodel_uid = client.launch_model(model_name=\"chatglm2\")  # download model from HuggingFace and deploy\nmodel = client.get_model(model_uid)\n\nchat_history = []\nprompt = \"What is the largest animal?\"\nmodel.chat(\n    prompt,\n    chat_history,\n    generate_config={\"max_tokens\": 1024}\n)\n</code></pre>"},{"location":"BioChatter/features/open-llm/#ollama","title":"Ollama","text":"<p>Ollama is arguably the biggest open-source project for local LLM hosting right now. In comparison to Xinference it lacks the complete freedom of running any HuggingFace model in a simple fashion, but has the benefit of higher stability for the supported models. The list of supported models is updated diligently by the Ollama community. BioChatter support was added by implementing the LangChain ChatOllama and LangChain OllamaEmbeddings classes, connecting to Ollama APIs.</p>"},{"location":"BioChatter/features/open-llm/#usage_1","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>OllamaConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"BioChatter/features/open-llm/#connecting-to-the-model-from-biochatter_1","title":"Connecting to the model from BioChatter","text":"<p>Once Ollama has been set up (see below), you can directly use BioChatter to connect to the API endpoint and start any available model. It will be downloaded and launched on-demand. You can now configure the <code>OllamaConversation</code> instance setting the <code>base_url</code> and <code>model_name</code> parameters. For example:</p> <pre><code>from biochatter.llm_connect import OllamaConversation\n\nconversation = OllamaConversation(\n    base_url=\"http://localhost:11434\",\n    prompts={},\n    model_name='llama3',\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"BioChatter/features/open-llm/#deploying-locally-via-docker_1","title":"Deploying locally via Docker","text":"<p>To deploy Ollama with Docker is extremely easy and well documented. You can follow the official Ollama Docker blog post for that or check the Ollama DockerHub page that will also help you with the installation of the required <code>nvidia-container-toolkit</code> library if you want to use GPUs from Docker containers.</p>"},{"location":"BioChatter/features/open-llm/#deploying-locally-without-docker_1","title":"Deploying locally without Docker","text":""},{"location":"BioChatter/features/open-llm/#installation_1","title":"Installation","text":"<p>You can download and run Ollama also directly on your computer. For this you can just visit the official website that provides you with an installer for any OS. More info on the setup and startup process can be found in the GitHub README.</p>"},{"location":"BioChatter/features/podcast/","title":"Podcast my Paper","text":"<p>We provide a module to perform document processing and text-to-speech to enable listening to any document in podcast style.  The functionality can be accessed through the podcast API or by running the script <code>scripts/podcast_single_document.py</code>.</p>"},{"location":"BioChatter/features/podcast/#api-access","title":"API access","text":"<p>The podcast API is available through the <code>podcast</code> module. An end-to-end workflow looks like this (modified from the test module):</p> <pre><code>from biochatter.podcast import Podcaster\nfrom biochatter.vectorstore import DocumentReader\n\n# Load document\nreader = DocumentReader()\ndocument = reader.load_document(\"test/dcn.pdf\")\n\n# Initialise podcaster\npodcaster = Podcaster(document)\n\n# Generate podcast (LLM task)\npodcaster.generate_podcast(characters_per_paragraph=5000)\n\n# Employ text-to-speech to generate audio file (optional)\npodcaster.podcast_to_file(\"test/test.mp3\", model=\"tts-1-hd\", voice=\"alloy\")\n</code></pre> <p>If you do not want audio output, you can simply access the generated text though the function <code>podcaster.podcast_to_text()</code>.</p> <p>This example uses the paid OpenAI text-to-speech API to generate the audio file. The default of the <code>podcast_to_file</code> function is to use the free Google text-to-speech API.  When using OpenAI, due to the input length limit of 4096 characters, the podcast is split into multiple parts indicated by integer suffixes.</p>"},{"location":"BioChatter/features/podcast/#command-line-access","title":"Command line access","text":"<p>To generate a podcast from a single document more quickly, you can use the <code>scripts/podcast_single_document.py</code> script.  It accepts two arguments, the path to the document and the path to the desired output file.  If the output file ends in <code>.mp3</code>, the OpenAI text-to-speech API will be used to generate an audio file.  Otherwise, the script will generate a text file and skip the text-to-speech step.  If using the OpenAI text-to-speech API, multiple files will be generated with integer suffixes.  If you installed BioChatter with poetry, you can run the script like this (from the root directory of the repository):</p> <pre><code>poetry run python scripts/podcast_single_document.py test/dcn.pdf test/test.mp3\n</code></pre>"},{"location":"BioChatter/features/rag/","title":"Retrieval-Augmented Generation","text":""},{"location":"BioChatter/features/rag/#overview","title":"Overview","text":"<p>To provide a unified interface for various ways of retrieval-augmented generation (RAG), we provide an implementation that connects to different types of databases and APIs.  These functions are provided by the <code>RagAgent</code> class, which encapsulates the connection, query, search, retrieval, and summarisation functions of the individual agents.</p> <pre><code>classDiagram\n    class RagAgent {\n        -mode: which tool to use, on usage see below\n        -model_name: LLM model, DEPRECATED, should use conversation_factory\n        -connection_args: connectivity to databases\n        -n_results: how many results to use for RAG\n        -use_prompt: whether to use custom prompt for tool, i.e., essentially turning off the RAG mechanism\n        -schema_config_or_info_dict: knowledge graph schema\n        -conversation_factory: function to create Conversation object, i.e., interact with specific LLM\n        -embedding_func: vector DB function to use for vector embeddings\n        -documentids_workspace: ID of vector database scope for search\n        -agent_desc: description of the RagAgent to be used for LLM-driven tool selection\n        +connect()\n        +get_query_results()\n        +similarity_search()\n        +get_description()\n    }\n\n    class DatabaseAgent {\n        -model_name\n        -connection_args\n        -schema_config_or_info_dict\n        -conversation_factory\n        +connect()\n        +get_query_results()\n    }\n\n    class VectorDatabaseAgentMilvus {\n        -embedding_func\n        -connection_args\n        +connect()\n        +similarity_search()\n    }\n\n    class APIAgent {\n        -BlastFetcher\n        -BlastInterpreter\n        -BlastQueryBuilder\n        +connect()\n        +get_query_results()\n    }\n\n    RagAgent --&gt; DatabaseAgent : uses when mode is 'kg'\n    RagAgent --&gt; VectorDatabaseAgentMilvus : uses when mode is 'vectorstore'\n    RagAgent --&gt; APIAgent : uses when mode is, e.g., 'api_blast'\n</code></pre> <p>Each instance of an agent is responsible for receiving a user's question, using the appropriate method to generate queries, fetching results, and returning them in a structured or summarised format to the agent that called it (often, the \"primary\" conversation between the user and an LLM). The decision on which LLM to use is made in supplying a <code>conversation_factory</code> function to the agent. For tool choice, in addition to manual selection, we can also use the <code>RagAgentSelector</code> class from <code>selector_agent.py</code> to automatically select the most appropriate tool based on the question and the description (<code>agent_desc</code>) of the available agents. For more information on the workflows used inside the individual agents, see the documentation for the Knowledge Graph-, Vector Store-, and API Agents.</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant RagAgent\n    participant DatabaseAgent/APIAgent\n\n    User/Primary Agent -&gt;&gt; RagAgent: question\n    RagAgent -&gt;&gt; RagAgent: mode select\n    RagAgent -&gt;&gt; DatabaseAgent/APIAgent: question\n    DatabaseAgent/APIAgent -&gt;&gt; DatabaseAgent/APIAgent: build query\n    DatabaseAgent/APIAgent -&gt;&gt; DatabaseAgent/APIAgent: fetch results\n    DatabaseAgent/APIAgent -&gt;&gt; DatabaseAgent/APIAgent: summarise (optional)\n    DatabaseAgent/APIAgent -&gt;&gt; RagAgent: results\n    RagAgent -&gt;&gt; RagAgent: summarise (optional)\n    RagAgent -&gt;&gt; User/Primary Agent: results</code></pre>"},{"location":"BioChatter/features/rag/#knowledge-graph-rag","title":"Knowledge Graph RAG","text":"<p>To increase accessibility of databases, we can leverage the BioCypher integration of BioChatter.  In BioCypher, we use a YAML configuration (<code>schema_config.yaml</code>) to specify the contents of the knowledge graph and their ontological associations.  We also generate a more extensive, but essentially similar YAML file during the BioCypher creation of a knowledge graph (<code>schema_info.yaml</code>), which contains more information pertinent to LLM interaction with the database.  The current prototypical implementation of query generation through an LLM is implemented in the <code>prompts.py</code> module on the example of a Neo4j knowledge graph connection.</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant DatabaseAgent\n    participant Knowledge Graph\n\n    User/Primary Agent -&gt;&gt; DatabaseAgent: question\n    Knowledge Graph -&gt;&gt; DatabaseAgent: schema information\n    DatabaseAgent -&gt;&gt; DatabaseAgent: select entities\n    DatabaseAgent -&gt;&gt; DatabaseAgent: select relationships\n    DatabaseAgent -&gt;&gt; DatabaseAgent: select properties\n    DatabaseAgent -&gt;&gt; DatabaseAgent: generate query\n    DatabaseAgent -&gt;&gt; Knowledge Graph: submit query\n    Knowledge Graph -&gt;&gt; DatabaseAgent: return results\n    DatabaseAgent -&gt;&gt; DatabaseAgent: summarise (optional)\n    DatabaseAgent -&gt;&gt; User/Primary Agent: return results</code></pre>"},{"location":"BioChatter/features/rag/#connecting","title":"Connecting","text":"<p>The database connectivity of BioChatter to BioCypher knowledge graphs is handled by the <code>DatabaseAgent</code> class (in <code>database_agent.py</code>). In addition, database connectivity can also be handled by the application that uses BioChatter, as shown in the example below. In that case, the application implements calls to the <code>BioCypherPromptEngine</code> class directly without going through the <code>DatabaseAgent</code>. To illustrate this simpler procedure, we provide an example of Neo4j query generation as used in the Pole Crime Dataset demo repository.</p>"},{"location":"BioChatter/features/rag/#querying","title":"Querying","text":"<p>The generation of a query based on BioCypher configuration files is a multi-step process. This is partly to account for the limited token input space of some models, and partly to better be able to test and compare the individual steps. The steps can also be wrapped in a single function, <code>generate_query</code>, which handles the entire process. In addition, we can use the Reflexion Agent to iteratively improve a query based on its results.</p>"},{"location":"BioChatter/features/rag/#setup","title":"Setup","text":"<p>We use the <code>BioCypherPromptEngine</code> class to handle the LLM conversation.</p> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    schema_config_or_info_path=\"test/schema_info.yaml\"\n)\n</code></pre> <p>This will load the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file and set up the conversation.</p>"},{"location":"BioChatter/features/rag/#query-generation","title":"Query generation","text":"<p>Using the <code>generate_query</code> wrapper, we can generate a query from a question and a database language.</p> <pre><code>query = prompt_engine.generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    query_language=\"Cypher\",\n)\n</code></pre> <p>This will return a query that can be used in the database query language (e.g., Cypher). This end-to-end process executes the steps detailed below, namely, entity selection, relationship selection, and property selection, as well as the generation of the final query using the selected components. While you can run each of these steps individually, you need to ensure that the process is run in the correct order, passing the results of each step to the next step as implemented in the <code>generate_query</code> wrapper. The individual steps below are provided for explanation and testing purposes.</p>"},{"location":"BioChatter/features/rag/#entity-selection","title":"Entity selection","text":"<p>Starting from the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file, we first have the model decide which entities in the database are relevant to the user's question.</p> <pre><code>success = prompt_engine._select_entities(\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre> <p>This will select a number of entities from the database schema to be used subsequently, and return True or False to indicate success.</p>"},{"location":"BioChatter/features/rag/#relationship-selection","title":"Relationship selection","text":"<p>Next, we will use the entities determined in the first step to select relationships between them.  The entities selected in the first step will be stored in the <code>selected_entities</code> attribute of the <code>BioCypherPromptEngine</code> instance, and the question is stored in the <code>question</code> attribute.  Both are automatically used to select relationships.</p> <pre><code>success = prompt_engine._select_relationships()\n</code></pre>"},{"location":"BioChatter/features/rag/#property-selection","title":"Property selection","text":"<p>To not unnecessarily waste token input space, we are only interested in selecting properties of entities that are of interest given the question asked. We do so in the third step, which uses the entities and relationships determined in the first steps.  Again, <code>question</code>, <code>selected_entities</code>, and <code>selected_relationships</code> are automatically used to select properties.</p> <pre><code>success = prompt_engine._select_properties()\n</code></pre> <p>This will select a number of properties to be used in the query, and also return True or False to indicate success.</p>"},{"location":"BioChatter/features/rag/#query-generation_1","title":"Query generation","text":"<p>Finally, we can use the entities and relationships, as well as the selected properties, to ask the LLM to generate a query in the desired language.</p> <pre><code>query = prompt_engine._generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    entities=[\"Gene\", \"Disease\"],\n    relationships=[\"GeneToDiseaseAssociation\"],\n    properties={\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]},\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will (hopefully) return a query that can be used in the database query language (e.g., Cypher).</p>"},{"location":"BioChatter/features/rag/#query-interaction","title":"Query interaction","text":"<p>As an optional follow-up, you can interact with the returned query using the <code>BioCypherQueryHandler</code> class (<code>query_interaction.py</code>). It takes the query, the original question and the KG information so that the interaction is still aware of the KG.</p> <pre><code>from biochatter.query_interaction import BioCypherQueryHandler\nquery_handler = BioCypherQueryHandler(\n    query=query,\n    query_lang=\"Cypher\",\n    kg_selected={\n        entities: [\"Gene\", \"Disease\"],\n        relationships: [\"GeneToDiseaseAssociation\"],\n        properties: {\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]}\n    },\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre>"},{"location":"BioChatter/features/rag/#explanation","title":"Explanation","text":"<p>You can retrieve an explanation of the returned query with:</p> <pre><code>explanation = query_handler.explain_query()\n</code></pre>"},{"location":"BioChatter/features/rag/#updating","title":"Updating","text":"<p>Alternatively, you can ask the LLM for an update of the query with:</p> <pre><code>request = \"Only return 10 results and sort them alphabetically\"\nexplanation = query_handler.update_query(request)\n</code></pre> <p>NB: for updates, it might sometimes be relevant that all the KG enitites/relationships/properties are known to the LLM instead of only those that were selected to be relevant for the original question. For this, you can optionally pass them as input to the query handler with <code>kg</code> (similar to <code>kg_selected</code>).</p> <p>(Tip: the prompt_engine object contains both the selected and non-selected entities, relationships, and properties as attributes.)</p>"},{"location":"BioChatter/features/rag/#vectorstore-rag","title":"Vectorstore RAG","text":"<p>To connect to a vector database for using semantic similarity search and retrieval-augmented generation (RAG), we provide an implementation that connects to a Milvus instance (local or remote).  These functions are provided by the modules <code>vectorstore.py</code> (for performing embeddings) and <code>vectorstore_agent.py</code> (for maintaining the connection and search).</p> <p>This is implemented in the BioChatter Light Docker workflow and the BioChatter Docker compose found in this repository.  To start Milvus on its own in these repositories, you can call <code>docker compose up -d standalone</code> (<code>standalone</code> being the Milvus endpoint, which starts two other services alongside it).</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant VectorDatabaseAgent\n    participant Vector Database\n    participant Documents\n\n    Documents -&gt;&gt; Vector Database: embed text fragments\n    User/Primary Agent -&gt;&gt; VectorDatabaseAgent: question\n    VectorDatabaseAgent -&gt;&gt; VectorDatabaseAgent: generate artificial answer (optional)\n    VectorDatabaseAgent -&gt;&gt; VectorDatabaseAgent: embed question or artificial answer\n    VectorDatabaseAgent -&gt;&gt; Vector Database: submit search query embedding\n    Vector Database -&gt;&gt; VectorDatabaseAgent: return most similar embedded fragments\n    VectorDatabaseAgent -&gt;&gt; VectorDatabaseAgent: summarise (optional)\n    VectorDatabaseAgent -&gt;&gt; User/Primary Agent: return results</code></pre>"},{"location":"BioChatter/features/rag/#connecting_1","title":"Connecting","text":"<p>To connect to a vector DB host, we can use the corresponding class:</p> <pre><code>from biochatter.vectorstore_agent import VectorDatabaseAgentMilvus\n\ndbHost = VectorDatabaseAgentMilvus(\n    embedding_func=OpenAIEmbeddings(),\n    connection_args={\"host\": _HOST, \"port\": _PORT},\n    embedding_collection_name=EMBEDDING_NAME,\n    metadata_collection_name=METADATA_NAME\n)\ndbHost.connect()\n</code></pre> <p>This establishes a connection with the vector database (using a host IP and port) and uses two collections, one for the embeddings and one for the metadata of embedded text (e.g. the title and authors of the paper that was embedded).</p>"},{"location":"BioChatter/features/rag/#embedding-documents","title":"Embedding documents","text":"<p>To embed text from documents, we use the LangChain and BioChatter functionalities for processing and passing the text to the vector database.</p> <pre><code>from biochatter.vectorstore import DocumentReader()\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# read and split document at `pdf_path`\nreader = DocumentReader()\ndocs = reader.load_document(pdf_path)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separators=[\" \", \",\", \"\\n\"],\n)\nsplit_text = text_splitter.split_documents(docs)\n\n# embed and store embeddings in the connected vector DB\ndoc_id = dbHost.store_embeddings(splitted_docs)\n</code></pre> <p>The dbHost class takes care of calling an embedding model, storing the embedding in the database, and returning a document ID that can be used to refer to the stored document.</p>"},{"location":"BioChatter/features/rag/#semantic-search","title":"Semantic search","text":"<p>To perform a semantic similarity search, all that is left to do is pass a question or statement to the <code>dbHost</code>, which will be embedded and compared to the present embeddings, returning a number <code>k</code> most similar text fragments.</p> <pre><code>results = dbHost.similarity_search(\n    query=\"Semantic similarity search query\",\n    k=3,\n)\n</code></pre>"},{"location":"BioChatter/features/rag/#vectorstore-management","title":"Vectorstore management","text":"<p>Using the collections we created at setup, we can delete entries in the vector database using their IDs. We can also return a list of all collected docs to determine which we want to delete.</p> <pre><code>docs = dbHost.get_all_documents()\nres = dbHost.remove_document(docs[0][\"id\"])\n</code></pre>"},{"location":"BioChatter/features/rag/#api-calling","title":"API Calling","text":"<p>API calling functionality now has its own docs page.</p>"},{"location":"BioChatter/features/reflexion-agent/","title":"Reflexion Agent","text":"<p>While current LLMs have many capabilities, their outputs can be unstable at times. To stabilise responses and allow more complex agent workflows, we have introduced a Reflexion Agent, allowing agents to reflect on their experiences, score their output, and self-improve.</p>"},{"location":"BioChatter/features/reflexion-agent/#workflow","title":"Workflow","text":"<p>The workflow of a Reflexion Agent is composed of individual nodes that can either generate or consume data as follows:</p> <pre><code>flowchart TD\n    START --&gt; Draft\n    Draft --&gt; ExecuteTool\n    ExecuteTool --&gt; Revise\n    Revise --&gt; ShouldContinue?\n    ShouldContinue? --&gt;|no| END\n    ShouldContinue? --&gt;|yes| ExecuteTool</code></pre> Draft: in this node, an LLM is initially prompted to generate a specific text and action. (mem &lt;- AIMessage(...)). Execute tool: this node executes a tool function based on an action/text generated in the previous node. (mem &lt;- ToolMessage(...)). Revise: this node scores the output of the tool call and generates a self-reflection to provide feedback aimed at improving the results. (mem &lt;- AIMessage(...)). Evaluate (ShouldContinue): this node assesses the quality of the generated outputs. Memory (mem): a list of BaseMessage."},{"location":"BioChatter/features/reflexion-agent/#usage","title":"Usage","text":"<p>The <code>ReflexionAgent</code> class can be used to implement a reflexion workflow. Here, we demonstrate the ability to generate Cypher queries based on user's question using the <code>KGQueryReflexionAgent</code> class, which is derived from the abstract <code>ReflexionAgent</code> base class.</p> <p>To use the <code>KGQueryReflexionAgent</code>:</p> <ol> <li>We pass in connection arguments that enable connection to the target graph database and a conversation factory, which can create an instance of GptConversation (see Basic Usage: Chat).</li> </ol> <pre><code>import os\nfrom biochatter.llm_connect import GptConversation\nfrom biochatter.kg_langgraph_agent import KGQueryReflexionAgent\ndef create_conversation():\n    conversation = GptConversation(model_name=\"gpt-3.5-turbo\", prompts={})\n    conversation.set_api_key(os.getenv(\"OPENAI_API_KEY\"), user=\"my_user\")\n    return conversation\n\nconnection_args = {\n    \"host\": \"127.0.0.1\",\n    \"port\": \"7687\",\n}\n\nagent = KGQueryReflexionAgent(\n    connection_args=connection_args,\n    conversation_factory=create_conversation,\n)\n</code></pre> <ol> <li>We generate the basic Knowledge Graph prompt for the LLM based on a user's question with the <code>BioCypherPromptEngine</code>, which provides node info, edge info, and node and edge properties based on the user's question.</li> </ol> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    model_name=\"gpt-3.5-turbo\",\n    schema_config_or_info_dict=schema_dict,  # the schema definition of our graph\n    conversation_factory=create_conversation,\n)\nkg_prompt = prompt_engine.generate_query_prompt(question)\n</code></pre> <ol> <li>We can now use the agent to generate and reflect on the Cypher query and optimise it.</li> </ol> <pre><code>cypher_query = agent.execute(question, kg_prompt)\n</code></pre>"},{"location":"BioChatter/features/reflexion-agent/#implementation","title":"Implementation","text":"<p>To use the <code>ReflexionAgent</code> class, we need to implement the following abstract methods:</p> <ol> <li> <p>_tool_function(self, state: List[BaseMessage]): execute tool function based on previous action/text and return ToolMessage</p> </li> <li> <p>_create_initial_responder(self, prompt: Option[str]): create draft responder, which is used to generate the initial answer</p> </li> <li> <p>_create_revise_responder(self, prompt: Optional[str]): create revise responder, which is used to score outputs and revise the answers</p> </li> <li> <p>_log_step_message(self, step: int, node: str, output: BaseMessage): parse step message and generate logs</p> </li> <li> <p>_log_final_result(self, output: BaseMessage): parse final result and generate logs</p> </li> <li> <p>_parse_final_result(self, output: BaseMessage): parse final result</p> </li> </ol> <p>As an example, we use the <code>kg_langgraph_agent.py</code> implementation that can reflect on the task of generating a knowledge graph query. The <code>KGQueryReflexionAgent</code> derived from <code>ReflexionAgent</code> is the main class to perform this task. In the <code>KGQueryReflexionAgent</code>, we have implemented the abstract methods described above:</p> <ol> <li> <p>_tool_function(self, state: List[BaseMessage]): connect to kg database and query KG in draft/revise node</p> </li> <li> <p>_create_initial_responder(self, prompt: Option[str]): create initial responder, which prompts LLM to generate the query</p> </li> </ol> <p>initial prompts: <pre><code>(\n    \"system\",\n    (\n        \"As a senior biomedical researcher and graph database expert, \"\n        f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n        \"\"\"Current time {time}. {instruction}\"\"\"\n    ),\n),\n(\n    \"system\",\n    \"Only generate query according to the user's question above.\",\n),\n</code></pre></p> <p>Initial answer schema:</p> <pre><code>class GenerateQuery(BaseModel):\n    \"\"\"Generate the query.\"\"\"\n\n    answer: str = Field(\n        description=\"Cypher query for graph database according to user's question.\"\n    )\n    reflection: str = Field(\n        description=\"Your reflection on the initial answer, critique of what to improve\"\n    )\n    search_queries: List[str] = Field(description=\"query for graph database\")\n</code></pre> <ol> <li>_create_revise_responder(self, prompt: Optional[str]): create revise responder, which prompts LLM to score the outputs, reflects on the outputs, and revises the current query</li> </ol> <p>Revise prompts:</p> <pre><code>\"\"\"\nRevise your previous query using the query result and follow the guidelines:\n1. If you consistently obtain empty results, please consider removing constraints such as relationship constraints to try to obtain a result.\n2. You should use previous critique to improve your query.\n3. Only generate a query without returning any other text.\n\"\"\"\n</code></pre> <p>Revise answer schema:</p> <pre><code>class ReviseQuery(GenerateQuery):\n    \"\"\"Revise your previous query according to your question.\"\"\"\n\n    revised_query: str = Field(description=REVISED_QUERY_\"Revised query\"DESCRIPTION)\n    score: str = Field(description=(\n    \"the score for the query based on its query result\"\n    \" and relevance to the user's question,\"\n    \" with 0 representing the lowest score and 10 representing the highest score.\"))\n</code></pre> <ol> <li> <p>_log_step_message(self, step: int, node: str, output: BaseMessage): parse message from current step and generate logs</p> </li> <li> <p>_log_final_result(self, output: BaseMessage): parse final result and generate logs</p> </li> <li> <p>_parse_final_result(self, output: BaseMessage): parse final result</p> </li> <li> <p>_should_continue(self, state: List[BaseMessage]): assess output and determine if we can exit loop based on the following rules:   1). if loop steps are greater than limit (30 or user defined), exit   2). if score in previous revise node is greater than 7, exit   3). if query result in execute_tool node is not empty, exit</p> </li> </ol>"},{"location":"BioChatter/features/wasm/","title":"LLM in your Browser - WebAssembly","text":"<p>Coming soon.</p>"},{"location":"BioChatter/scripts/__init__/","title":"init","text":""},{"location":"BioChatter/scripts/_plotting/","title":"plotting","text":"In\u00a0[\u00a0]: Copied! <pre>import math\nimport re\n</pre> import math import re In\u00a0[\u00a0]: Copied! <pre>import colorcet as cc\nimport matplotlib\nimport numpy as np\nimport seaborn as sns\n</pre> import colorcet as cc import matplotlib import numpy as np import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> matplotlib.use(\"Agg\") import matplotlib.pyplot as plt import pandas as pd In\u00a0[\u00a0]: Copied! <pre>BENCHMARKED_MODELS = [\n    \"chatglm3\",\n    \"claude-3-5-sonnet-20240620\",\n    \"claude-3-opus-20240229\",\n    \"code-llama-instruct\",\n    \"gpt-3.5-turbo-0613\",\n    \"gpt-3.5-turbo-0125\",\n    \"gpt-4-0613\",\n    \"gpt-4-0125-preview\",\n    \"gpt-4-1106-preview\",\n    \"gpt-4-turbo-2024-04-09\",\n    \"gpt-4o-2024-05-13\",\n    \"gpt-4o-2024-08-06\",\n    \"gpt-4o-2024-11-20\",\n    \"gpt-4o-mini-2024-07-18\",\n    \"llama-2-chat\",\n    \"llama-3-instruct\",\n    \"llama-3.1-instruct\",\n    \"mixtral-instruct-v0.1\",\n    \"mistral-instruct-v0.2\",\n    \"openhermes-2.5\",\n]\n</pre> BENCHMARKED_MODELS = [     \"chatglm3\",     \"claude-3-5-sonnet-20240620\",     \"claude-3-opus-20240229\",     \"code-llama-instruct\",     \"gpt-3.5-turbo-0613\",     \"gpt-3.5-turbo-0125\",     \"gpt-4-0613\",     \"gpt-4-0125-preview\",     \"gpt-4-1106-preview\",     \"gpt-4-turbo-2024-04-09\",     \"gpt-4o-2024-05-13\",     \"gpt-4o-2024-08-06\",     \"gpt-4o-2024-11-20\",     \"gpt-4o-mini-2024-07-18\",     \"llama-2-chat\",     \"llama-3-instruct\",     \"llama-3.1-instruct\",     \"mixtral-instruct-v0.1\",     \"mistral-instruct-v0.2\",     \"openhermes-2.5\", ] In\u00a0[\u00a0]: Copied! <pre>MODEL_SIZE_ORDER = [\n    \"Unknown\",\n    \"175\",\n    \"70\",\n    \"46,7\",\n    \"34\",\n    \"13\",\n    \"8\",\n    \"7\",\n    \"6\",\n]\n</pre> MODEL_SIZE_ORDER = [     \"Unknown\",     \"175\",     \"70\",     \"46,7\",     \"34\",     \"13\",     \"8\",     \"7\",     \"6\", ] In\u00a0[\u00a0]: Copied! <pre>def plot_text2cypher() -&gt; None:\n    \"\"\"Plot text2cypher tasks.\n\n    Get entity_selection, relationship_selection, property_selection,\n    property_exists, query_generation, and end_to_end_query_generation results\n    files, combine and preprocess them and plot the accuracy for each model as a\n    boxplot.\n    \"\"\"\n    entity_selection = pd.read_csv(\"benchmark/results/entity_selection.csv\")\n    entity_selection[\"task\"] = \"entity_selection\"\n    relationship_selection = pd.read_csv(\n        \"benchmark/results/relationship_selection.csv\",\n    )\n    relationship_selection[\"task\"] = \"relationship_selection\"\n    property_selection = pd.read_csv(\"benchmark/results/property_selection.csv\")\n    property_selection[\"task\"] = \"property_selection\"\n    property_exists = pd.read_csv(\"benchmark/results/property_exists.csv\")\n    property_exists[\"task\"] = \"property_exists\"\n    query_generation = pd.read_csv(\"benchmark/results/query_generation.csv\")\n    query_generation[\"task\"] = \"query_generation\"\n    end_to_end_query_generation = pd.read_csv(\n        \"benchmark/results/end_to_end_query_generation.csv\",\n    )\n    end_to_end_query_generation[\"task\"] = \"end_to_end_query_generation\"\n\n    # combine all results\n    results = pd.concat(\n        [\n            entity_selection,\n            relationship_selection,\n            property_selection,\n            property_exists,\n            query_generation,\n            end_to_end_query_generation,\n        ],\n    )\n\n    # calculate accuracy\n    results[\"score_possible\"] = results[\"score\"].apply(\n        lambda x: float(x.split(\"/\")[1]),\n    )\n    results[\"scores\"] = results[\"score\"].apply(lambda x: x.split(\"/\")[0])\n    results[\"score_achieved\"] = results[\"scores\"].apply(\n        lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),\n    )\n    results[\"accuracy\"] = results[\"score_achieved\"] / results[\"score_possible\"]\n    results[\"score_sd\"] = results[\"scores\"].apply(\n        lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),\n    )\n\n    results[\"model\"] = results[\"model_name\"].apply(lambda x: x.split(\":\")[0])\n    # create labels: openhermes, llama-3, gpt, based on model name, for all\n    # other models, use \"other open source\"\n    results[\"model_family\"] = results[\"model\"].apply(\n        lambda x: (\n            \"openhermes\"\n            if \"openhermes\" in x\n            else (\"llama-3\" if \"llama-3\" in x else \"gpt\" if \"gpt\" in x else \"other open source\")\n        ),\n    )\n\n    # order task by median accuracy ascending\n    task_order = results.groupby(\"task\")[\"accuracy\"].median().sort_values().index\n\n    # order model_family by median accuracy ascending within each task\n    results[\"model_family\"] = results[\"model_family\"].astype(\n        pd.CategoricalDtype(\n            categories=[\"other open source\", \"llama-3\", \"openhermes\", \"gpt\"],\n            ordered=True,\n        ),\n    )\n\n    # plot results per task\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    plt.xticks(rotation=45, ha=\"right\")\n    sns.boxplot(\n        x=\"task\",\n        y=\"accuracy\",\n        hue=\"model_family\",\n        data=results,\n        order=task_order,\n    )\n    plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n    plt.savefig(\n        \"docs/images/boxplot-text2cypher.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n</pre> def plot_text2cypher() -&gt; None:     \"\"\"Plot text2cypher tasks.      Get entity_selection, relationship_selection, property_selection,     property_exists, query_generation, and end_to_end_query_generation results     files, combine and preprocess them and plot the accuracy for each model as a     boxplot.     \"\"\"     entity_selection = pd.read_csv(\"benchmark/results/entity_selection.csv\")     entity_selection[\"task\"] = \"entity_selection\"     relationship_selection = pd.read_csv(         \"benchmark/results/relationship_selection.csv\",     )     relationship_selection[\"task\"] = \"relationship_selection\"     property_selection = pd.read_csv(\"benchmark/results/property_selection.csv\")     property_selection[\"task\"] = \"property_selection\"     property_exists = pd.read_csv(\"benchmark/results/property_exists.csv\")     property_exists[\"task\"] = \"property_exists\"     query_generation = pd.read_csv(\"benchmark/results/query_generation.csv\")     query_generation[\"task\"] = \"query_generation\"     end_to_end_query_generation = pd.read_csv(         \"benchmark/results/end_to_end_query_generation.csv\",     )     end_to_end_query_generation[\"task\"] = \"end_to_end_query_generation\"      # combine all results     results = pd.concat(         [             entity_selection,             relationship_selection,             property_selection,             property_exists,             query_generation,             end_to_end_query_generation,         ],     )      # calculate accuracy     results[\"score_possible\"] = results[\"score\"].apply(         lambda x: float(x.split(\"/\")[1]),     )     results[\"scores\"] = results[\"score\"].apply(lambda x: x.split(\"/\")[0])     results[\"score_achieved\"] = results[\"scores\"].apply(         lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),     )     results[\"accuracy\"] = results[\"score_achieved\"] / results[\"score_possible\"]     results[\"score_sd\"] = results[\"scores\"].apply(         lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),     )      results[\"model\"] = results[\"model_name\"].apply(lambda x: x.split(\":\")[0])     # create labels: openhermes, llama-3, gpt, based on model name, for all     # other models, use \"other open source\"     results[\"model_family\"] = results[\"model\"].apply(         lambda x: (             \"openhermes\"             if \"openhermes\" in x             else (\"llama-3\" if \"llama-3\" in x else \"gpt\" if \"gpt\" in x else \"other open source\")         ),     )      # order task by median accuracy ascending     task_order = results.groupby(\"task\")[\"accuracy\"].median().sort_values().index      # order model_family by median accuracy ascending within each task     results[\"model_family\"] = results[\"model_family\"].astype(         pd.CategoricalDtype(             categories=[\"other open source\", \"llama-3\", \"openhermes\", \"gpt\"],             ordered=True,         ),     )      # plot results per task     sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     plt.xticks(rotation=45, ha=\"right\")     sns.boxplot(         x=\"task\",         y=\"accuracy\",         hue=\"model_family\",         data=results,         order=task_order,     )     plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")     plt.savefig(         \"docs/images/boxplot-text2cypher.png\",         bbox_inches=\"tight\",         dpi=300,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_text2cypher_safety_only():\n    \"\"\"Get entity_selection, relationship_selection, property_selection,\n    property_exists, query_generation, and end_to_end_query_generation results\n    files, combine and preprocess them and plot the accuracy for each model as a\n    boxplot. Only use data that has 'safety' in the task name.\n\n    \"\"\"\n    entity_selection = pd.read_csv(\"benchmark/results/entity_selection.csv\")\n    entity_selection[\"task\"] = \"entity_selection\"\n    relationship_selection = pd.read_csv(\n        \"benchmark/results/relationship_selection.csv\",\n    )\n    relationship_selection[\"task\"] = \"relationship_selection\"\n    property_selection = pd.read_csv(\"benchmark/results/property_selection.csv\")\n    property_selection[\"task\"] = \"property_selection\"\n    property_exists = pd.read_csv(\"benchmark/results/property_exists.csv\")\n    property_exists[\"task\"] = \"property_exists\"\n    query_generation = pd.read_csv(\"benchmark/results/query_generation.csv\")\n    query_generation[\"task\"] = \"query_generation\"\n    end_to_end_query_generation = pd.read_csv(\n        \"benchmark/results/end_to_end_query_generation.csv\",\n    )\n    end_to_end_query_generation[\"task\"] = \"end_to_end_query_generation\"\n\n    # combine all results\n    results = pd.concat(\n        [\n            entity_selection,\n            relationship_selection,\n            property_selection,\n            property_exists,\n            query_generation,\n            end_to_end_query_generation,\n        ],\n    )\n\n    # only include rows where 'safety' is in the subtask name\n    results = results[results[\"subtask\"].str.contains(\"safety\")]\n\n    # calculate accuracy\n    results[\"score_possible\"] = results[\"score\"].apply(\n        lambda x: float(x.split(\"/\")[1]),\n    )\n    results[\"scores\"] = results[\"score\"].apply(lambda x: x.split(\"/\")[0])\n    results[\"score_achieved\"] = results[\"scores\"].apply(\n        lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),\n    )\n    results[\"accuracy\"] = results[\"score_achieved\"] / results[\"score_possible\"]\n    results[\"score_sd\"] = results[\"scores\"].apply(\n        lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),\n    )\n\n    results[\"model\"] = results[\"model_name\"].apply(lambda x: x.split(\":\")[0])\n\n    # order task by median accuracy ascending\n    task_order = results.groupby(\"task\")[\"accuracy\"].median().sort_values().index\n\n    # order model by median accuracy ascending within each task\n    results[\"model\"] = results[\"model\"].astype(\n        pd.CategoricalDtype(\n            categories=results.groupby(\"model\")[\"accuracy\"].median().sort_values().index,\n            ordered=True,\n        ),\n    )\n\n    # plot results per task\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n\n    plt.xticks(rotation=45, ha=\"right\")\n    sns.stripplot(\n        x=\"model\",\n        y=\"accuracy\",\n        hue=\"subtask\",\n        data=results,\n    )\n    plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n    plt.savefig(\n        \"docs/images/stripplot-text2cypher-safety.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n</pre> def plot_text2cypher_safety_only():     \"\"\"Get entity_selection, relationship_selection, property_selection,     property_exists, query_generation, and end_to_end_query_generation results     files, combine and preprocess them and plot the accuracy for each model as a     boxplot. Only use data that has 'safety' in the task name.      \"\"\"     entity_selection = pd.read_csv(\"benchmark/results/entity_selection.csv\")     entity_selection[\"task\"] = \"entity_selection\"     relationship_selection = pd.read_csv(         \"benchmark/results/relationship_selection.csv\",     )     relationship_selection[\"task\"] = \"relationship_selection\"     property_selection = pd.read_csv(\"benchmark/results/property_selection.csv\")     property_selection[\"task\"] = \"property_selection\"     property_exists = pd.read_csv(\"benchmark/results/property_exists.csv\")     property_exists[\"task\"] = \"property_exists\"     query_generation = pd.read_csv(\"benchmark/results/query_generation.csv\")     query_generation[\"task\"] = \"query_generation\"     end_to_end_query_generation = pd.read_csv(         \"benchmark/results/end_to_end_query_generation.csv\",     )     end_to_end_query_generation[\"task\"] = \"end_to_end_query_generation\"      # combine all results     results = pd.concat(         [             entity_selection,             relationship_selection,             property_selection,             property_exists,             query_generation,             end_to_end_query_generation,         ],     )      # only include rows where 'safety' is in the subtask name     results = results[results[\"subtask\"].str.contains(\"safety\")]      # calculate accuracy     results[\"score_possible\"] = results[\"score\"].apply(         lambda x: float(x.split(\"/\")[1]),     )     results[\"scores\"] = results[\"score\"].apply(lambda x: x.split(\"/\")[0])     results[\"score_achieved\"] = results[\"scores\"].apply(         lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),     )     results[\"accuracy\"] = results[\"score_achieved\"] / results[\"score_possible\"]     results[\"score_sd\"] = results[\"scores\"].apply(         lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),     )      results[\"model\"] = results[\"model_name\"].apply(lambda x: x.split(\":\")[0])      # order task by median accuracy ascending     task_order = results.groupby(\"task\")[\"accuracy\"].median().sort_values().index      # order model by median accuracy ascending within each task     results[\"model\"] = results[\"model\"].astype(         pd.CategoricalDtype(             categories=results.groupby(\"model\")[\"accuracy\"].median().sort_values().index,             ordered=True,         ),     )      # plot results per task     sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))      plt.xticks(rotation=45, ha=\"right\")     sns.stripplot(         x=\"model\",         y=\"accuracy\",         hue=\"subtask\",         data=results,     )     plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")     plt.savefig(         \"docs/images/stripplot-text2cypher-safety.png\",         bbox_inches=\"tight\",         dpi=300,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_image_caption_confidence():\n    \"\"\"Get multimodal_answer_confidence.csv file, preprocess it and plot the\n    confidence scores for correct and incorrect answers as histograms. Correct\n    answer confidence values are in the correct_confidence column and incorrect\n    answer confidence values are in the incorrect_confidence column; both\n    columns contain individual confidence values (integers between 1 and 10)\n    separated by semicolons.\n    \"\"\"\n    results = pd.read_csv(\"benchmark/results/multimodal_answer_confidence.csv\")\n    correct_values = results[\"correct_confidence\"].to_list()\n    incorrect_values = results[\"incorrect_confidence\"].to_list()\n    # flatten lists of confidence values\n    correct_values = [int(value) for sublist in correct_values for value in sublist.split(\";\")]\n    for value in list(incorrect_values):\n        if math.isnan(value):\n            incorrect_values.remove(value)\n        if isinstance(value, float):\n            continue\n        if \";\" in value:\n            incorrect_values.remove(value)\n            incorrect_values.extend([int(val) for val in value.split(\";\")])\n\n    incorrect_values = [int(value) for value in incorrect_values]\n\n    # plot histograms of both correct and incorrect confidence values with\n    # transparency, correct green, incorrect red\n    plt.figure(figsize=(6, 4))\n    plt.hist(\n        [correct_values, incorrect_values],\n        bins=range(1, 12),\n        color=[\"green\", \"red\"],\n        label=[\"Correct\", \"Incorrect\"],\n    )\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Count\")\n    plt.xticks(range(1, 11))\n    plt.legend()\n    plt.savefig(\n        \"docs/images/histogram-image-caption-confidence.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n</pre> def plot_image_caption_confidence():     \"\"\"Get multimodal_answer_confidence.csv file, preprocess it and plot the     confidence scores for correct and incorrect answers as histograms. Correct     answer confidence values are in the correct_confidence column and incorrect     answer confidence values are in the incorrect_confidence column; both     columns contain individual confidence values (integers between 1 and 10)     separated by semicolons.     \"\"\"     results = pd.read_csv(\"benchmark/results/multimodal_answer_confidence.csv\")     correct_values = results[\"correct_confidence\"].to_list()     incorrect_values = results[\"incorrect_confidence\"].to_list()     # flatten lists of confidence values     correct_values = [int(value) for sublist in correct_values for value in sublist.split(\";\")]     for value in list(incorrect_values):         if math.isnan(value):             incorrect_values.remove(value)         if isinstance(value, float):             continue         if \";\" in value:             incorrect_values.remove(value)             incorrect_values.extend([int(val) for val in value.split(\";\")])      incorrect_values = [int(value) for value in incorrect_values]      # plot histograms of both correct and incorrect confidence values with     # transparency, correct green, incorrect red     plt.figure(figsize=(6, 4))     plt.hist(         [correct_values, incorrect_values],         bins=range(1, 12),         color=[\"green\", \"red\"],         label=[\"Correct\", \"Incorrect\"],     )     plt.xlabel(\"Confidence\")     plt.ylabel(\"Count\")     plt.xticks(range(1, 11))     plt.legend()     plt.savefig(         \"docs/images/histogram-image-caption-confidence.png\",         bbox_inches=\"tight\",         dpi=300,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_accuracy_per_model(overview) -&gt; None:\n    overview_melted = melt_and_process(overview)\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(10, 6))\n    sns.stripplot(\n        x=\"Model name\",\n        y=\"Accuracy\",\n        hue=\"Size\",\n        data=overview_melted,\n    )\n    plt.title(\n        \"Strip plot across tasks, per Model, coloured by size (billions of parameters)\",\n    )\n    plt.ylim(-0.1, 1.1)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.legend(bbox_to_anchor=(0, 0), loc=\"lower left\")\n    plt.savefig(\n        \"docs/images/stripplot-per-model.png\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n</pre> def plot_accuracy_per_model(overview) -&gt; None:     overview_melted = melt_and_process(overview)      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(10, 6))     sns.stripplot(         x=\"Model name\",         y=\"Accuracy\",         hue=\"Size\",         data=overview_melted,     )     plt.title(         \"Strip plot across tasks, per Model, coloured by size (billions of parameters)\",     )     plt.ylim(-0.1, 1.1)     plt.xticks(rotation=45, ha=\"right\")     plt.legend(bbox_to_anchor=(0, 0), loc=\"lower left\")     plt.savefig(         \"docs/images/stripplot-per-model.png\",         bbox_inches=\"tight\",     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_accuracy_per_quantisation(overview) -&gt; None:\n    overview_melted = melt_and_process(overview)\n\n    sns.set_theme(style=\"whitegrid\")\n\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(\n        x=\"Model name\",\n        y=\"Accuracy\",\n        hue=\"Quantisation\",\n        data=overview_melted,\n    )\n    plt.title(\"Boxplot across tasks, per Quantisation\")\n    plt.xticks(rotation=45)\n    plt.savefig(\n        \"docs/images/boxplot-per-quantisation.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.close()\n</pre> def plot_accuracy_per_quantisation(overview) -&gt; None:     overview_melted = melt_and_process(overview)      sns.set_theme(style=\"whitegrid\")      plt.figure(figsize=(10, 6))     sns.boxplot(         x=\"Model name\",         y=\"Accuracy\",         hue=\"Quantisation\",         data=overview_melted,     )     plt.title(\"Boxplot across tasks, per Quantisation\")     plt.xticks(rotation=45)     plt.savefig(         \"docs/images/boxplot-per-quantisation.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_accuracy_per_task(overview):\n    overview_melted = melt_and_process(overview)\n\n    # concatenate model name and quantisation\n    overview_melted[\"Coarse model name\"] = overview_melted[\"Model name\"].replace(\n        {\n            \"gpt-3.5-turbo-0613\": \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-0125\": \"gpt-3.5-turbo\",\n            \"gpt-4-0613\": \"gpt-4\",\n            \"gpt-4-0125-preview\": \"gpt-4\",\n            \"gpt-4o-2024-05-13\": \"gpt-4\",\n            \"gpt-4o-2024-08-06\": \"gpt-4\",\n        },\n        regex=True,\n    )\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(20, 10))\n\n    # Define the color palette\n    palette = sns.color_palette(\n        \"Set1\",\n        len(overview_melted[\"Coarse model name\"].unique()),\n    )\n\n    # Calculate mean accuracy for each task\n    task_order = overview_melted.groupby(\"Task\")[\"Accuracy\"].mean().sort_values().index[::-1]\n\n    # Sort the dataframe\n    overview_melted[\"Task\"] = pd.Categorical(\n        overview_melted[\"Task\"],\n        categories=task_order,\n        ordered=True,\n    )\n    overview_melted = overview_melted.sort_values(\"Task\")\n\n    sns.stripplot(\n        x=\"Task\",\n        y=\"Accuracy\",\n        hue=\"Coarse model name\",\n        data=overview_melted,\n        dodge=True,\n        palette=palette,\n        jitter=0.2,\n        alpha=0.8,\n    )\n\n    sns.lineplot(\n        x=\"Task\",\n        y=\"Accuracy\",\n        hue=\"Coarse model name\",\n        data=overview_melted,\n        sort=False,\n        legend=False,\n        palette=palette,\n        alpha=0.3,\n    )\n\n    # Get current axis\n    ax = plt.gca()\n\n    # Add vertical lines at each x tick\n    for x in ax.get_xticks():\n        ax.axvline(x=x, color=\"gray\", linestyle=\"--\", lw=0.5)\n\n    plt.legend(bbox_to_anchor=(1, 1), loc=\"upper right\")\n    plt.title(\"Dot plot across models / quantisations, per Task\")\n    plt.xticks(rotation=45)\n    plt.savefig(\n        \"docs/images/dotplot-per-task.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/dotplot-per-task.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n</pre> def plot_accuracy_per_task(overview):     overview_melted = melt_and_process(overview)      # concatenate model name and quantisation     overview_melted[\"Coarse model name\"] = overview_melted[\"Model name\"].replace(         {             \"gpt-3.5-turbo-0613\": \"gpt-3.5-turbo\",             \"gpt-3.5-turbo-0125\": \"gpt-3.5-turbo\",             \"gpt-4-0613\": \"gpt-4\",             \"gpt-4-0125-preview\": \"gpt-4\",             \"gpt-4o-2024-05-13\": \"gpt-4\",             \"gpt-4o-2024-08-06\": \"gpt-4\",         },         regex=True,     )      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(20, 10))      # Define the color palette     palette = sns.color_palette(         \"Set1\",         len(overview_melted[\"Coarse model name\"].unique()),     )      # Calculate mean accuracy for each task     task_order = overview_melted.groupby(\"Task\")[\"Accuracy\"].mean().sort_values().index[::-1]      # Sort the dataframe     overview_melted[\"Task\"] = pd.Categorical(         overview_melted[\"Task\"],         categories=task_order,         ordered=True,     )     overview_melted = overview_melted.sort_values(\"Task\")      sns.stripplot(         x=\"Task\",         y=\"Accuracy\",         hue=\"Coarse model name\",         data=overview_melted,         dodge=True,         palette=palette,         jitter=0.2,         alpha=0.8,     )      sns.lineplot(         x=\"Task\",         y=\"Accuracy\",         hue=\"Coarse model name\",         data=overview_melted,         sort=False,         legend=False,         palette=palette,         alpha=0.3,     )      # Get current axis     ax = plt.gca()      # Add vertical lines at each x tick     for x in ax.get_xticks():         ax.axvline(x=x, color=\"gray\", linestyle=\"--\", lw=0.5)      plt.legend(bbox_to_anchor=(1, 1), loc=\"upper right\")     plt.title(\"Dot plot across models / quantisations, per Task\")     plt.xticks(rotation=45)     plt.savefig(         \"docs/images/dotplot-per-task.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/dotplot-per-task.pdf\",         bbox_inches=\"tight\",     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_scatter_per_quantisation(overview):\n    overview_melted = melt_and_process(overview)\n\n    # remove individual task accuracy columns\n    overview_melted = overview_melted[\n        [\n            \"Model name\",\n            \"Size\",\n            \"Quantisation\",\n            \"Mean Accuracy\",\n            \"Median Accuracy\",\n            \"SD\",\n        ]\n    ]\n\n    # deduplicate remaining rows\n    overview_melted = overview_melted.drop_duplicates()\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(12, 8))\n    # order x axis quantisation values numerically\n    overview_melted[\"Quantisation\"] = pd.Categorical(\n        overview_melted[\"Quantisation\"],\n        categories=[\n            \"2-bit\",\n            \"3-bit\",\n            \"4-bit\",\n            \"5-bit\",\n            \"6-bit\",\n            \"8-bit\",\n            \"&gt;= 16-bit*\",\n        ],\n        ordered=True,\n    )\n    overview_melted[\"Size\"] = pd.Categorical(\n        overview_melted[\"Size\"],\n        categories=[\n            \"Unknown\",\n            \"175\",\n            \"70\",\n            \"46,7\",\n            \"34\",\n            \"13\",\n            \"8\",\n            \"7\",\n            \"6\",\n        ],\n        ordered=True,\n    )\n\n    # Add jitter to x-coordinates\n    x = pd.Categorical(overview_melted[\"Quantisation\"]).codes.astype(float)\n\n    # Create a mask for 'openhermes' and closed models\n    mask_openhermes = overview_melted[\"Model name\"] == \"openhermes-2.5\"\n    mask_closed = overview_melted[\"Model name\"].str.contains(\n        \"gpt|claude\",\n        case=False,\n        regex=True,\n    )\n\n    # Do not add jitter for 'openhermes' model\n    x[mask_openhermes] += 0\n\n    # Manually enter jitter values for closed models\n    jitter_values = {\n        \"gpt-3\": -0.2,\n        \"gpt-4\": 0.2,\n        \"claude-3-opus-20240229\": -0.05,\n        \"claude-3-5-sonnet-20240620\": 0.05,\n    }\n\n    for model, jitter in jitter_values.items():\n        mask_model = overview_melted[\"Model name\"].str.contains(model)\n        x[mask_model] += jitter\n\n    # For other models, add the original jitter\n    x[~mask_openhermes &amp; ~mask_closed] += np.random.normal(\n        0,\n        0.1,\n        size=len(x[~mask_openhermes &amp; ~mask_closed]),\n    )\n\n    # Define the order of model names\n    model_names_order = BENCHMARKED_MODELS\n\n    # Define the order of sizes\n    size_order = MODEL_SIZE_ORDER\n\n    # Create a ColorBrewer palette\n    palette = sns.color_palette(cc.glasbey, n_colors=len(model_names_order))\n\n    # Define a dictionary mapping model names to colors using the order list\n    color_dict = {model: palette[i] for i, model in enumerate(model_names_order)}\n\n    # Use the dictionary as the palette argument in sns.scatterplot\n    ax = sns.scatterplot(\n        x=x,\n        y=\"Median Accuracy\",\n        hue=\"Model name\",\n        size=\"Size\",\n        sizes=(10, 300),\n        data=overview_melted,\n        palette=color_dict,  # Use the color dictionary here\n        alpha=0.5,\n    )\n\n    # Reorder the legend using the same order list\n    handles, labels = ax.get_legend_handles_labels()\n    order = (\n        [\"Model name\"]\n        + [name for name in model_names_order if name in labels]\n        + [\"Size\"]\n        + [size for size in size_order if size in labels]\n    )\n    order_indices = [labels.index(name) for name in order if name in labels]\n    plt.legend(\n        [handles[idx] for idx in order_indices],\n        [labels[idx] for idx in order_indices],\n        bbox_to_anchor=(1.05, 1),\n        loc=\"upper left\",\n    )\n\n    plt.ylim(0, 1)\n    plt.xticks(\n        ticks=range(len(overview_melted[\"Quantisation\"].unique())),\n        labels=overview_melted[\"Quantisation\"].cat.categories,\n    )\n    plt.title(\n        \"Scatter plot across models, per quantisation, coloured by model name, size by model size (billions of parameters)\",\n    )\n    plt.xticks(rotation=45)\n    plt.savefig(\n        \"docs/images/scatter-per-quantisation-name.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/scatter-per-quantisation-name.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n</pre> def plot_scatter_per_quantisation(overview):     overview_melted = melt_and_process(overview)      # remove individual task accuracy columns     overview_melted = overview_melted[         [             \"Model name\",             \"Size\",             \"Quantisation\",             \"Mean Accuracy\",             \"Median Accuracy\",             \"SD\",         ]     ]      # deduplicate remaining rows     overview_melted = overview_melted.drop_duplicates()      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(12, 8))     # order x axis quantisation values numerically     overview_melted[\"Quantisation\"] = pd.Categorical(         overview_melted[\"Quantisation\"],         categories=[             \"2-bit\",             \"3-bit\",             \"4-bit\",             \"5-bit\",             \"6-bit\",             \"8-bit\",             \"&gt;= 16-bit*\",         ],         ordered=True,     )     overview_melted[\"Size\"] = pd.Categorical(         overview_melted[\"Size\"],         categories=[             \"Unknown\",             \"175\",             \"70\",             \"46,7\",             \"34\",             \"13\",             \"8\",             \"7\",             \"6\",         ],         ordered=True,     )      # Add jitter to x-coordinates     x = pd.Categorical(overview_melted[\"Quantisation\"]).codes.astype(float)      # Create a mask for 'openhermes' and closed models     mask_openhermes = overview_melted[\"Model name\"] == \"openhermes-2.5\"     mask_closed = overview_melted[\"Model name\"].str.contains(         \"gpt|claude\",         case=False,         regex=True,     )      # Do not add jitter for 'openhermes' model     x[mask_openhermes] += 0      # Manually enter jitter values for closed models     jitter_values = {         \"gpt-3\": -0.2,         \"gpt-4\": 0.2,         \"claude-3-opus-20240229\": -0.05,         \"claude-3-5-sonnet-20240620\": 0.05,     }      for model, jitter in jitter_values.items():         mask_model = overview_melted[\"Model name\"].str.contains(model)         x[mask_model] += jitter      # For other models, add the original jitter     x[~mask_openhermes &amp; ~mask_closed] += np.random.normal(         0,         0.1,         size=len(x[~mask_openhermes &amp; ~mask_closed]),     )      # Define the order of model names     model_names_order = BENCHMARKED_MODELS      # Define the order of sizes     size_order = MODEL_SIZE_ORDER      # Create a ColorBrewer palette     palette = sns.color_palette(cc.glasbey, n_colors=len(model_names_order))      # Define a dictionary mapping model names to colors using the order list     color_dict = {model: palette[i] for i, model in enumerate(model_names_order)}      # Use the dictionary as the palette argument in sns.scatterplot     ax = sns.scatterplot(         x=x,         y=\"Median Accuracy\",         hue=\"Model name\",         size=\"Size\",         sizes=(10, 300),         data=overview_melted,         palette=color_dict,  # Use the color dictionary here         alpha=0.5,     )      # Reorder the legend using the same order list     handles, labels = ax.get_legend_handles_labels()     order = (         [\"Model name\"]         + [name for name in model_names_order if name in labels]         + [\"Size\"]         + [size for size in size_order if size in labels]     )     order_indices = [labels.index(name) for name in order if name in labels]     plt.legend(         [handles[idx] for idx in order_indices],         [labels[idx] for idx in order_indices],         bbox_to_anchor=(1.05, 1),         loc=\"upper left\",     )      plt.ylim(0, 1)     plt.xticks(         ticks=range(len(overview_melted[\"Quantisation\"].unique())),         labels=overview_melted[\"Quantisation\"].cat.categories,     )     plt.title(         \"Scatter plot across models, per quantisation, coloured by model name, size by model size (billions of parameters)\",     )     plt.xticks(rotation=45)     plt.savefig(         \"docs/images/scatter-per-quantisation-name.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/scatter-per-quantisation-name.pdf\",         bbox_inches=\"tight\",     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_task_comparison(overview):\n    overview_melted = melt_and_process(overview)\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(\n        x=\"Task\",\n        y=\"Accuracy\",\n        hue=\"Task\",\n        data=overview_melted,\n    )\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.savefig(\n        \"docs/images/boxplot-tasks.png\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n</pre> def plot_task_comparison(overview):     overview_melted = melt_and_process(overview)      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(10, 6))     sns.boxplot(         x=\"Task\",         y=\"Accuracy\",         hue=\"Task\",         data=overview_melted,     )     plt.xticks(rotation=45, ha=\"right\")     plt.savefig(         \"docs/images/boxplot-tasks.png\",         bbox_inches=\"tight\",     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_rag_tasks(overview):\n    overview_melted = melt_and_process(overview)\n\n    # select tasks naive_query_generation_using_schema and query_generation\n    overview_melted = overview_melted[\n        overview_melted[\"Task\"].isin(\n            [\n                \"explicit_relevance_of_single_fragments\",\n                \"implicit_relevance_of_multiple_fragments\",\n            ],\n        )\n    ]\n\n    # order models by median accuracy, inverse\n    overview_melted[\"Model name\"] = pd.Categorical(\n        overview_melted[\"Model name\"],\n        categories=overview_melted.groupby(\"Model name\")[\"Median Accuracy\"].median().sort_values().index[::-1],\n        ordered=True,\n    )\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    sns.stripplot(\n        x=\"Model name\",\n        y=\"Accuracy\",\n        hue=\"Quantisation\",\n        data=overview_melted,\n        jitter=0.2,\n        alpha=0.8,\n    )\n    plt.xlabel(None)\n    plt.xticks(rotation=45)\n    plt.legend(bbox_to_anchor=(1, 0), loc=\"lower left\")\n    # Get current axis\n    ax = plt.gca()\n\n    # Add vertical lines at each x tick\n    for x in ax.get_xticks():\n        ax.axvline(x=x, color=\"gray\", linestyle=\"--\", lw=0.5)\n\n    plt.savefig(\n        \"docs/images/stripplot-rag-tasks.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/stripplot-rag-tasks.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n</pre> def plot_rag_tasks(overview):     overview_melted = melt_and_process(overview)      # select tasks naive_query_generation_using_schema and query_generation     overview_melted = overview_melted[         overview_melted[\"Task\"].isin(             [                 \"explicit_relevance_of_single_fragments\",                 \"implicit_relevance_of_multiple_fragments\",             ],         )     ]      # order models by median accuracy, inverse     overview_melted[\"Model name\"] = pd.Categorical(         overview_melted[\"Model name\"],         categories=overview_melted.groupby(\"Model name\")[\"Median Accuracy\"].median().sort_values().index[::-1],         ordered=True,     )      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     sns.stripplot(         x=\"Model name\",         y=\"Accuracy\",         hue=\"Quantisation\",         data=overview_melted,         jitter=0.2,         alpha=0.8,     )     plt.xlabel(None)     plt.xticks(rotation=45)     plt.legend(bbox_to_anchor=(1, 0), loc=\"lower left\")     # Get current axis     ax = plt.gca()      # Add vertical lines at each x tick     for x in ax.get_xticks():         ax.axvline(x=x, color=\"gray\", linestyle=\"--\", lw=0.5)      plt.savefig(         \"docs/images/stripplot-rag-tasks.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/stripplot-rag-tasks.pdf\",         bbox_inches=\"tight\",     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_extraction_tasks():\n    \"\"\"Load raw result file for sourcedata_info_extraction; aggregate based on the\n    subtask name and calculate mean accuracy for each model. Plot a stripplot\n    of the mean accuracy across models, coloured by subtask.\n    \"\"\"\n    sourcedata_info_extraction = pd.read_csv(\n        \"benchmark/results/sourcedata_info_extraction.csv\",\n    )\n    # split subtask at colon and use second element\n    sourcedata_info_extraction[\"subtask\"] = sourcedata_info_extraction[\"subtask\"].apply(lambda x: x.split(\":\")[1])\n    sourcedata_info_extraction[\"score_possible\"] = sourcedata_info_extraction[\"score\"].apply(\n        lambda x: float(x.split(\"/\")[1]),\n    )\n    sourcedata_info_extraction[\"scores\"] = sourcedata_info_extraction[\"score\"].apply(lambda x: x.split(\"/\")[0])\n    sourcedata_info_extraction[\"score_achieved\"] = sourcedata_info_extraction[\"scores\"].apply(\n        lambda x: np.mean(float(x.split(\";\")[0])) if \";\" in x else float(x),\n    )\n    sourcedata_info_extraction[\"score_sd\"] = sourcedata_info_extraction[\"scores\"].apply(\n        lambda x: np.std(float(x.split(\";\")[0])) if \";\" in x else 0,\n    )\n    aggregated_scores = sourcedata_info_extraction.groupby(\n        [\"model_name\", \"subtask\"],\n    ).agg(\n        {\n            \"score_possible\": \"sum\",\n            \"score_achieved\": \"sum\",\n            \"score_sd\": \"first\",\n            \"iterations\": \"first\",\n        },\n    )\n\n    aggregated_scores[\"Accuracy\"] = aggregated_scores.apply(\n        lambda row: (row[\"score_achieved\"] / row[\"score_possible\"] if row[\"score_possible\"] != 0 else 0),\n        axis=1,\n    )\n\n    aggregated_scores[\"Full model name\"] = aggregated_scores.index.get_level_values(\"model_name\")\n    aggregated_scores[\"Subtask\"] = aggregated_scores.index.get_level_values(\n        \"subtask\",\n    )\n    aggregated_scores[\"Score achieved\"] = aggregated_scores[\"score_achieved\"]\n    aggregated_scores[\"Score possible\"] = aggregated_scores[\"score_possible\"]\n    aggregated_scores[\"Score SD\"] = aggregated_scores[\"score_sd\"]\n    aggregated_scores[\"Iterations\"] = aggregated_scores[\"iterations\"]\n    new_order = [\n        \"Full model name\",\n        \"Subtask\",\n        \"Score achieved\",\n        \"Score possible\",\n        \"Score SD\",\n        \"Accuracy\",\n        \"Iterations\",\n    ]\n    results = aggregated_scores[new_order]\n    results = results.sort_values(by=\"Accuracy\", ascending=False)\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    sns.stripplot(\n        x=\"Subtask\",\n        y=\"Accuracy\",\n        hue=\"Full model name\",\n        data=results,\n    )\n\n    plt.title(\"Strip plot across models, per subtask, coloured by model name\")\n    plt.ylim(-0.1, 1.1)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n    plt.savefig(\n        \"docs/images/stripplot-extraction-tasks.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n</pre> def plot_extraction_tasks():     \"\"\"Load raw result file for sourcedata_info_extraction; aggregate based on the     subtask name and calculate mean accuracy for each model. Plot a stripplot     of the mean accuracy across models, coloured by subtask.     \"\"\"     sourcedata_info_extraction = pd.read_csv(         \"benchmark/results/sourcedata_info_extraction.csv\",     )     # split subtask at colon and use second element     sourcedata_info_extraction[\"subtask\"] = sourcedata_info_extraction[\"subtask\"].apply(lambda x: x.split(\":\")[1])     sourcedata_info_extraction[\"score_possible\"] = sourcedata_info_extraction[\"score\"].apply(         lambda x: float(x.split(\"/\")[1]),     )     sourcedata_info_extraction[\"scores\"] = sourcedata_info_extraction[\"score\"].apply(lambda x: x.split(\"/\")[0])     sourcedata_info_extraction[\"score_achieved\"] = sourcedata_info_extraction[\"scores\"].apply(         lambda x: np.mean(float(x.split(\";\")[0])) if \";\" in x else float(x),     )     sourcedata_info_extraction[\"score_sd\"] = sourcedata_info_extraction[\"scores\"].apply(         lambda x: np.std(float(x.split(\";\")[0])) if \";\" in x else 0,     )     aggregated_scores = sourcedata_info_extraction.groupby(         [\"model_name\", \"subtask\"],     ).agg(         {             \"score_possible\": \"sum\",             \"score_achieved\": \"sum\",             \"score_sd\": \"first\",             \"iterations\": \"first\",         },     )      aggregated_scores[\"Accuracy\"] = aggregated_scores.apply(         lambda row: (row[\"score_achieved\"] / row[\"score_possible\"] if row[\"score_possible\"] != 0 else 0),         axis=1,     )      aggregated_scores[\"Full model name\"] = aggregated_scores.index.get_level_values(\"model_name\")     aggregated_scores[\"Subtask\"] = aggregated_scores.index.get_level_values(         \"subtask\",     )     aggregated_scores[\"Score achieved\"] = aggregated_scores[\"score_achieved\"]     aggregated_scores[\"Score possible\"] = aggregated_scores[\"score_possible\"]     aggregated_scores[\"Score SD\"] = aggregated_scores[\"score_sd\"]     aggregated_scores[\"Iterations\"] = aggregated_scores[\"iterations\"]     new_order = [         \"Full model name\",         \"Subtask\",         \"Score achieved\",         \"Score possible\",         \"Score SD\",         \"Accuracy\",         \"Iterations\",     ]     results = aggregated_scores[new_order]     results = results.sort_values(by=\"Accuracy\", ascending=False)      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     sns.stripplot(         x=\"Subtask\",         y=\"Accuracy\",         hue=\"Full model name\",         data=results,     )      plt.title(\"Strip plot across models, per subtask, coloured by model name\")     plt.ylim(-0.1, 1.1)     plt.xticks(rotation=45, ha=\"right\")     plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")     plt.savefig(         \"docs/images/stripplot-extraction-tasks.png\",         bbox_inches=\"tight\",         dpi=300,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_medical_exam():\n    \"\"\"Load raw result for medical_exam; aggregate based on the language and\n    calculate mean accuracy for each model. Plot a stripplot of the mean\n    accuracy across models, coloured by language.\n    \"\"\"\n    medical_exam = pd.read_csv(\"benchmark/results/medical_exam.csv\")\n\n    medical_exam[\"score_possible\"] = medical_exam[\"score\"].apply(\n        lambda x: float(x.split(\"/\")[1]),\n    )\n    medical_exam[\"scores\"] = medical_exam[\"score\"].apply(\n        lambda x: x.split(\"/\")[0],\n    )\n    medical_exam[\"score_achieved\"] = medical_exam[\"scores\"].apply(\n        lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),\n    )\n    medical_exam[\"accuracy\"] = medical_exam[\"score_achieved\"] / medical_exam[\"score_possible\"]\n    medical_exam[\"score_sd\"] = medical_exam[\"scores\"].apply(\n        lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),\n    )\n    medical_exam[\"task\"] = medical_exam[\"subtask\"].apply(\n        lambda x: x.split(\":\")[0],\n    )\n    medical_exam[\"domain\"] = medical_exam[\"subtask\"].apply(\n        lambda x: x.split(\":\")[1],\n    )\n    medical_exam[\"language\"] = medical_exam[\"subtask\"].apply(\n        lambda x: x.split(\":\")[2],\n    )\n\n    # processing: remove \"short_words\" task, not informative\n    medical_exam = medical_exam[medical_exam[\"task\"] != \"short_words\"]\n\n    # plot language comparison\n    aggregated_scores_language = medical_exam.groupby(\n        [\"model_name\", \"language\"],\n    ).agg(\n        {\n            \"accuracy\": \"mean\",\n            \"score_sd\": \"mean\",\n        },\n    )\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(\n        x=\"language\",\n        y=\"accuracy\",\n        data=aggregated_scores_language,\n    )\n\n    plt.savefig(\n        \"docs/images/boxplot-medical-exam-language.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n\n    # plot language comparison per domain\n    aggregated_scores_language_domain = medical_exam.groupby(\n        [\"model_name\", \"language\", \"domain\"],\n    ).agg(\n        {\n            \"accuracy\": \"mean\",\n            \"score_sd\": \"mean\",\n        },\n    )\n    # calculate mean accuracy per language and domain\n    mean_accuracy = aggregated_scores_language_domain.groupby(\n        [\"language\", \"domain\"],\n    )[\"accuracy\"].mean()\n    # sort domains by mean accuracy\n    sorted_domains = mean_accuracy.sort_values(\n        ascending=False,\n    ).index.get_level_values(\"domain\")\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    plt.xticks(rotation=45, ha=\"right\")\n    sns.boxplot(\n        x=\"domain\",\n        y=\"accuracy\",\n        hue=\"language\",\n        data=aggregated_scores_language_domain,\n        order=sorted_domains,\n    )\n\n    plt.savefig(\n        \"docs/images/boxplot-medical-exam-language-domain.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n\n    # plot task comparison\n    aggregated_scores_task = medical_exam.groupby([\"model_name\", \"task\"]).agg(\n        {\n            \"accuracy\": \"mean\",\n            \"score_sd\": \"mean\",\n        },\n    )\n    # calculate mean accuracy per task\n    mean_accuracy = aggregated_scores_task.groupby(\"task\")[\"accuracy\"].mean()\n    # sort tasks by mean accuracy\n    sorted_tasks = mean_accuracy.sort_values(ascending=False).index\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    plt.xticks(rotation=45, ha=\"right\")\n    sns.boxplot(\n        x=\"task\",\n        y=\"accuracy\",\n        data=aggregated_scores_task,\n        order=sorted_tasks,\n    )\n\n    plt.savefig(\n        \"docs/images/boxplot-medical-exam-task.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n\n    # plot domain comparison\n    aggregated_scores_domain = medical_exam.groupby(\n        [\"model_name\", \"domain\"],\n    ).agg(\n        {\n            \"accuracy\": \"mean\",\n            \"score_sd\": \"mean\",\n        },\n    )\n    # calculate mean accuracy per domain\n    mean_accuracy = aggregated_scores_domain.groupby(\"domain\")[\"accuracy\"].mean()\n    # sort domains by mean accuracy\n    sorted_domains = mean_accuracy.sort_values(ascending=False).index\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    plt.xticks(rotation=45, ha=\"right\")\n    sns.boxplot(\n        x=\"domain\",\n        y=\"accuracy\",\n        data=aggregated_scores_domain,\n        order=sorted_domains,\n    )\n\n    plt.savefig(\n        \"docs/images/boxplot-medical-exam-domain.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n</pre> def plot_medical_exam():     \"\"\"Load raw result for medical_exam; aggregate based on the language and     calculate mean accuracy for each model. Plot a stripplot of the mean     accuracy across models, coloured by language.     \"\"\"     medical_exam = pd.read_csv(\"benchmark/results/medical_exam.csv\")      medical_exam[\"score_possible\"] = medical_exam[\"score\"].apply(         lambda x: float(x.split(\"/\")[1]),     )     medical_exam[\"scores\"] = medical_exam[\"score\"].apply(         lambda x: x.split(\"/\")[0],     )     medical_exam[\"score_achieved\"] = medical_exam[\"scores\"].apply(         lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),     )     medical_exam[\"accuracy\"] = medical_exam[\"score_achieved\"] / medical_exam[\"score_possible\"]     medical_exam[\"score_sd\"] = medical_exam[\"scores\"].apply(         lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),     )     medical_exam[\"task\"] = medical_exam[\"subtask\"].apply(         lambda x: x.split(\":\")[0],     )     medical_exam[\"domain\"] = medical_exam[\"subtask\"].apply(         lambda x: x.split(\":\")[1],     )     medical_exam[\"language\"] = medical_exam[\"subtask\"].apply(         lambda x: x.split(\":\")[2],     )      # processing: remove \"short_words\" task, not informative     medical_exam = medical_exam[medical_exam[\"task\"] != \"short_words\"]      # plot language comparison     aggregated_scores_language = medical_exam.groupby(         [\"model_name\", \"language\"],     ).agg(         {             \"accuracy\": \"mean\",             \"score_sd\": \"mean\",         },     )     sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     sns.boxplot(         x=\"language\",         y=\"accuracy\",         data=aggregated_scores_language,     )      plt.savefig(         \"docs/images/boxplot-medical-exam-language.png\",         bbox_inches=\"tight\",         dpi=300,     )      # plot language comparison per domain     aggregated_scores_language_domain = medical_exam.groupby(         [\"model_name\", \"language\", \"domain\"],     ).agg(         {             \"accuracy\": \"mean\",             \"score_sd\": \"mean\",         },     )     # calculate mean accuracy per language and domain     mean_accuracy = aggregated_scores_language_domain.groupby(         [\"language\", \"domain\"],     )[\"accuracy\"].mean()     # sort domains by mean accuracy     sorted_domains = mean_accuracy.sort_values(         ascending=False,     ).index.get_level_values(\"domain\")      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     plt.xticks(rotation=45, ha=\"right\")     sns.boxplot(         x=\"domain\",         y=\"accuracy\",         hue=\"language\",         data=aggregated_scores_language_domain,         order=sorted_domains,     )      plt.savefig(         \"docs/images/boxplot-medical-exam-language-domain.png\",         bbox_inches=\"tight\",         dpi=300,     )      # plot task comparison     aggregated_scores_task = medical_exam.groupby([\"model_name\", \"task\"]).agg(         {             \"accuracy\": \"mean\",             \"score_sd\": \"mean\",         },     )     # calculate mean accuracy per task     mean_accuracy = aggregated_scores_task.groupby(\"task\")[\"accuracy\"].mean()     # sort tasks by mean accuracy     sorted_tasks = mean_accuracy.sort_values(ascending=False).index      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     plt.xticks(rotation=45, ha=\"right\")     sns.boxplot(         x=\"task\",         y=\"accuracy\",         data=aggregated_scores_task,         order=sorted_tasks,     )      plt.savefig(         \"docs/images/boxplot-medical-exam-task.png\",         bbox_inches=\"tight\",         dpi=300,     )      # plot domain comparison     aggregated_scores_domain = medical_exam.groupby(         [\"model_name\", \"domain\"],     ).agg(         {             \"accuracy\": \"mean\",             \"score_sd\": \"mean\",         },     )     # calculate mean accuracy per domain     mean_accuracy = aggregated_scores_domain.groupby(\"domain\")[\"accuracy\"].mean()     # sort domains by mean accuracy     sorted_domains = mean_accuracy.sort_values(ascending=False).index      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     plt.xticks(rotation=45, ha=\"right\")     sns.boxplot(         x=\"domain\",         y=\"accuracy\",         data=aggregated_scores_domain,         order=sorted_domains,     )      plt.savefig(         \"docs/images/boxplot-medical-exam-domain.png\",         bbox_inches=\"tight\",         dpi=300,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_comparison_naive_biochatter(overview):\n    overview_melted = melt_and_process(overview)\n\n    # select tasks naive_query_generation_using_schema and query_generation\n    overview_melted = overview_melted[\n        overview_melted[\"Task\"].isin(\n            [\"naive_query_generation_using_schema\", \"query_generation\"],\n        )\n    ]\n\n    # print number of rows of each task\n    print(overview_melted[\"Task\"].value_counts())\n\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(\n        x=\"Task\",\n        y=\"Accuracy\",\n        hue=\"Task\",\n        data=overview_melted,\n    )\n    plt.ylim(0, 1)\n    plt.xlabel(None)\n    plt.xticks(\n        ticks=range(len(overview_melted[\"Task\"].unique())),\n        labels=[\"BioChatter\", \"Naive LLM (using full YAML schema)\"],\n    )\n    plt.savefig(\n        \"docs/images/boxplot-naive-vs-biochatter.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/boxplot-naive-vs-biochatter.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n    # plot scatter plot\n    plt.figure(figsize=(6, 4))\n    sns.stripplot(\n        x=\"Task\",\n        y=\"Accuracy\",\n        data=overview_melted,\n        jitter=0.2,\n        alpha=0.8,\n    )\n    plt.ylim(0, 1)\n    plt.xlabel(None)\n    plt.xticks(\n        ticks=range(len(overview_melted[\"Task\"].unique())),\n        labels=[\"BioChatter\", \"Naive LLM (using full YAML schema)\"],\n    )\n    plt.savefig(\n        \"docs/images/scatter-naive-vs-biochatter.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/scatter-naive-vs-biochatter.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n    # plit violin plot\n    plt.figure(figsize=(6, 4))\n    sns.violinplot(\n        x=\"Task\",\n        y=\"Accuracy\",\n        data=overview_melted,\n    )\n    plt.ylim(0, 1)\n    plt.xlabel(None)\n    plt.xticks(\n        ticks=range(len(overview_melted[\"Task\"].unique())),\n        labels=[\"BioChatter\", \"Naive LLM (using full YAML schema)\"],\n    )\n    plt.savefig(\n        \"docs/images/violin-naive-vs-biochatter.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/violin-naive-vs-biochatter.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n</pre> def plot_comparison_naive_biochatter(overview):     overview_melted = melt_and_process(overview)      # select tasks naive_query_generation_using_schema and query_generation     overview_melted = overview_melted[         overview_melted[\"Task\"].isin(             [\"naive_query_generation_using_schema\", \"query_generation\"],         )     ]      # print number of rows of each task     print(overview_melted[\"Task\"].value_counts())      sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))     sns.boxplot(         x=\"Task\",         y=\"Accuracy\",         hue=\"Task\",         data=overview_melted,     )     plt.ylim(0, 1)     plt.xlabel(None)     plt.xticks(         ticks=range(len(overview_melted[\"Task\"].unique())),         labels=[\"BioChatter\", \"Naive LLM (using full YAML schema)\"],     )     plt.savefig(         \"docs/images/boxplot-naive-vs-biochatter.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/boxplot-naive-vs-biochatter.pdf\",         bbox_inches=\"tight\",     )     plt.close()      # plot scatter plot     plt.figure(figsize=(6, 4))     sns.stripplot(         x=\"Task\",         y=\"Accuracy\",         data=overview_melted,         jitter=0.2,         alpha=0.8,     )     plt.ylim(0, 1)     plt.xlabel(None)     plt.xticks(         ticks=range(len(overview_melted[\"Task\"].unique())),         labels=[\"BioChatter\", \"Naive LLM (using full YAML schema)\"],     )     plt.savefig(         \"docs/images/scatter-naive-vs-biochatter.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/scatter-naive-vs-biochatter.pdf\",         bbox_inches=\"tight\",     )     plt.close()      # plit violin plot     plt.figure(figsize=(6, 4))     sns.violinplot(         x=\"Task\",         y=\"Accuracy\",         data=overview_melted,     )     plt.ylim(0, 1)     plt.xlabel(None)     plt.xticks(         ticks=range(len(overview_melted[\"Task\"].unique())),         labels=[\"BioChatter\", \"Naive LLM (using full YAML schema)\"],     )     plt.savefig(         \"docs/images/violin-naive-vs-biochatter.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/violin-naive-vs-biochatter.pdf\",         bbox_inches=\"tight\",     )     plt.close() In\u00a0[\u00a0]: Copied! <pre>def melt_and_process(overview: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Melt the overview table and process it for plotting.\"\"\"\n    overview_melted = overview.melt(\n        id_vars=[\n            \"Full model name\",\n            \"Model name\",\n            \"Size\",\n            \"Version\",\n            \"Quantisation\",\n            \"Mean Accuracy\",\n            \"Median Accuracy\",\n            \"SD\",\n        ],\n        var_name=\"Task\",\n        value_name=\"Accuracy\",\n    )\n    # unify quantisation names: 2-bit, 3-bit, etc\n    digit_pattern = r\"\\d+\"\n    overview_melted[\"Quantisation\"] = overview_melted[\"Quantisation\"].apply(\n        lambda x: f\"{re.findall(digit_pattern, x)[0]}-bit\" if x else \"None\",\n    )\n    # set quantisation of gpt models to None\n    overview_melted[\"Quantisation\"] = overview_melted.apply(\n        lambda row: (\n            \"&gt;= 16-bit*\"\n            if \"gpt-3.5-turbo\" in row[\"Model name\"] or \"gpt-4\" in row[\"Model name\"] or \"claude\" in row[\"Model name\"]\n            else row[\"Quantisation\"]\n        ),\n        axis=1,\n    )\n\n    return overview_melted\n</pre> def melt_and_process(overview: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"Melt the overview table and process it for plotting.\"\"\"     overview_melted = overview.melt(         id_vars=[             \"Full model name\",             \"Model name\",             \"Size\",             \"Version\",             \"Quantisation\",             \"Mean Accuracy\",             \"Median Accuracy\",             \"SD\",         ],         var_name=\"Task\",         value_name=\"Accuracy\",     )     # unify quantisation names: 2-bit, 3-bit, etc     digit_pattern = r\"\\d+\"     overview_melted[\"Quantisation\"] = overview_melted[\"Quantisation\"].apply(         lambda x: f\"{re.findall(digit_pattern, x)[0]}-bit\" if x else \"None\",     )     # set quantisation of gpt models to None     overview_melted[\"Quantisation\"] = overview_melted.apply(         lambda row: (             \"&gt;= 16-bit*\"             if \"gpt-3.5-turbo\" in row[\"Model name\"] or \"gpt-4\" in row[\"Model name\"] or \"claude\" in row[\"Model name\"]             else row[\"Quantisation\"]         ),         axis=1,     )      return overview_melted"},{"location":"BioChatter/scripts/_preprocess/","title":"preprocess","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def preprocess_results_for_frontend(\n    raw_results: pd.DataFrame,\n    path: str,\n    file_name: str,\n) -&gt; None:\n    \"\"\"Preprocesses the results for the frontend and store them on disk.\n\n    Args:\n    ----\n        raw_results (pd.DataFrame): The raw results.\n        path (str): The path to the result files.\n        file_name (str): The file name of the result file.\n\n    \"\"\"\n    raw_results[\"score_possible\"] = raw_results.apply(\n        lambda x: float(x[\"score\"].split(\"/\")[1]) * x[\"iterations\"],\n        axis=1,\n    )\n    raw_results[\"scores\"] = raw_results[\"score\"].apply(\n        lambda x: x.split(\"/\")[0],\n    )\n    raw_results[\"score_achieved\"] = raw_results[\"scores\"].apply(\n        lambda x: (np.sum([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),\n    )\n    # multiply score_achieved by iterations if no semicolon in scores\n    # TODO remove once all benchmarks are in new format\n    raw_results[\"score_achieved\"] = raw_results.apply(\n        lambda x: (x[\"score_achieved\"] * x[\"iterations\"] if \";\" not in x[\"scores\"] else x[\"score_achieved\"]),\n        axis=1,\n    )\n    raw_results[\"score_sd\"] = raw_results[\"scores\"].apply(\n        lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),\n    )\n    aggregated_scores = raw_results.groupby([\"model_name\"]).agg(\n        {\n            \"score_possible\": \"sum\",\n            \"score_achieved\": \"sum\",\n            \"score_sd\": \"sum\",\n            \"iterations\": \"first\",\n        },\n    )\n\n    aggregated_scores[\"Accuracy\"] = aggregated_scores.apply(\n        lambda row: (row[\"score_achieved\"] / row[\"score_possible\"] if row[\"score_possible\"] != 0 else 0),\n        axis=1,\n    )\n\n    aggregated_scores[\"Full model name\"] = aggregated_scores.index.get_level_values(\"model_name\")\n    aggregated_scores[\"Score achieved\"] = aggregated_scores[\"score_achieved\"]\n    aggregated_scores[\"Score possible\"] = aggregated_scores[\"score_possible\"]\n    aggregated_scores[\"Score SD\"] = aggregated_scores[\"score_sd\"]\n    aggregated_scores[\"Iterations\"] = aggregated_scores[\"iterations\"]\n    new_order = [\n        \"Full model name\",\n        \"Score achieved\",\n        \"Score possible\",\n        \"Score SD\",\n        \"Accuracy\",\n        \"Iterations\",\n    ]\n    results = aggregated_scores[new_order]\n    results = results.sort_values(by=\"Accuracy\", ascending=False)\n    results.to_csv(\n        f\"{path}processed/{file_name}\",\n        index=False,\n    )\n\n    if file_name == \"sourcedata_info_extraction.csv\":\n        write_individual_extraction_task_results(raw_results)\n</pre> def preprocess_results_for_frontend(     raw_results: pd.DataFrame,     path: str,     file_name: str, ) -&gt; None:     \"\"\"Preprocesses the results for the frontend and store them on disk.      Args:     ----         raw_results (pd.DataFrame): The raw results.         path (str): The path to the result files.         file_name (str): The file name of the result file.      \"\"\"     raw_results[\"score_possible\"] = raw_results.apply(         lambda x: float(x[\"score\"].split(\"/\")[1]) * x[\"iterations\"],         axis=1,     )     raw_results[\"scores\"] = raw_results[\"score\"].apply(         lambda x: x.split(\"/\")[0],     )     raw_results[\"score_achieved\"] = raw_results[\"scores\"].apply(         lambda x: (np.sum([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),     )     # multiply score_achieved by iterations if no semicolon in scores     # TODO remove once all benchmarks are in new format     raw_results[\"score_achieved\"] = raw_results.apply(         lambda x: (x[\"score_achieved\"] * x[\"iterations\"] if \";\" not in x[\"scores\"] else x[\"score_achieved\"]),         axis=1,     )     raw_results[\"score_sd\"] = raw_results[\"scores\"].apply(         lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),     )     aggregated_scores = raw_results.groupby([\"model_name\"]).agg(         {             \"score_possible\": \"sum\",             \"score_achieved\": \"sum\",             \"score_sd\": \"sum\",             \"iterations\": \"first\",         },     )      aggregated_scores[\"Accuracy\"] = aggregated_scores.apply(         lambda row: (row[\"score_achieved\"] / row[\"score_possible\"] if row[\"score_possible\"] != 0 else 0),         axis=1,     )      aggregated_scores[\"Full model name\"] = aggregated_scores.index.get_level_values(\"model_name\")     aggregated_scores[\"Score achieved\"] = aggregated_scores[\"score_achieved\"]     aggregated_scores[\"Score possible\"] = aggregated_scores[\"score_possible\"]     aggregated_scores[\"Score SD\"] = aggregated_scores[\"score_sd\"]     aggregated_scores[\"Iterations\"] = aggregated_scores[\"iterations\"]     new_order = [         \"Full model name\",         \"Score achieved\",         \"Score possible\",         \"Score SD\",         \"Accuracy\",         \"Iterations\",     ]     results = aggregated_scores[new_order]     results = results.sort_values(by=\"Accuracy\", ascending=False)     results.to_csv(         f\"{path}processed/{file_name}\",         index=False,     )      if file_name == \"sourcedata_info_extraction.csv\":         write_individual_extraction_task_results(raw_results) In\u00a0[\u00a0]: Copied! <pre>def write_individual_extraction_task_results(raw_results: pd.DataFrame) -&gt; None:\n    \"\"\"Write one csv file to disk per subtask.\n\n    Writes individual files for sourcedata_info_extraction results in the\n    same format as the other results files.\n    \"\"\"\n    raw_results[\"subtask\"] = raw_results[\"subtask\"].apply(\n        lambda x: x.split(\":\")[1],\n    )\n    raw_results[\"score_possible\"] = raw_results.apply(\n        lambda x: float(x[\"score\"].split(\"/\")[1]) * x[\"iterations\"],\n        axis=1,\n    )\n    raw_results[\"scores\"] = raw_results[\"score\"].apply(\n        lambda x: x.split(\"/\")[0],\n    )\n    raw_results[\"score_achieved\"] = raw_results[\"scores\"].apply(\n        lambda x: (np.sum([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),\n    )\n    raw_results[\"score_sd\"] = raw_results[\"scores\"].apply(\n        lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),\n    )\n    aggregated_scores = raw_results.groupby([\"model_name\", \"subtask\"]).agg(\n        {\n            \"score_possible\": \"sum\",\n            \"score_achieved\": \"sum\",\n            \"score_sd\": \"mean\",\n            \"iterations\": \"first\",\n        },\n    )\n\n    aggregated_scores[\"Accuracy\"] = aggregated_scores.apply(\n        lambda row: (row[\"score_achieved\"] / row[\"score_possible\"] if row[\"score_possible\"] != 0 else 0),\n        axis=1,\n    )\n\n    aggregated_scores[\"Full model name\"] = aggregated_scores.index.get_level_values(\"model_name\")\n    aggregated_scores[\"Subtask\"] = aggregated_scores.index.get_level_values(\n        \"subtask\",\n    )\n    aggregated_scores[\"Score achieved\"] = aggregated_scores[\"score_achieved\"]\n    aggregated_scores[\"Score possible\"] = aggregated_scores[\"score_possible\"]\n    aggregated_scores[\"Score SD\"] = aggregated_scores[\"score_sd\"]\n    aggregated_scores[\"Iterations\"] = aggregated_scores[\"iterations\"]\n    new_order = [\n        \"Full model name\",\n        \"Subtask\",\n        \"Score achieved\",\n        \"Score possible\",\n        \"Score SD\",\n        \"Accuracy\",\n        \"Iterations\",\n    ]\n    results = aggregated_scores[new_order]\n    results = results.sort_values(by=\"Accuracy\", ascending=False)\n    for subtask in results[\"Subtask\"].unique():\n        results_subtask = results[results[\"Subtask\"] == subtask]\n        results_subtask.to_csv(\n            f\"benchmark/results/processed/extraction_{subtask}.csv\",\n            index=False,\n        )\n</pre> def write_individual_extraction_task_results(raw_results: pd.DataFrame) -&gt; None:     \"\"\"Write one csv file to disk per subtask.      Writes individual files for sourcedata_info_extraction results in the     same format as the other results files.     \"\"\"     raw_results[\"subtask\"] = raw_results[\"subtask\"].apply(         lambda x: x.split(\":\")[1],     )     raw_results[\"score_possible\"] = raw_results.apply(         lambda x: float(x[\"score\"].split(\"/\")[1]) * x[\"iterations\"],         axis=1,     )     raw_results[\"scores\"] = raw_results[\"score\"].apply(         lambda x: x.split(\"/\")[0],     )     raw_results[\"score_achieved\"] = raw_results[\"scores\"].apply(         lambda x: (np.sum([float(score) for score in x.split(\";\")]) if \";\" in x else float(x)),     )     raw_results[\"score_sd\"] = raw_results[\"scores\"].apply(         lambda x: (np.std([float(score) for score in x.split(\";\")], ddof=1) if \";\" in x else 0),     )     aggregated_scores = raw_results.groupby([\"model_name\", \"subtask\"]).agg(         {             \"score_possible\": \"sum\",             \"score_achieved\": \"sum\",             \"score_sd\": \"mean\",             \"iterations\": \"first\",         },     )      aggregated_scores[\"Accuracy\"] = aggregated_scores.apply(         lambda row: (row[\"score_achieved\"] / row[\"score_possible\"] if row[\"score_possible\"] != 0 else 0),         axis=1,     )      aggregated_scores[\"Full model name\"] = aggregated_scores.index.get_level_values(\"model_name\")     aggregated_scores[\"Subtask\"] = aggregated_scores.index.get_level_values(         \"subtask\",     )     aggregated_scores[\"Score achieved\"] = aggregated_scores[\"score_achieved\"]     aggregated_scores[\"Score possible\"] = aggregated_scores[\"score_possible\"]     aggregated_scores[\"Score SD\"] = aggregated_scores[\"score_sd\"]     aggregated_scores[\"Iterations\"] = aggregated_scores[\"iterations\"]     new_order = [         \"Full model name\",         \"Subtask\",         \"Score achieved\",         \"Score possible\",         \"Score SD\",         \"Accuracy\",         \"Iterations\",     ]     results = aggregated_scores[new_order]     results = results.sort_values(by=\"Accuracy\", ascending=False)     for subtask in results[\"Subtask\"].unique():         results_subtask = results[results[\"Subtask\"] == subtask]         results_subtask.to_csv(             f\"benchmark/results/processed/extraction_{subtask}.csv\",             index=False,         ) In\u00a0[\u00a0]: Copied! <pre>def create_overview_table(result_files_path: str, result_file_names: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Create an overview table of benchmark results.\n\n    Creates a table for visualisation on the website with y-axis = models and\n    x-axis = tasks.\n\n    Args:\n    ----\n        result_files_path (str): The path to the result files.\n        result_file_names (List[str]): The names of the result files.\n\n    \"\"\"\n    subtask_results = []\n    for file in result_file_names:\n        subtask_result = pd.read_csv(f\"{result_files_path}processed/{file}\")\n        file_name_without_extension = os.path.splitext(file)[0]\n        subtask_result[file_name_without_extension] = subtask_result[\"Accuracy\"]\n        subtask_result.set_index(\"Full model name\", inplace=True)\n        subtask_result = subtask_result[file_name_without_extension]\n        subtask_results.append(subtask_result)\n    overview = pd.concat(subtask_results, axis=1)\n    overview[\"Mean Accuracy\"] = overview.mean(axis=1)\n    overview[\"Median Accuracy\"] = overview.median(axis=1)\n    overview[\"SD\"] = overview.std(axis=1)\n    overview = overview.sort_values(by=\"Median Accuracy\", ascending=False)\n    # split \"Full model name\" at : to get Model name, size, version, and quantisation\n    overview.to_csv(\n        f\"{result_files_path}processed/overview.csv\",\n        index=True,\n    )\n\n    overview_per_quantisation = overview\n    overview_per_quantisation[\"Full model name\"] = overview_per_quantisation.index\n    overview_per_quantisation[[\"Model name\", \"Size\", \"Version\", \"Quantisation\"]] = overview_per_quantisation[\n        \"Full model name\"\n    ].str.split(\":\", expand=True)\n    # convert underscores in Size to commas\n    overview_per_quantisation[\"Size\"] = overview_per_quantisation[\"Size\"].str.replace(\"_\", \",\")\n    # add size 175 for gpt-3.5-turbo and Unknown for gpt-4\n    overview_per_quantisation[\"Size\"] = overview_per_quantisation.apply(\n        lambda row: (\"175\" if \"gpt-3.5-turbo\" in row[\"Model name\"] else row[\"Size\"]),\n        axis=1,\n    )\n    overview_per_quantisation[\"Size\"] = overview_per_quantisation.apply(\n        lambda row: (\"Unknown\" if \"gpt-4\" in row[\"Model name\"] or \"claude\" in row[\"Model name\"] else row[\"Size\"]),\n        axis=1,\n    )\n    overview_per_quantisation = overview_per_quantisation[\n        [\n            \"Model name\",\n            \"Size\",\n            \"Version\",\n            \"Quantisation\",\n            \"Median Accuracy\",\n            \"SD\",\n        ]\n    ]\n    # round mean and sd to 2 decimal places\n    overview_per_quantisation.loc[:, \"Median Accuracy\"] = overview_per_quantisation[\"Median Accuracy\"].round(2)\n    overview_per_quantisation.loc[:, \"SD\"] = overview_per_quantisation[\"SD\"].round(2)\n    overview_per_quantisation.to_csv(\n        f\"{result_files_path}processed/overview-quantisation.csv\",\n        index=False,\n    )\n\n    # group by model name and size, aggregate different quantisations\n    # keep models that do not have sizes\n    overview_per_size = overview_per_quantisation.groupby(\n        [\"Model name\", \"Size\"],\n    ).agg(\n        {\n            \"Median Accuracy\": \"median\",\n            \"SD\": \"mean\",\n        },\n    )\n    # round mean and SD to 2 decimal places\n    overview_per_size[\"Median Accuracy\"] = overview_per_size[\"Median Accuracy\"].round(2)\n    overview_per_size[\"SD\"] = overview_per_size[\"SD\"].round(2)\n    # sort by mean, descending\n    overview_per_size = overview_per_size.sort_values(\n        by=\"Median Accuracy\",\n        ascending=False,\n    )\n    overview_per_size.to_csv(\n        f\"{result_files_path}processed/overview-model.csv\",\n        index=True,\n    )\n\n    return overview\n</pre> def create_overview_table(result_files_path: str, result_file_names: list[str]) -&gt; pd.DataFrame:     \"\"\"Create an overview table of benchmark results.      Creates a table for visualisation on the website with y-axis = models and     x-axis = tasks.      Args:     ----         result_files_path (str): The path to the result files.         result_file_names (List[str]): The names of the result files.      \"\"\"     subtask_results = []     for file in result_file_names:         subtask_result = pd.read_csv(f\"{result_files_path}processed/{file}\")         file_name_without_extension = os.path.splitext(file)[0]         subtask_result[file_name_without_extension] = subtask_result[\"Accuracy\"]         subtask_result.set_index(\"Full model name\", inplace=True)         subtask_result = subtask_result[file_name_without_extension]         subtask_results.append(subtask_result)     overview = pd.concat(subtask_results, axis=1)     overview[\"Mean Accuracy\"] = overview.mean(axis=1)     overview[\"Median Accuracy\"] = overview.median(axis=1)     overview[\"SD\"] = overview.std(axis=1)     overview = overview.sort_values(by=\"Median Accuracy\", ascending=False)     # split \"Full model name\" at : to get Model name, size, version, and quantisation     overview.to_csv(         f\"{result_files_path}processed/overview.csv\",         index=True,     )      overview_per_quantisation = overview     overview_per_quantisation[\"Full model name\"] = overview_per_quantisation.index     overview_per_quantisation[[\"Model name\", \"Size\", \"Version\", \"Quantisation\"]] = overview_per_quantisation[         \"Full model name\"     ].str.split(\":\", expand=True)     # convert underscores in Size to commas     overview_per_quantisation[\"Size\"] = overview_per_quantisation[\"Size\"].str.replace(\"_\", \",\")     # add size 175 for gpt-3.5-turbo and Unknown for gpt-4     overview_per_quantisation[\"Size\"] = overview_per_quantisation.apply(         lambda row: (\"175\" if \"gpt-3.5-turbo\" in row[\"Model name\"] else row[\"Size\"]),         axis=1,     )     overview_per_quantisation[\"Size\"] = overview_per_quantisation.apply(         lambda row: (\"Unknown\" if \"gpt-4\" in row[\"Model name\"] or \"claude\" in row[\"Model name\"] else row[\"Size\"]),         axis=1,     )     overview_per_quantisation = overview_per_quantisation[         [             \"Model name\",             \"Size\",             \"Version\",             \"Quantisation\",             \"Median Accuracy\",             \"SD\",         ]     ]     # round mean and sd to 2 decimal places     overview_per_quantisation.loc[:, \"Median Accuracy\"] = overview_per_quantisation[\"Median Accuracy\"].round(2)     overview_per_quantisation.loc[:, \"SD\"] = overview_per_quantisation[\"SD\"].round(2)     overview_per_quantisation.to_csv(         f\"{result_files_path}processed/overview-quantisation.csv\",         index=False,     )      # group by model name and size, aggregate different quantisations     # keep models that do not have sizes     overview_per_size = overview_per_quantisation.groupby(         [\"Model name\", \"Size\"],     ).agg(         {             \"Median Accuracy\": \"median\",             \"SD\": \"mean\",         },     )     # round mean and SD to 2 decimal places     overview_per_size[\"Median Accuracy\"] = overview_per_size[\"Median Accuracy\"].round(2)     overview_per_size[\"SD\"] = overview_per_size[\"SD\"].round(2)     # sort by mean, descending     overview_per_size = overview_per_size.sort_values(         by=\"Median Accuracy\",         ascending=False,     )     overview_per_size.to_csv(         f\"{result_files_path}processed/overview-model.csv\",         index=True,     )      return overview"},{"location":"BioChatter/scripts/_stats/","title":"stats","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Statistical analysis of the results.\"\"\"\n</pre> \"\"\"Statistical analysis of the results.\"\"\" In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom importlib.util import module_from_spec, spec_from_file_location\nfrom pathlib import Path\n</pre> import sys from importlib.util import module_from_spec, spec_from_file_location from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import pandas as pd import seaborn as sns In\u00a0[\u00a0]: Copied! <pre># Import plotting module using full path\nCURRENT_DIR = Path(__file__).parent\nplotting_path = CURRENT_DIR / \"_plotting.py\"\n</pre> # Import plotting module using full path CURRENT_DIR = Path(__file__).parent plotting_path = CURRENT_DIR / \"_plotting.py\" In\u00a0[\u00a0]: Copied! <pre>if \"_plotting\" not in sys.modules:\n    spec = spec_from_file_location(\"_plotting\", plotting_path)\n    if spec is None or spec.loader is None:\n        msg = \"Could not load _plotting module\"\n        raise ImportError(msg)\n    plotting = module_from_spec(spec)\n    sys.modules[\"_plotting\"] = plotting\n    spec.loader.exec_module(plotting)\nelse:\n    plotting = sys.modules[\"_plotting\"]\n</pre> if \"_plotting\" not in sys.modules:     spec = spec_from_file_location(\"_plotting\", plotting_path)     if spec is None or spec.loader is None:         msg = \"Could not load _plotting module\"         raise ImportError(msg)     plotting = module_from_spec(spec)     sys.modules[\"_plotting\"] = plotting     spec.loader.exec_module(plotting) else:     plotting = sys.modules[\"_plotting\"] In\u00a0[\u00a0]: Copied! <pre>melt_and_process = plotting.melt_and_process\n</pre> melt_and_process = plotting.melt_and_process In\u00a0[\u00a0]: Copied! <pre>def calculate_stats(overview: pd.DataFrame) -&gt; None:\n    overview_melted = melt_and_process(overview)\n    # calculate p-value between naive and biochatter\n    from scipy.stats import ttest_ind\n\n    biochatter = overview_melted[overview_melted[\"Task\"] == \"query_generation\"][\"Accuracy\"].dropna()\n    naive = overview_melted[overview_melted[\"Task\"] == \"naive_query_generation_using_schema\"][\"Accuracy\"].dropna()\n\n    t_stat, p_value = ttest_ind(biochatter, naive)\n\n    # write to file\n    with open(\"benchmark/results/processed/naive_vs_biochatter.txt\", \"w\") as f:\n        f.write(f\"mean: {biochatter.mean()} vs {naive.mean()}\\n\")\n        f.write(f\"std: {biochatter.std()} vs {naive.std()}\\n\")\n        f.write(f\"p-value: {p_value}\\n\")\n        f.write(f\"t-statistic: {t_stat}\\n\")\n\n    # TODO publish this test and other related ones on website as well?\n\n    # calculate correlation between LLM size and accuracy for all tasks\n    # convert size to float, make Unknown = 300, replace commas with dots\n    size = overview_melted[\"Size\"].apply(\n        lambda x: 300 if x == \"Unknown\" else float(x.replace(\",\", \".\")),\n    )\n    from scipy.stats import pearsonr\n\n    # Create a mask of rows where 'Accuracy' is not NaN\n    mask = overview_melted[\"Accuracy\"].notna()\n\n    # Use the mask to select only the rows from 'size' and 'Accuracy' where 'Accuracy' is not NaN\n    size = size[mask]\n    accuracy = overview_melted[\"Accuracy\"][mask]\n\n    size_accuracy_corr, size_accuracy_p_value = pearsonr(size, accuracy)\n\n    # plot scatter plot\n    plt.figure(figsize=(6, 4))\n    sns.scatterplot(x=size, y=overview_melted[\"Accuracy\"])\n    plt.xlabel(\"Model size (billions of parameters)\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Scatter plot of model size vs accuracy\")\n    plt.savefig(\n        \"docs/images/scatter-size-accuracy.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/scatter-size-accuracy.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n    # calculate correlation between quantisation and accuracy for all tasks\n    # convert quantisation to float, make &gt;= 16-bit* = 16, replace -bit with nothing\n    quantisation = overview_melted[\"Quantisation\"].apply(\n        lambda x: 16 if x == \"&gt;= 16-bit*\" else float(x.replace(\"-bit\", \"\")),\n    )\n    # Create a mask of rows where 'Accuracy' is not NaN\n    mask = overview_melted[\"Accuracy\"].notna()\n\n    # Use the mask to select only the rows from 'quantisation' and 'Accuracy' where 'Accuracy' is not NaN\n    quantisation = quantisation[mask]\n    accuracy = overview_melted[\"Accuracy\"][mask]\n\n    quant_accuracy_corr, quant_accuracy_p_value = pearsonr(\n        quantisation,\n        accuracy,\n    )\n    # plot scatter plot\n    plt.figure(figsize=(6, 4))\n    sns.scatterplot(x=quantisation, y=overview_melted[\"Accuracy\"])\n    plt.xlabel(\"Quantisation (bits)\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Scatter plot of quantisation vs accuracy\")\n    plt.savefig(\n        \"docs/images/scatter-quantisation-accuracy.png\",\n        bbox_inches=\"tight\",\n        dpi=300,\n    )\n    plt.savefig(\n        \"docs/images/scatter-quantisation-accuracy.pdf\",\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n    # write to file\n    with open(\"benchmark/results/processed/correlations.txt\", \"w\") as f:\n        f.write(f\"Size vs accuracy Pearson correlation: {size_accuracy_corr}\\n\")\n        f.write(\n            f\"Size vs accuracy Pearson correlation p-value: {size_accuracy_p_value}\\n\",\n        )\n        f.write(\n            f\"Quantisation vs accuracy Pearson correlation: {quant_accuracy_corr}\\n\",\n        )\n        f.write(\n            f\"Quantisation vs accuracy Pearson correlation p-value: {quant_accuracy_p_value}\\n\",\n        )\n</pre> def calculate_stats(overview: pd.DataFrame) -&gt; None:     overview_melted = melt_and_process(overview)     # calculate p-value between naive and biochatter     from scipy.stats import ttest_ind      biochatter = overview_melted[overview_melted[\"Task\"] == \"query_generation\"][\"Accuracy\"].dropna()     naive = overview_melted[overview_melted[\"Task\"] == \"naive_query_generation_using_schema\"][\"Accuracy\"].dropna()      t_stat, p_value = ttest_ind(biochatter, naive)      # write to file     with open(\"benchmark/results/processed/naive_vs_biochatter.txt\", \"w\") as f:         f.write(f\"mean: {biochatter.mean()} vs {naive.mean()}\\n\")         f.write(f\"std: {biochatter.std()} vs {naive.std()}\\n\")         f.write(f\"p-value: {p_value}\\n\")         f.write(f\"t-statistic: {t_stat}\\n\")      # TODO publish this test and other related ones on website as well?      # calculate correlation between LLM size and accuracy for all tasks     # convert size to float, make Unknown = 300, replace commas with dots     size = overview_melted[\"Size\"].apply(         lambda x: 300 if x == \"Unknown\" else float(x.replace(\",\", \".\")),     )     from scipy.stats import pearsonr      # Create a mask of rows where 'Accuracy' is not NaN     mask = overview_melted[\"Accuracy\"].notna()      # Use the mask to select only the rows from 'size' and 'Accuracy' where 'Accuracy' is not NaN     size = size[mask]     accuracy = overview_melted[\"Accuracy\"][mask]      size_accuracy_corr, size_accuracy_p_value = pearsonr(size, accuracy)      # plot scatter plot     plt.figure(figsize=(6, 4))     sns.scatterplot(x=size, y=overview_melted[\"Accuracy\"])     plt.xlabel(\"Model size (billions of parameters)\")     plt.ylabel(\"Accuracy\")     plt.title(\"Scatter plot of model size vs accuracy\")     plt.savefig(         \"docs/images/scatter-size-accuracy.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/scatter-size-accuracy.pdf\",         bbox_inches=\"tight\",     )     plt.close()      # calculate correlation between quantisation and accuracy for all tasks     # convert quantisation to float, make &gt;= 16-bit* = 16, replace -bit with nothing     quantisation = overview_melted[\"Quantisation\"].apply(         lambda x: 16 if x == \"&gt;= 16-bit*\" else float(x.replace(\"-bit\", \"\")),     )     # Create a mask of rows where 'Accuracy' is not NaN     mask = overview_melted[\"Accuracy\"].notna()      # Use the mask to select only the rows from 'quantisation' and 'Accuracy' where 'Accuracy' is not NaN     quantisation = quantisation[mask]     accuracy = overview_melted[\"Accuracy\"][mask]      quant_accuracy_corr, quant_accuracy_p_value = pearsonr(         quantisation,         accuracy,     )     # plot scatter plot     plt.figure(figsize=(6, 4))     sns.scatterplot(x=quantisation, y=overview_melted[\"Accuracy\"])     plt.xlabel(\"Quantisation (bits)\")     plt.ylabel(\"Accuracy\")     plt.title(\"Scatter plot of quantisation vs accuracy\")     plt.savefig(         \"docs/images/scatter-quantisation-accuracy.png\",         bbox_inches=\"tight\",         dpi=300,     )     plt.savefig(         \"docs/images/scatter-quantisation-accuracy.pdf\",         bbox_inches=\"tight\",     )     plt.close()      # write to file     with open(\"benchmark/results/processed/correlations.txt\", \"w\") as f:         f.write(f\"Size vs accuracy Pearson correlation: {size_accuracy_corr}\\n\")         f.write(             f\"Size vs accuracy Pearson correlation p-value: {size_accuracy_p_value}\\n\",         )         f.write(             f\"Quantisation vs accuracy Pearson correlation: {quant_accuracy_corr}\\n\",         )         f.write(             f\"Quantisation vs accuracy Pearson correlation p-value: {quant_accuracy_p_value}\\n\",         )"},{"location":"BioChatter/scripts/api_analysis/","title":"Api analysis","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>def analyze_api_calling():\n    \"\"\"Analyze and plot results from the API calling benchmarks.\"\"\"\n    # Read and process Python API calling results\n    python_results = preprocess_results(\"benchmark/results/python_api_calling.csv\", \"Python API\")\n    python_reduced_results = preprocess_results(\n        \"benchmark/results/python_api_calling_reduced.csv\", \"Python API (Reduced)\"\n    )\n\n    # Combine Python results\n    all_results = pd.concat([python_results, python_reduced_results])\n\n    # Plot results\n    plot_model_family_comparison(all_results)\n    plot_benchmark_comparison(all_results)\n    plot_task_type_comparison(all_results)\n    save_summary_statistics(all_results)\n</pre> def analyze_api_calling():     \"\"\"Analyze and plot results from the API calling benchmarks.\"\"\"     # Read and process Python API calling results     python_results = preprocess_results(\"benchmark/results/python_api_calling.csv\", \"Python API\")     python_reduced_results = preprocess_results(         \"benchmark/results/python_api_calling_reduced.csv\", \"Python API (Reduced)\"     )      # Combine Python results     all_results = pd.concat([python_results, python_reduced_results])      # Plot results     plot_model_family_comparison(all_results)     plot_benchmark_comparison(all_results)     plot_task_type_comparison(all_results)     save_summary_statistics(all_results) In\u00a0[\u00a0]: Copied! <pre>def preprocess_results(filepath: str, benchmark_type: str) -&gt; pd.DataFrame:\n    \"\"\"Preprocess results from a single benchmark file.\"\"\"\n    results = pd.read_csv(filepath)\n\n    # Calculate accuracy\n    results[\"score_possible\"] = results[\"score\"].apply(lambda x: float(x.split(\"/\")[1]))\n    results[\"scores\"] = results[\"score\"].apply(lambda x: x.split(\"/\")[0])\n    results[\"score_achieved\"] = results[\"scores\"].apply(\n        lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x))\n    )\n    results[\"accuracy\"] = results[\"score_achieved\"] / results[\"score_possible\"]\n\n    # Add benchmark type\n    results[\"benchmark_type\"] = benchmark_type\n\n    # Add task type analysis and merge similar categories\n    results[\"task_type\"] = results[\"subtask\"].apply(lambda x: x.split(\" - \")[0].split(\":\")[-1])\n\n    # Merge similar categories\n    results[\"task_type\"] = results[\"task_type\"].replace(\n        {\n            \"abbreviations\": \"abbreviation\",\n            \"general_question\": \"general\",\n            \"explicit_variable_names\": \"specific\",\n        }\n    )\n\n    return results\n</pre> def preprocess_results(filepath: str, benchmark_type: str) -&gt; pd.DataFrame:     \"\"\"Preprocess results from a single benchmark file.\"\"\"     results = pd.read_csv(filepath)      # Calculate accuracy     results[\"score_possible\"] = results[\"score\"].apply(lambda x: float(x.split(\"/\")[1]))     results[\"scores\"] = results[\"score\"].apply(lambda x: x.split(\"/\")[0])     results[\"score_achieved\"] = results[\"scores\"].apply(         lambda x: (np.mean([float(score) for score in x.split(\";\")]) if \";\" in x else float(x))     )     results[\"accuracy\"] = results[\"score_achieved\"] / results[\"score_possible\"]      # Add benchmark type     results[\"benchmark_type\"] = benchmark_type      # Add task type analysis and merge similar categories     results[\"task_type\"] = results[\"subtask\"].apply(lambda x: x.split(\" - \")[0].split(\":\")[-1])      # Merge similar categories     results[\"task_type\"] = results[\"task_type\"].replace(         {             \"abbreviations\": \"abbreviation\",             \"general_question\": \"general\",             \"explicit_variable_names\": \"specific\",         }     )      return results In\u00a0[\u00a0]: Copied! <pre>def plot_model_family_comparison(results: pd.DataFrame) -&gt; None:\n    \"\"\"Create violin plot comparing individual models across all benchmarks.\"\"\"\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(12, 6))\n\n    # # Define model order (from oldest to newest)\n    # model_order = [\n    #     \"gpt-3.5-turbo-0125\",\n    #     \"gpt-4-0613\",\n    #     \"gpt-4-0125-preview\",\n    #     \"gpt-4-1106-preview\",\n    #     \"gpt-4-turbo-2024-04-09\",\n    #     \"gpt-4o-2024-08-06\",\n    #     \"gpt-4o-2023-11-20\",\n    #     \"gpt-4o-mini-2024-07-18\"\n    # ]\n    # # Ensure model order has no duplicates and only includes models present in the data\n    # model_order = [m for m in model_order if m in results['model_name'].unique()]\n\n    sns.violinplot(\n        x=\"model_name\",\n        y=\"accuracy\",\n        hue=\"benchmark_type\",\n        data=results,\n        cut=0,  # Don't extend beyond observed data\n        # order=model_order\n    )\n\n    plt.title(\"API Calling Performance by Model and Benchmark Type\")\n    plt.xlabel(\"Model\")\n    plt.ylabel(\"Accuracy\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n    plt.savefig(\"docs/images/api-calling-model-comparison.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n</pre> def plot_model_family_comparison(results: pd.DataFrame) -&gt; None:     \"\"\"Create violin plot comparing individual models across all benchmarks.\"\"\"     sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(12, 6))      # # Define model order (from oldest to newest)     # model_order = [     #     \"gpt-3.5-turbo-0125\",     #     \"gpt-4-0613\",     #     \"gpt-4-0125-preview\",     #     \"gpt-4-1106-preview\",     #     \"gpt-4-turbo-2024-04-09\",     #     \"gpt-4o-2024-08-06\",     #     \"gpt-4o-2023-11-20\",     #     \"gpt-4o-mini-2024-07-18\"     # ]     # # Ensure model order has no duplicates and only includes models present in the data     # model_order = [m for m in model_order if m in results['model_name'].unique()]      sns.violinplot(         x=\"model_name\",         y=\"accuracy\",         hue=\"benchmark_type\",         data=results,         cut=0,  # Don't extend beyond observed data         # order=model_order     )      plt.title(\"API Calling Performance by Model and Benchmark Type\")     plt.xlabel(\"Model\")     plt.ylabel(\"Accuracy\")     plt.xticks(rotation=45, ha=\"right\")     plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")      plt.savefig(\"docs/images/api-calling-model-comparison.png\", bbox_inches=\"tight\", dpi=300)     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_benchmark_comparison(results: pd.DataFrame) -&gt; None:\n    \"\"\"Create violin plot comparing benchmark types.\"\"\"\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n\n    sns.violinplot(\n        x=\"benchmark_type\",\n        y=\"accuracy\",\n        data=results,\n        hue=\"benchmark_type\",\n        cut=0,  # Don't extend beyond observed data\n    )\n\n    plt.title(\"Performance Comparison Across API Calling Benchmarks\")\n    plt.xticks(rotation=45)\n\n    plt.savefig(\"docs/images/api-calling-benchmark-comparison.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n</pre> def plot_benchmark_comparison(results: pd.DataFrame) -&gt; None:     \"\"\"Create violin plot comparing benchmark types.\"\"\"     sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(6, 4))      sns.violinplot(         x=\"benchmark_type\",         y=\"accuracy\",         data=results,         hue=\"benchmark_type\",         cut=0,  # Don't extend beyond observed data     )      plt.title(\"Performance Comparison Across API Calling Benchmarks\")     plt.xticks(rotation=45)      plt.savefig(\"docs/images/api-calling-benchmark-comparison.png\", bbox_inches=\"tight\", dpi=300)     plt.close() In\u00a0[\u00a0]: Copied! <pre>def plot_task_type_comparison(results: pd.DataFrame) -&gt; None:\n    \"\"\"Create violin plot comparing performance across different task types.\"\"\"\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(12, 6))\n\n    sns.violinplot(\n        x=\"task_type\",\n        y=\"accuracy\",\n        hue=\"benchmark_type\",\n        data=results,\n        cut=0,  # Don't extend beyond observed data\n    )\n\n    plt.title(\"API Calling Performance by Task Type\")\n    plt.xlabel(\"Task Type\")\n    plt.ylabel(\"Accuracy\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n    plt.savefig(\"docs/images/api-calling-task-type-comparison.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n</pre> def plot_task_type_comparison(results: pd.DataFrame) -&gt; None:     \"\"\"Create violin plot comparing performance across different task types.\"\"\"     sns.set_theme(style=\"whitegrid\")     plt.figure(figsize=(12, 6))      sns.violinplot(         x=\"task_type\",         y=\"accuracy\",         hue=\"benchmark_type\",         data=results,         cut=0,  # Don't extend beyond observed data     )      plt.title(\"API Calling Performance by Task Type\")     plt.xlabel(\"Task Type\")     plt.ylabel(\"Accuracy\")     plt.xticks(rotation=45, ha=\"right\")     plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")      plt.savefig(\"docs/images/api-calling-task-type-comparison.png\", bbox_inches=\"tight\", dpi=300)     plt.close() In\u00a0[\u00a0]: Copied! <pre>def save_summary_statistics(results: pd.DataFrame) -&gt; None:\n    \"\"\"Calculate and save summary statistics.\"\"\"\n    # Summary by model and benchmark type\n    summary = (\n        results.groupby([\"benchmark_type\", \"model_name\"])[\"accuracy\"]\n        .agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])\n        .round(3)\n    )\n\n    summary.to_csv(\"benchmark/results/processed/api_calling_summary.csv\")\n\n    # Overall benchmark comparison\n    benchmark_summary = (\n        results.groupby(\"benchmark_type\")[\"accuracy\"].agg([\"mean\", \"std\", \"min\", \"max\", \"count\"]).round(3)\n    )\n\n    benchmark_summary.to_csv(\"benchmark/results/processed/api_calling_benchmark_summary.csv\")\n\n    # Add task type summary\n    task_summary = (\n        results.groupby([\"benchmark_type\", \"task_type\"])[\"accuracy\"]\n        .agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])\n        .round(3)\n    )\n\n    task_summary.to_csv(\"benchmark/results/processed/api_calling_task_summary.csv\")\n</pre> def save_summary_statistics(results: pd.DataFrame) -&gt; None:     \"\"\"Calculate and save summary statistics.\"\"\"     # Summary by model and benchmark type     summary = (         results.groupby([\"benchmark_type\", \"model_name\"])[\"accuracy\"]         .agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])         .round(3)     )      summary.to_csv(\"benchmark/results/processed/api_calling_summary.csv\")      # Overall benchmark comparison     benchmark_summary = (         results.groupby(\"benchmark_type\")[\"accuracy\"].agg([\"mean\", \"std\", \"min\", \"max\", \"count\"]).round(3)     )      benchmark_summary.to_csv(\"benchmark/results/processed/api_calling_benchmark_summary.csv\")      # Add task type summary     task_summary = (         results.groupby([\"benchmark_type\", \"task_type\"])[\"accuracy\"]         .agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])         .round(3)     )      task_summary.to_csv(\"benchmark/results/processed/api_calling_task_summary.csv\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    analyze_api_calling()\n</pre> if __name__ == \"__main__\":     analyze_api_calling()"},{"location":"BioChatter/scripts/hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"MkDocs hooks for preprocessing data and generating plots.\n\nPreprocessing and plotting scripts are run during documentation build.\n\"\"\"\n</pre> \"\"\"MkDocs hooks for preprocessing data and generating plots.  Preprocessing and plotting scripts are run during documentation build. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nfrom importlib.util import module_from_spec, spec_from_file_location\nfrom pathlib import Path\n</pre> import os import sys from importlib.util import module_from_spec, spec_from_file_location from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre># Get the current directory\nCURRENT_DIR = Path(__file__).parent\n</pre> # Get the current directory CURRENT_DIR = Path(__file__).parent In\u00a0[\u00a0]: Copied! <pre># Dynamically import local modules with full paths\ndef import_local_module(module_name: str) -&gt; None:\n    \"\"\"Import a local module.\"\"\"\n    module_path = CURRENT_DIR / f\"_{module_name}.py\"\n    module = None\n\n    # If module is already loaded, return it\n    if f\"_{module_name}\" in sys.modules:\n        return sys.modules[f\"_{module_name}\"]\n\n    spec = spec_from_file_location(f\"_{module_name}\", module_path)\n    if spec is None or spec.loader is None:\n        msg = f\"Could not load module {module_name}\"\n        raise ImportError(msg)\n    module = module_from_spec(spec)\n    sys.modules[module.__name__] = module\n    spec.loader.exec_module(module)\n    return module\n</pre> # Dynamically import local modules with full paths def import_local_module(module_name: str) -&gt; None:     \"\"\"Import a local module.\"\"\"     module_path = CURRENT_DIR / f\"_{module_name}.py\"     module = None      # If module is already loaded, return it     if f\"_{module_name}\" in sys.modules:         return sys.modules[f\"_{module_name}\"]      spec = spec_from_file_location(f\"_{module_name}\", module_path)     if spec is None or spec.loader is None:         msg = f\"Could not load module {module_name}\"         raise ImportError(msg)     module = module_from_spec(spec)     sys.modules[module.__name__] = module     spec.loader.exec_module(module)     return module In\u00a0[\u00a0]: Copied! <pre># Import the modules\nplotting = import_local_module(\"plotting\")\npreprocess = import_local_module(\"preprocess\")\nstats = import_local_module(\"stats\")\n</pre> # Import the modules plotting = import_local_module(\"plotting\") preprocess = import_local_module(\"preprocess\") stats = import_local_module(\"stats\") In\u00a0[\u00a0]: Copied! <pre># Extract the needed functions\nplot_accuracy_per_model = plotting.plot_accuracy_per_model\nplot_accuracy_per_quantisation = plotting.plot_accuracy_per_quantisation\nplot_accuracy_per_task = plotting.plot_accuracy_per_task\nplot_comparison_naive_biochatter = plotting.plot_comparison_naive_biochatter\nplot_extraction_tasks = plotting.plot_extraction_tasks\nplot_image_caption_confidence = plotting.plot_image_caption_confidence\nplot_medical_exam = plotting.plot_medical_exam\nplot_rag_tasks = plotting.plot_rag_tasks\nplot_scatter_per_quantisation = plotting.plot_scatter_per_quantisation\nplot_task_comparison = plotting.plot_task_comparison\nplot_text2cypher = plotting.plot_text2cypher\nplot_text2cypher_safety_only = plotting.plot_text2cypher_safety_only\n</pre> # Extract the needed functions plot_accuracy_per_model = plotting.plot_accuracy_per_model plot_accuracy_per_quantisation = plotting.plot_accuracy_per_quantisation plot_accuracy_per_task = plotting.plot_accuracy_per_task plot_comparison_naive_biochatter = plotting.plot_comparison_naive_biochatter plot_extraction_tasks = plotting.plot_extraction_tasks plot_image_caption_confidence = plotting.plot_image_caption_confidence plot_medical_exam = plotting.plot_medical_exam plot_rag_tasks = plotting.plot_rag_tasks plot_scatter_per_quantisation = plotting.plot_scatter_per_quantisation plot_task_comparison = plotting.plot_task_comparison plot_text2cypher = plotting.plot_text2cypher plot_text2cypher_safety_only = plotting.plot_text2cypher_safety_only In\u00a0[\u00a0]: Copied! <pre>create_overview_table = preprocess.create_overview_table\npreprocess_results_for_frontend = preprocess.preprocess_results_for_frontend\n</pre> create_overview_table = preprocess.create_overview_table preprocess_results_for_frontend = preprocess.preprocess_results_for_frontend In\u00a0[\u00a0]: Copied! <pre>calculate_stats = stats.calculate_stats\n</pre> calculate_stats = stats.calculate_stats In\u00a0[\u00a0]: Copied! <pre># Add both the scripts directory and project root to Python path\ncurrent_dir = str(Path(__file__).parent)\nproject_root = str(Path(__file__).parent.parent.parent)\n</pre> # Add both the scripts directory and project root to Python path current_dir = str(Path(__file__).parent) project_root = str(Path(__file__).parent.parent.parent) In\u00a0[\u00a0]: Copied! <pre>for path in [current_dir, project_root]:\n    if path not in sys.path:\n        sys.path.insert(0, path)\n</pre> for path in [current_dir, project_root]:     if path not in sys.path:         sys.path.insert(0, path) In\u00a0[\u00a0]: Copied! <pre>def on_pre_build() -&gt; None:\n    \"\"\"Run pre-processing and plotting scripts.\n\n    This function is no longer called when building the documentation. The hooks\n    are run by a GitHub action (see .github/workflows/benchmark-processing.yml).\n    For local development, the hooks can be run by executing this file directly.\n    \"\"\"\n    result_files_path = \"benchmark/results/\"\n\n    result_file_names = [\n        f\n        for f in os.listdir(result_files_path)\n        if (Path(result_files_path) / f).is_file()\n        and f.endswith(\".csv\")\n        and \"failure_mode\" not in f\n        and \"confidence\" not in f\n    ]\n\n    for file_name in result_file_names:\n        results = pd.read_csv(f\"{result_files_path}{file_name}\")\n        preprocess_results_for_frontend(results, result_files_path, file_name)\n\n    overview = create_overview_table(result_files_path, result_file_names)\n\n    plot_text2cypher()\n    plot_text2cypher_safety_only()\n    plot_image_caption_confidence()\n    plot_medical_exam()\n    plot_extraction_tasks()\n    plot_scatter_per_quantisation(overview)\n    plot_accuracy_per_model(overview)\n    plot_accuracy_per_quantisation(overview)\n    plot_accuracy_per_task(overview)\n    plot_task_comparison(overview)\n    plot_rag_tasks(overview)\n    plot_comparison_naive_biochatter(overview)\n    calculate_stats(overview)\n</pre> def on_pre_build() -&gt; None:     \"\"\"Run pre-processing and plotting scripts.      This function is no longer called when building the documentation. The hooks     are run by a GitHub action (see .github/workflows/benchmark-processing.yml).     For local development, the hooks can be run by executing this file directly.     \"\"\"     result_files_path = \"benchmark/results/\"      result_file_names = [         f         for f in os.listdir(result_files_path)         if (Path(result_files_path) / f).is_file()         and f.endswith(\".csv\")         and \"failure_mode\" not in f         and \"confidence\" not in f     ]      for file_name in result_file_names:         results = pd.read_csv(f\"{result_files_path}{file_name}\")         preprocess_results_for_frontend(results, result_files_path, file_name)      overview = create_overview_table(result_files_path, result_file_names)      plot_text2cypher()     plot_text2cypher_safety_only()     plot_image_caption_confidence()     plot_medical_exam()     plot_extraction_tasks()     plot_scatter_per_quantisation(overview)     plot_accuracy_per_model(overview)     plot_accuracy_per_quantisation(overview)     plot_accuracy_per_task(overview)     plot_task_comparison(overview)     plot_rag_tasks(overview)     plot_comparison_naive_biochatter(overview)     calculate_stats(overview) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    on_pre_build()\n</pre> if __name__ == \"__main__\":     on_pre_build()"},{"location":"BioChatter/vignettes/","title":"BioChatter Vignettes","text":"<p>Here, we demonstrate BioChatter usage in various use cases to get you started with your own implementation. Find the different use cases in the navigation side bar on the left.</p>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/","title":"Custom BioChatter Light: Advanced Use Case","text":"<p>For more advanced workflows, you may want to customise the BioChatter Light interface to display a new way of visualising your data or interacting with it. Due to the flexible, modular, and easy-to-use Streamlit framework, you can easily create new tabs and customise the existing ones. Here, we show how to create new tabs on the example of a project management tool we developed as a demonstration use case. The final web app is available at https://project.biochatter.org.</p>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/#background","title":"Background","text":"<p>Managing a scientific group is challenging for multiple reasons, particularly one which has multiple interdependent projects, each of which is pursued by a small team of junior and senior researchers. To enhance productivity and communication, it could be beneficial to have a tool that takes away some of the burden of project management, to increase the available \"thinking time\" for the scientists (for further reading, see for example this article). In the context of our work, there are two components we see as essential: data management (FAIR and transparent) and simple interfaces (driven by conversational AI). We will be using BioCypher and BioChatter for these two components.</p> <p>We will use a GitHub Project board (here) as the data source for our project management tool. This is close to a real-world scenario and allows connectivity to code repositories, issues, pull requests, and other components of computational collaboration. The linked project is \"synthetic\" data for demonstration purposes. The repository to build the KG and deploy the BioChatter Light web app is available here.</p>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/#build-the-kg","title":"Build the KG","text":"<p>GitHub Project access token</p> <p>Be aware that running this script will require a GitHub token with access to the project board. This token should be stored in the environment variable <code>BIOCYPHER_GITHUB_PROJECT_TOKEN</code>.</p> <p>We modified an existing adapter for the GitHub GraphQL API to pull data from the GitHub Project board. Thus, the time investment to build the KG was minimal (~3h); this is one central principle of BioCypher. We adapted the code (<code>project_planning/adapters/github_adapter.py</code>) and KG schema (<code>config/schema_config.yaml</code>) to represent the relevant features of the GitHub Project board. The pre-existing KG build and deploy scripts were used via the <code>docker-compose.yml</code> file. For public deployment, we also added a <code>docker-compose-password.yml</code>, which builds a password-protected version of the KG. Deployment and setup of the cloud VM took another ~2h.</p> <p>The schema of the knowledge graph is as follows:</p> <pre><code>graph LR;\n    Person -- Leads --&gt; Project\n    Project -- PartOf --&gt; Iteration\n    Project -- HasComment --&gt; Comment</code></pre> <p>Other available fields include <code>Title</code>, <code>Assignees</code>, <code>Status</code>, <code>Labels</code>, <code>Linked pull requests</code>, <code>Milestone</code>, <code>Repository</code>, <code>Reviewers</code>, <code>Priority</code>, <code>Size</code>, <code>Estimate</code>, <code>Iteration</code>, <code>Start date</code>, and <code>End date</code>. Some of these, such as <code>Priority</code> and <code>Size</code>, are properties of the project item in our current implementation. These assignments, including the schmema of the graph, can be flexibly adapted by using BioCypher mechanisms.</p>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/#add-the-additional-tabs-to-biochatter-light","title":"Add the additional tabs to BioChatter Light","text":"<p>BioChatter Light has a modular architecture to accommodate flexible layout changes. We also added a configuration that allows turning on or off specific tabs via environment variables. For this project, we added three new tabs: \"Summary\", \"Tasks\", and \"Settings\". The \"Summary\" tab shows an overview of the completed tasks in the current iteration of the project, the \"Tasks\" tab shows the upcoming tasks of the group and each team member, and the \"Settings\" tab allows configuration of the queries and LLM instructions used to generate the content for the other tabs.</p> <p>The solution as a web app is only an illustration of the project management tool; for example, we'd envision the deployed version rather as an integration of common messengers (Zulip, Slack, etc.) that acts as a conversational assistant to the group and its members. The web app is a proof of concept and demonstration of the capabilities of BioChatter Light, simulating feedback to the group and individual users via the simplified interface.</p> <p>The tabs were added to the BioChatter Light codebase in the corresponding module (<code>components/panels/project.py</code>), which contains all three tabs. The Streamlit framework makes this relatively easy; each tab only requires about 100 lines of code in this module and only contains simple components such as columns, buttons, and text fields. We also added environment variables to the configuration (<code>components/config.py</code>) to allow turning on or off the new tabs.</p>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/#configure-the-biochatter-light-docker-container","title":"Configure the BioChatter Light Docker container","text":"<p>As in the previous vignette, we can now configure the BioChatter Light Docker container to show only the new tabs. We provide these settings via the environment variables we introduced above, while turning off the default tabs. We have also added configurable environment variables for setting a page title, header, and subheader for the web app without having to change the source code.</p> <pre><code>services:\n  ## ... build, import, and deploy the KG ...\n  app:\n    image: biocypher/biochatter-light:0.6.10\n    container_name: app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - biochatter\n    depends_on:\n      import:\n        condition: service_completed_successfully\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - BIOCHATTER_LIGHT_TITLE=Project Planner\n      - BIOCHATTER_LIGHT_HEADER=GitHub Project Planning Assistant\n      - BIOCHATTER_LIGHT_SUBHEADER=A BioChatter Demonstration App for integrated project planning using LLMs\n      - DOCKER_COMPOSE=true\n      - CHAT_TAB=false\n      - PROMPT_ENGINEERING_TAB=false\n      - RAG_TAB=false\n      - CORRECTING_AGENT_TAB=false\n      - KNOWLEDGE_GRAPH_TAB=false\n      - LAST_WEEKS_SUMMARY_TAB=true\n      - THIS_WEEKS_TASKS_TAB=true\n      - TASK_SETTINGS_PANEL_TAB=true\n</code></pre> <p>Authentication</p> <p>For using the app with the standard OpenAI LLM, we need to provide the <code>OPENAI_API_KEY</code> environment variable. This key can be obtained from the OpenAI website.</p> <p>You can see the full configuration in the <code>docker-compose.yml</code> file of the project-planning repository. For public deployment, we also added a password-protected version of the KG, which only requires a few additional lines in the <code>docker-compose-password.yml</code> file.</p> <p>To deploy the tool on a cloud VM, we now only need to run the following commands:</p> <pre><code>git clone https://github.com/biocypher/project-planning.git\ndocker-compose -f project-planning/docker-compose-password.yml up -d\n</code></pre>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/#useful-tips-for-deployment","title":"Useful tips for deployment","text":"<p>Many vendors offer cloud VMs with pre-installed Docker and Docker Compose, as well as Nginx for reverse proxying. We recommend using a reverse proxy to provide HTTPS and a domain name for the web app. This can be done with a few lines in the Nginx configuration file. For example, to deploy the project management tool on a cloud VM with a domain name <code>project.biochatter.org</code>, you can use the following Nginx configuration:</p> <pre><code>server {\n    listen 80;\n    server_name project.biochatter.org;\n\n    location / {\n        proxy_pass http://localhost:8501;\n        proxy_http_version 1.1;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_read_timeout 86400;\n    }\n}\n</code></pre> <p>You can find more info here. We also recommend to set up certification with Let's Encrypt for HTTPS. This can be done with the Certbot tool, which is available for most Linux distributions.</p> <p>In addition, you need to make sure that your Neo4j deployment is accessible from your web app, and that the connection is secure. You can either make the DB accessible only on the VM's network, which would allow running it without encryption, or you can set up a secure connection with a password. Both options are implemented in the <code>docker-compose.yml</code> and <code>docker-compose-password.yml</code> files of the <code>project-planning</code> repository. You can find more info here.</p>"},{"location":"BioChatter/vignettes/custom-bclight-advanced/#conclusion","title":"Conclusion","text":"<p>This vignette showcases the creation of a custom BioChatter Light web app for a dedicated purpose, in this case, project management. The app is a demonstration of the flexibility and ease of use of the BioChatter Light framework, which allows for the rapid development of conversational AI interfaces for various applications. The project management tool is a proof of concept and will be further developed into a conversational assistant that can not only summarise, but interact with the group members, and provide administrative support for larger groups and even organisations.</p> <p>The capabilities of GitHub Projects and their API allow the transfer of issues between boards, which allows for a multi-level approach to project management. Higher-level master boards (such as the complex BioCypher ecosystem overview board) can collect the tasks and issues of a larger group, and the project management assistant can help in collating those into manageable chunks for smaller teams (such as the board of our synthetic project). The same abstraction can be used at the organisation level, where the aims and challenges of the organisation are broken down into projects and tasks for larger groups.</p>"},{"location":"BioChatter/vignettes/custom-bclight-simple/","title":"Custom BioChatter Light: Simple Use Case","text":"<p>For prototyping your own text2cypher workflow, it can be useful to have a simple interface for troubleshooting your queries. This vignette demonstrates how to customise the pre-built BioChatter Light Docker compose to turn off all tabs except the Knowledge Graph (KG) tab. This allows the deployment of an integrated KG build, deployment, and web app for LLM-based question answering.</p>"},{"location":"BioChatter/vignettes/custom-bclight-simple/#build-your-kg","title":"Build your KG","text":"<p>First, build your KG. For this example, we use the BioCypher Pole KG as a demo KG. The KG is based on an open-source dataset of crime statistics in Manchester. The schema of the demo KG is described in the Knowledge Graph RAG vignette. For building your own KG, refer to the BioCypher documentation.</p> <p>This KG is built, imported, and deployed in the first three stages of the <code>docker-compose.yml</code> file.</p>"},{"location":"BioChatter/vignettes/custom-bclight-simple/#configure-the-biochatter-light-docker-container","title":"Configure the BioChatter Light Docker container","text":"<p>We provide a simple way to customise the BioChatter Light Docker container to show only select components. We can provide these settings via environment variables, so in the case of running from <code>docker-compose.yml</code>, we can set these in the <code>environment</code> section of the <code>app</code> service.</p> <pre><code>services:\n  ## ... build, import, and deploy the KG ...\n  app:\n    image: biocypher/biochatter-light:0.6.10\n    container_name: app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - biochatter\n    depends_on:\n      import:\n        condition: service_completed_successfully\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DOCKER_COMPOSE=true\n      - CHAT_TAB=false\n      - PROMPT_ENGINEERING_TAB=false\n      - RAG_TAB=false\n      - CORRECTING_AGENT_TAB=false\n      - KNOWLEDGE_GRAPH_TAB=true\n</code></pre> <p>In this example, we provide our OpenAI API key and set the <code>DOCKER_COMPOSE</code> flag to <code>true</code>, which tells BioChatter Light to connect to the KG on the Docker network, which uses the service name as the hostname, so in this case, <code>deploy</code> instead of the default <code>localhost</code>.</p> <p>Authentication</p> <p>For using the app with the standard OpenAI LLM, we need to provide the <code>OPENAI_API_KEY</code> environment variable. This key can be obtained from the OpenAI website.</p> <p>We then turn off all default tabs (chatting, prompt engineering, RAG, and the correcting agent) and turn on the KG tab. Running the docker compose with these settings will build and deploy the KG and the BioChatter Light web app with only the KG tab enabled.</p> <pre><code>git clone https://github.com/biocypher/pole\ncd pole\ndocker-compose up -d\n</code></pre> <p>We are constantly expanding our repertoire of BioChatter Light tabs, so check back for more options in the future. Creating your own tabs is also accessible via our modular architecture and the simple Streamlit framework for UI design. Check the advanced vignette and the codebase of BioChatter Light for more information.</p> <p>Open-source LLMs</p> <p>You can use a locally hosted open-source LLM model by using the <code>docker-compose-ollama.yml</code> as an example (this deployment requires Ollama to be installed on your machine). See more here.</p>"},{"location":"BioChatter/vignettes/custom-decider-use-case/","title":"Custom BioChatter Light and Next: Cancer Genetics Use Case","text":"<p>This example is part of the BioChatter manuscript supplement. We demonstrate the resulting applications as web apps at https://decider-light.biochatter.org and https://decider-next.biochatter.org. Find more information on how to build and use these apps below.</p>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#background","title":"Background","text":"<p>Personalised medicine tailors treatment to a patient's unique genetic makeup. In cancer care, this approach helps categorize patients and assign them to specific treatment groups in clinical trials. However, interpreting and making decisions based on this data is challenging due to the complexity of genetic variations, the interaction between genes and environmental factors, tumor diversity, patient histories, and the vast amount of data produced by advanced technologies.</p> <p>In the DECIDER consortium, we aim to improve clinical decisions by providing support systems, for instance for the geneticists working on these cases. The code for the use case lives at https://github.com/biocypher/decider-genetics.</p> <p>Below, we show how we build a support application for this use case.</p>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#sources-of-knowledge","title":"Sources of knowledge","text":"<p>We integrate knowledge from diverse resources, using BioCypher to build a knowledge graph of:</p> <ol> <li> <p>Processed whole genome sequencing data of ovarian cancer patients (synthetic data)</p> <ul> <li> <p>genomic changes</p> <ul> <li> <p>classified by consequence (protein truncation, amino acid change)</p> </li> <li> <p>algorithmic prediction of deleteriousness</p> </li> <li> <p>variant identifiers </p> </li> </ul> </li> <li> <p>allele dosages</p> <ul> <li> <p>gene allele copy number (amplifications, deletions, loss-of-heterogeneity)</p> </li> <li> <p>mutation pervasiveness (estimate of number of affected alleles, or suspected subclonality)</p> </li> </ul> </li> <li> <p>proportion of cancer cells in the sample (tumour purity)</p> </li> </ul> </li> <li> <p>the patients' clinical history (synthetic data)</p> <ul> <li> <p>personal information (age at diagnosis, BMI, etc.)</p> </li> <li> <p>treatment history, known side effects, clinical response</p> </li> <li> <p>lab test results (blood, imaging, histopathology)</p> </li> <li> <p>common treatment-relevant mutations (BRCA), HR deficiency, PARP-inhibitor maintenance</p> </li> </ul> </li> <li> <p>data from open resources (real data)</p> <ul> <li> <p>variant annotations (as provided by the genetics pipeline of the DECIDER consortium)</p> </li> <li> <p>gene annotations (as provided by the genetics pipeline of the DECIDER consortium)</p> </li> <li> <p>pathway / process annotations (from public databases such as Gene Ontology)</p> </li> <li> <p>drug annotations (from OncoKB)</p> </li> </ul> </li> </ol> <p>In addition, we provide access to more resources via the RAG and API agents:</p> <ol> <li> <p>relevant publications from PubMed (real data) embedded in a vector database</p> </li> <li> <p>relevant knowledge streamed live from OncoKB (see below) via API access through BioChatter's API agent</p> </li> </ol>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#the-geneticists-workflow","title":"The geneticist's workflow","text":"<p>Personalised cancer therapy is guided by identifying somatic genomic driver events in specific genes, particularly when these involve well-known hotspot mutations. However, unique somatic events in the same genes or pathways can create a \"grey zone\" that requires manual geneticist analysis to determine their clinical significance.</p> <p>To address this, a comprehensive BioCypher backend processes whole-genome sequencing data to catalog somatic changes, annotating their consequences and potential actionability. These data can then be linked to external resources for clinical interpretation. For example, certain mutations in the BRCA1 or ERBB2 genes can indicate sensitivity to specific treatments such as PARP inhibitors or trastuzumab.</p> <p>To fully leverage actionable data, the integration of patient-specific information with literature on drug targets and mechanisms of action or resistance is essential. OncoKB is the primary resource for this information, accessible through drug annotations added to the knowledge graph (KG) and via the BioChatter API calling mechanism.</p> <p>Additionally, semantic search tools facilitate access to relevant biomedical literature, enabling geneticists to quickly verify findings against established treatments or resistance mechanisms.</p> <p>In summary, the main contributions of our use case to the productivity of this workflow are:</p> <ul> <li> <p>making processed and analysed genomic data locally available in a centralised resource by building a custom KG</p> </li> <li> <p>allowing comparison to literature via semantic search inside a vector database with relevant publications</p> </li> <li> <p>providing live access to external resources via the API agent</p> </li> </ul>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#building-the-application","title":"Building the application","text":"<p>We will explain how to use the BioCypher ecosystem, specifically, BioCypher and BioChatter, to build a decision support application for a cancer geneticist. The code base for this use case, including all details on how to set up the KG and the applications, is available at https://github.com/biocypher/decider-genetics. You can find live demonstrations of the application at links provided in the README of the repository. The build procedures can be reproduced by cloning the repository and running <code>docker-compose up -d</code> (or the equivalent for the Next app) in the root directory (note that the default configuration requires authentication with OpenAI services). The process involves the following steps:</p> <ol> <li> <p>Identifying data sources and creating a knowledge graph schema</p> </li> <li> <p>Building the KG with BioCypher from the identified sources</p> </li> <li> <p>Using BioChatter Light to develop and troubleshoot the KG application</p> </li> <li> <p>Customising BioChatter Next to yield an integrated conversational interface</p> </li> <li> <p>Deploying the applications</p> </li> </ol>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#identifying-data-sources-and-creating-a-knowledge-graph-schema","title":"Identifying data sources and creating a knowledge graph schema","text":"<p>We examine the data sources described above and design a KG schema that can accommodate the data. The configuration file, schema_config.yaml, can be seen in the <code>config</code> directory of the repository. The schema should also be designed with LLM access in mind; performance in generating specific queries can be adjusted for in step three (troubleshooting using BioChatter Light). We created a bespoke adapters for the genetics data of the DECIDER cohort according to the output format of the genetics pipeline, and reused existing adapters for the open resources. They can be found in the decider_genetics/adapters directory of the repository. For this use case, we created synthetic data to stand in for the real data for privacy reasons; the synthetic data are available in the <code>data</code> directory.</p> <p>This is the schema of our KG:</p> <pre><code>graph TD;\n    Patient[Patient] --&gt;|PatientToSequenceVariantAssociation| SequenceVariant[SequenceVariant]\n    Patient[Patient] --&gt;|PatientToCopyNumberAlterationAssociation| CopyNumberAlteration[CopyNumberAlteration]\n    SequenceVariant[SequenceVariant] --&gt;|SequenceVariantToGeneAssociation| Gene[Gene]\n    CopyNumberAlteration[CopyNumberAlteration] --&gt;|CopyNumberAlterationToGeneAssociation| Gene[Gene]\n    Gene[Gene] --&gt;|GeneToBiologicalProcessAssociation| BiologicalProcess[BiologicalProcess]\n    Gene[Gene] --&gt;|GeneDruggabilityAssociation| Drug[Drug]</code></pre>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#building-the-kg-with-biocypher","title":"Building the KG with BioCypher","text":"<p>In the dedicated adapters for the DECIDER genetics data, we pull the data from the synthetic data files and build the KG. We perform simplifying computations, as described above, to facilitate standard workflows (such as counting alleles, identifying pathogenic variants, and calculating tumour purity). We mold the data into the specified schema in a transparent and reproducible manner by configuring the adapters (see the decider_genetics/adapters directory).</p> <p>After creating the schema and adapters, we run the build script to populate the KG. BioCypher is configured using the biocypher_config.yaml file in the <code>config</code> directory. Using the Docker Compose workflow included in the BioCypher template repository, we build a containerised version of the KG. We can inspect the KG in the Neo4j browser at <code>http://localhost:7474</code> after running the build script. Any changes, if needed, can be made to the configuration of schema and adapters.</p>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#using-biochatter-light-to-develop-and-troubleshoot-the-kg-application","title":"Using BioChatter Light to develop and troubleshoot the KG application","text":"<p>Upon deploying the KG via Docker, we can use a custom BioChatter Light application to interact with the KG. Briefly, we remove all components except the KG interaction panel via environment variables in the docker-compose.yml file (see also the corresponding vignette). This allows us to start the KG and interact with it using an LLM in a reproducible manner with just one command. We can then test the LLM-KG interaction by asking questions and examining the generated queries and its results from the KG. Once we are satisfied with the KG schema and LLM performance, we can advance to the next step.</p> <p>OpenAI API key needed</p> <p>In the standard configuration, we use the OpenAI API to generate queries. Provide your <code>OPENAI_API_KEY</code> in the shell environment, or modify the application to call a different LLM.</p> <p>The BioChatter Light application, including the KG creation, can be built using <code>docker compose up -d</code> in the root directory of the repository.</p> <p>An online demonstration of this application can be found at https://decider-light.biochatter.org. You can use this demonstration to test the KG - LLM interaction, asking questions such as:</p> <ul> <li> <p>How many patients do we have on record, and what are their names?</p> </li> <li> <p>What was patient1's response to previous treatment, and which treatment did they receive?</p> </li> <li> <p>Which patients have HR deficiency but have not received PARP inhibitors?</p> </li> <li> <p>How many patients had severe adverse reactions, and to which drugs?</p> </li> <li> <p>Does patient1 have a sequence variant in a gene that is druggable? Which drug, and what evidence level has the association?</p> </li> <li> <p>Does patient1 have a sequence variant in a gene that is druggable with evidence level \"1\"? Which drug? Return unique values.</p> </li> <li> <p>Does patient1 have a copy number variant in a gene that is druggable with evidence level \"1\"? Which drug? Return unique values.</p> </li> </ul> <p>The query returned by the model can also be modified and rerun without an additional call to the LLM, allowing for easy troubleshooting and exploration of the KG. The schema information of the KG is displayed in the lower section of the page for reference.</p>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#customising-biochatter-next-to-yield-an-integrated-conversational-interface","title":"Customising BioChatter Next to yield an integrated conversational interface","text":"<p>We can further customise the Docker workflow to start the BioChatter Next application, including its REST API middleware <code>biochatter-server</code>. In addition to deploying all software components, we can also customise its appearance and functionality. Using the biochatter-next.yaml configuration file (in <code>config</code>, as all other configuration files), we can adjust the welcome message, how-to-use section, the system prompts for the LLM, which tools can be used by the LLM agent, the connection details of externally hosted KG or vectorstore, and other parameters. We then start BioChatter Next using a dedicated Docker Compose file, which includes the <code>biochatter-server</code> middleware and the BioChatter Next application.</p> <p>OpenAI API key needed</p> <p>In the standard configuration, we use the OpenAI API to generate queries. Provide your <code>OPENAI_API_KEY</code> in the <code>.bioserver.env</code> file, or modify the application to call a different LLM.</p> <p>The BioChatter Next application, including the customisation of the LLM and the integration of the KG, can be built using <code>docker compose -f docker-compose-next.yml up -d</code> in the root directory of the repository. An online demonstration of this application can be found at https://decider-next.biochatter.org.</p>"},{"location":"BioChatter/vignettes/custom-decider-use-case/#deploying-the-applications","title":"Deploying the applications","text":"<p>The final step is to deploy one or both applications on a server. Using the Docker Compose workflow, we can deploy the applications in many different environments, from local servers to cloud-based solutions. The environment supplied by the Docker software allows for high reproducibility and easy scaling. The BioChatter Light app can be used for testing, but also to provide a simple one-way interface to the KG for users who do not need the full conversational interface. The BioChatter Next app can be configured to connect to KG and vectorstore deployments on different servers, allowing for a distributed architecture and dedicated maintenance of components; but it can also be deployed in tandem from one Docker Compose, for smaller setups or local use.</p>"},{"location":"BioChatter/vignettes/kg/","title":"Vignette: Knowledge Graph RAG","text":"<p>This vignette demonstrates the KG module of BioChatter as used by the BioChatter Next application. We connect to a BioCypher knowledge graph (KG) to retrieve relevant information for a given question. We then use the retrieved information to generate a response to the question. The application can connect to any real-world BioCypher KG by providing the connection details in the <code>KG Settings</code> dialog.</p>"},{"location":"BioChatter/vignettes/kg/#background","title":"Background","text":"<p>For the demonstration purposes of this vignette, we include a demo KG based on an open-source dataset of crime statistics in Manchester, because it allows us to redistribute the KG due to its small size and public domain licence, and because it is easily understood. This is non-trivial for most biomedical datasets; however, we are currently working on a synthetic biomedical example to extend this vignette. This is the schema of the demo KG:</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Crime -- INVESTIGATED_BY --&gt; Officer(:Officer)\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime</code></pre> <p>The KG is adapted from a Neo4j tutorial, and is available as a BioCypher adapter including a BioChatter Light integration here. We also include it in an optional BioChatter Next Docker Compose configuration to allow trying it out locally.</p>"},{"location":"BioChatter/vignettes/kg/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the KG functionality by clicking on the <code>KG Settings</code> button in the sidebar. In the settings dialog, we can activate the KG functionality and select how many results we want to retrieve. Returning to the conversation and enabling the KG functionality for the current chat (directly above the send button), we can then ask the model about the KG. The language model we use in this vignette is, as in the RAG vignette, <code>gpt-3.5-turbo-0613</code> (deprecated as of July 16 2024). The conversation is pasted below for convenience, including the queries generated by BioChatter.</p> <p></p> <p>In the background, the RagAgent module of BioChatter receives the question and generates a query to retrieve the desired information. This is then passed back to the primary model, which includes it in its answer generation.</p>"},{"location":"BioChatter/vignettes/kg/#conclusion","title":"Conclusion","text":"<p>The native integration of BioCypher KGs into the BioChatter framework allows for a seamless integration of KGs into the conversational AI. This in turn facilitates knowledge accessibility in a wide range of application domains.</p>"},{"location":"BioChatter/vignettes/kg/#note-the-apparent-inability-of-gpt-to-understand-certain-directionalities-and-how-biochatter-compensates-for-this","title":"Note: the apparent inability of GPT to understand certain directionalities, and how BioChatter compensates for this","text":"<p>Interestingly, while <code>gpt-3.5-turbo-0613</code> mostly does a formidable job at translating natural language questions into Cypher queries, it is remarkably obtuse in certain instances. For instance, for the relationship <code>INVESTIGATED_BY</code>, which connects a <code>Crime</code> to an <code>Officer</code>, GPT consistently fails to understand that the relationship implies that the <code>Officer</code> is the one who investigates the <code>Crime</code>. Instead, it consistently interprets the relationship as if the <code>Crime</code> investigates the <code>Officer</code>: it consistently proposes the query <code>MATCH (o:Officer)-[:INVESTIGATED_BY]-&gt;(c:Crime) RETURN c, o</code> instead of the correct <code>MATCH (c:Crime)-[:INVESTIGATED_BY]-&gt;(o:Officer) RETURN c, o</code>. We were not able to change this behaviour with any contextual prompt instructions.</p> <p>For this reason, the BioChatter <code>prompts.py</code> module uses the knowledge we have about the directionality of edges in the BioCypher KG to only propose options for patterns that actually exist in the KG. In the instance of <code>INVESTIGATED_BY</code>, this is the corresponding YAML definition in BioCypher:</p> <pre><code>investigated by:\n    is_a: [fact, core]\n    represented_as: edge\n    label_as_edge: INVESTIGATED_BY\n    input_label: INVESTIGATED_BY\n    source: crime\n    target: officer\n</code></pre> <p>The presence of the <code>source</code> and <code>target</code> annotations allows us to provide only the correct options to the LLM, which in turn allows the LLM to generate the correct query.</p> <p>Strikingly, this even applies to <code>GPT-4</code> and <code>ChatGPT</code>, which we used to generate the Mermaid diagram above. The model immediately proposed a correct Mermaid diagram for most of the entire schema definition YAML file, except for the relationship between <code>Crime</code> and <code>Officer</code>, which it reversed. Via the YAML we provide in the prompt, GPT has access to the correct directionality, but it still fails to understand it.</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Officer(:Officer) -- INVESTIGATED_BY --&gt; Crime\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime\n\n    classDef node fill:#f9f,stroke:#333,stroke-width:2px;\n    class Person,Officer,Location,Crime,PhoneCall,Object node;\n</code></pre> <p>Here is the full chat for reference.</p>"},{"location":"BioChatter/vignettes/kg/#conversation","title":"Conversation","text":"<p>The boxes below the BioChatter responses in the conversation show the queries generated by the RagAgent module.</p>"},{"location":"BioChatter/vignettes/kg/#message-from-you","title":"Message From You","text":"<p>How many crimes happened in total?</p>"},{"location":"BioChatter/vignettes/kg/#message-from-biochatter","title":"Message From BioChatter","text":"<p>According to the provided statement, the total number of crimes is 28,762. This information suggests that there have been 28,762 reported crimes in the given context. However, without additional information or context, it is difficult to determine the specific time period, location, or nature of these crimes. It's important to note that crime statistics can vary depending on the source, methodology, and reporting practices. Therefore, it is always advisable to consult official crime reports or local law enforcement agencies for the most accurate and up-to-date information on crime statistics.</p> <pre><code>MATCH (c:Crime)\nRETURN count(c) AS numberOfCrimes\n</code></pre>"},{"location":"BioChatter/vignettes/kg/#message-from-you_1","title":"Message From You","text":"<p>Where happened most crimes?</p>"},{"location":"BioChatter/vignettes/kg/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided statement, it appears that the location with the highest number of crimes is a parking area, with a total of 811 reported crimes. However, without additional context or information, it is difficult to determine the consistency or inconsistency of this statement with other available information. It is important to note that crime rates and locations can vary significantly depending on various factors such as the region, city, neighborhood, and specific time period being considered. To provide a more comprehensive analysis, it would be helpful to have access to a broader dataset or additional information about the specific location and its crime statistics.</p> <pre><code>MATCH (c:Crime)-[:OCCURRED_AT]-&gt;(l:Location)\nRETURN l.address AS location, COUNT(c) AS crimeCount\nORDER BY crimeCount DESC\n</code></pre>"},{"location":"BioChatter/vignettes/kg/#message-from-you_2","title":"Message From You","text":"<p>Who were the top 3 offenders?</p>"},{"location":"BioChatter/vignettes/kg/#message-from-biochatter_2","title":"Message From BioChatter","text":"<p>Based on the provided information, the top three offenders are:</p> <ol> <li> <p>Phillip Williamson: According to the information given, Phillip Williamson has committed 5 crimes.</p> </li> <li> <p>Jessica Kelly: The information states that Jessica Kelly has also committed 5 crimes.</p> </li> <li> <p>Brian Morales: The data indicates that Brian Morales has committed 4 crimes.</p> </li> </ol> <p>These three individuals, Phillip Williamson, Jessica Kelly, and Brian Morales, are the top offenders based on the number of crimes they have committed.</p> <pre><code>MATCH (p:Person)-[:PARTY_TO]-&gt;(c:Crime)\nRETURN p.name, p.surname, COUNT(c) AS num_crimes\nORDER BY num_crimes DESC\nLIMIT 3\n</code></pre>"},{"location":"BioChatter/vignettes/rag/","title":"Vignette: Retrieval-Augmented Generation (RAG)","text":"<p>This vignette demonstrates the RAG module of BioChatter as used by the BioChatter Next application. This basic use case involves an LLM manuscript reading assistant and a vector database with embedded scientific manuscripts. The manuscripts are recent reviews on the topic of ABC transporters (active transport proteins that often serve as efflux pumps in many tissues, with a major function of clearing exogenous substances) in drug resistance phenomena, which we currently investigate in our DECIDER cohort.  Manuscripts such as these can be absent from the knowledge base of LLMs for various reasons, such as their recency or the fact that they are not open access. RAG opens up the possibility to retrieve relevant information from these manuscripts, and to inject it into the LLM's generation process.</p>"},{"location":"BioChatter/vignettes/rag/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the RAG functionality by clicking on the <code>RAG Settings</code> button in the sidebar. In the settings dialog, we can activate the functionality and upload an arbitrary number of documents, which is only limited by the scale of the vector database system. In this case, and for demonstration purposes, we uploaded the four manuscripts, which leads to the state shown in the screenshot below. You can find the citations below.</p> <p></p> <p>Note that we chose to split the documents into fragments of 1000 characters each. We could have split by tokens instead, or changed the fragment length and overlap. Fragment length, overlap, and splitting by tokens or characters are decisions that should be made in tandem with the choice of LLM model, and with the number of fragments that should be retrieved for each query. Most importantly, the total text length should not exceed the input context length of the model. Here, we choose to inject 10 fragments per query.</p> <p>We can now start a new conversation (using the <code>New Persona</code> button in the sidebar), for which we select the <code>Research manuscript helper (RAG)</code> persona, which includes suitable contextual prompts. Upon activating the RAG in the chat (to signal that we wish to perform RAG for our question), we can enter into a dialogue with the assistant. We use <code>gpt-3.5-turbo-0613</code> in this example (deprecated as of July 16 2024), which is a very affordable model. The procedure is demonstrated in the GIF below.</p> <p></p>"},{"location":"BioChatter/vignettes/rag/#comparison-with-chatgpt","title":"Comparison with ChatGPT","text":"<p>We can ask the same question to ChatGPT 4 (only subscription access).  By employing web search, ChatGPT 4 is able to find the same study that was the RAG result we were asked about in our follow-up question (Xu et al. 2015); however, the answer is not as detailed as the answer provided by the RAG process, and includes fewer mentions of explicit studies. While it includes sensible information, there is no way of validating most of the claims made by the model. You can read the full answer here. Overall, the quality of ChatGPT 4 is comparable to the RAG output, but the user has less control over the knowledge used, and the answer cannot include information that is not easily found on the web (for instance due to access restrictions). Of note, we used <code>gpt-3.5-turbo-0613</code> in this example, which is less recent and much less expensive than GPT-4.</p> <p>Asking the naive ChatGPT 3.5 (free access), the same question yields a very different answer. The free version of ChatGPT cannot search the internet, and does not provide any citations for the general answer it provides. You can read the full answer here. For convenience, all three conversations are copied below.</p>"},{"location":"BioChatter/vignettes/rag/#conclusion","title":"Conclusion","text":"<p>In conclusion, the RAG procedure we implement is able to dramatically improve the quality of GPT-3 answers, making them comparable to GPT-4 with web search, and even surpassing it in some aspects. We invite you to try it out yourself, as we provide an instance of BioChatter Next, including GPT access and a vector database instance, free of charge at https://next.biochatter.org.</p>"},{"location":"BioChatter/vignettes/rag/#conversations","title":"Conversations","text":""},{"location":"BioChatter/vignettes/rag/#biochatter-rag","title":"BioChatter RAG","text":""},{"location":"BioChatter/vignettes/rag/#contextual-prompt","title":"Contextual Prompt","text":"<p>You are an assistant to a biomedical researcher.</p>"},{"location":"BioChatter/vignettes/rag/#contextual-prompt_1","title":"Contextual Prompt","text":"<p>You have access to text fragments from one or multiple scientific manuscripts, which were retrieved by their semantic similarity to the user's question.</p>"},{"location":"BioChatter/vignettes/rag/#contextual-prompt_2","title":"Contextual Prompt","text":"<p>Your task is to help the user understand the manuscripts and their relevance to their questions. Be concise in your explanations, provide quotes from the manuscripts where appropriate, and be open about limitations in cases where the text does not contain pertinent information.</p>"},{"location":"BioChatter/vignettes/rag/#message-from-you","title":"Message From You","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"BioChatter/vignettes/rag/#message-from-biochatter","title":"Message From BioChatter","text":"<p>Based on the provided information, there are several studies that suggest a potential role of the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein in resistance to ovarian cancer. Here are some key findings from the referenced manuscripts:</p> <ol> <li> <p>Gene expression profiling of ovarian tumors before and after chemotherapy revealed molecular signatures of chemoresistant tumors (M, et al., 2006). This study did not specifically mention CFTR, but it provides insights into the molecular changes associated with chemoresistance.</p> </li> <li> <p>ABCF2 expression, which is related to CFTR, was investigated in clear cell adenocarcinoma of the ovary. The study found a relationship between ABCF2 expression and response to chemotherapy or prognosis (Tsuda H, et al., 2010).</p> </li> <li> <p>Alterations in gene copy numbers, including CFTR, were identified as a mechanism for acquired drug resistance in cancer (Yasui K, et al., 2004).</p> </li> <li> <p>ABCF2, an Nrf2 target gene related to CFTR, was found to contribute to cisplatin resistance in ovarian cancer cells (Bao L, et al., 2017).</p> </li> <li> <p>CFTR was not specifically mentioned in the context of ovarian cancer, but a study on hepatocellular carcinoma found that silencing circ-TCF4.85 inhibited cancer progression through microRNA-486-5p-targeted inhibition of ABCF2 (Gao J, et al., 2020).</p> </li> <li> <p>Constitutive activation of nuclear factor \u03baB (NF-\u03baB) was associated with CFTR expression and promoted cancer progression and poor prognosis in cervical cancer (Wu Z, et al., 2013).</p> </li> <li> <p>High levels of CFTR expression were associated with tumor aggression in ovarian cancer, and knockdown of CFTR suppressed ovarian cancer cell proliferation (Xu J, et al., 2015).</p> </li> </ol> <p>It is important to note that while these studies suggest a potential association between CFTR and resistance to ovarian cancer, the exact mechanisms and clinical implications are still being investigated. Further research is needed to fully understand the role of CFTR in ovarian cancer resistance.</p>"},{"location":"BioChatter/vignettes/rag/#message-from-you_1","title":"Message From You","text":"<p>Tell me more about the studies of aggression and knockdown of CFTR in vivo.</p>"},{"location":"BioChatter/vignettes/rag/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided information, there are two relevant studies regarding the aggression and knockdown of CFTR in ovarian cancer:</p> <ol> <li> <p>The study by Wu et al. (2013) investigated the constitutive activation of nuclear factor \u03baB (NF-\u03baB) and its contribution to CFTR expression, cancer progression, and poor prognosis in cervical cancer. Although this study is not specific to ovarian cancer, it provides insights into the role of CFTR and NF-\u03baB in promoting cancer progression.</p> </li> <li> <p>The study by Xu et al. (2015) focused specifically on ovarian cancer and found that high levels of CFTR expression were associated with tumor aggression. They also demonstrated that knockdown of CFTR suppressed the proliferation of ovarian cancer cells in vitro and in vivo. This study suggests that CFTR may play a role in promoting ovarian cancer growth.</p> </li> </ol> <p>These studies provide evidence for the involvement of CFTR in cancer aggression and suggest that targeting CFTR could be a potential therapeutic strategy. However, it is important to note that these studies focus on different types of cancer (cervical and ovarian), and the specific mechanisms underlying CFTR's role in ovarian cancer aggression and its knockdown effects are not fully elucidated.</p> <p>It is worth mentioning that the other references provided in the text fragments are not directly related to the aggression and knockdown of CFTR in ovarian cancer. They discuss topics such as the structure and function of ABC transporters, drug resistance, and inflammation in different contexts.</p>"},{"location":"BioChatter/vignettes/rag/#chatgpt-4","title":"ChatGPT 4","text":""},{"location":"BioChatter/vignettes/rag/#anonymous","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"BioChatter/vignettes/rag/#chatgpt","title":"ChatGPT","text":"<p>The CFTR protein, known as the cystic fibrosis transmembrane conductance regulator, plays a significant role in ovarian cancer. Research has shown that CFTR is expressed more in ovarian cancer tissues compared to benign ovarian tumors and normal ovaries. This overexpression of CFTR is associated with more advanced stages of ovarian cancer, poor histopathological grade, and higher serum Ca-125 levels, which are all markers of more aggressive disease. Interestingly, the serous type of ovarian cancer shows stronger CFTR staining compared to other types.</p> <p>Moreover, the role of CFTR extends beyond its traditional function as an ion channel. It interacts with various intracellular signaling pathways that are crucial in cancer progression. For example, in ovarian cancer, CFTR overexpression is linked to interactions with the c-Src signaling pathway, which is involved in cellular growth.</p> <p>The implications of CFTR expression in ovarian cancer are significant. In studies where CFTR was downregulated, there was a decrease in cell invasion, migration, proliferation, adhesion, and colony formation in vitro. In vivo experiments also showed that the tumorigenic abilities of ovarian cancer cells were significantly repressed when CFTR expression was reduced.</p> <p>These findings highlight the complex role of CFTR in ovarian cancer and suggest that it could be a potential target for therapeutic intervention. However, it is important to note that the role of CFTR in cancer is multifaceted and varies among different types of cancers. Therefore, a detailed understanding of its function in specific cancer types, like ovarian cancer, is crucial for developing targeted treatments\u200b\u200b\u200b\u200b (https://pubmed.ncbi.nlm.nih.gov/25738998/, https://jmhg.springeropen.com/articles/10.1186/s43042-022-00308-7).</p>"},{"location":"BioChatter/vignettes/rag/#chatgpt-35","title":"ChatGPT 3.5","text":""},{"location":"BioChatter/vignettes/rag/#anonymous_1","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"BioChatter/vignettes/rag/#chatgpt_1","title":"ChatGPT","text":"<p>As of my last knowledge update in January 2022, there is limited direct information about the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein's role in resistance to ovarian cancer. CFTR is primarily associated with cystic fibrosis, a genetic disorder that affects the lungs and other organs. However, some research suggests potential links between CFTR and cancer, including ovarian cancer, although the mechanisms are not fully understood.</p> <p>Here are some key points to consider:</p> <p>CFTR Expression: Studies have shown that CFTR is expressed in various tissues, including the female reproductive system. Abnormal CFTR expression or function could potentially impact the development or progression of ovarian cancer. Research is ongoing to understand the role of CFTR in cancer biology. CFTR Mutations: Certain mutations in the CFTR gene have been associated with an increased risk of cancer, although the evidence for ovarian cancer specifically is limited. Mutations in CFTR are more commonly known for their role in cystic fibrosis. Individuals with cystic fibrosis have an increased risk of various cancers, including gastrointestinal cancers, but the link to ovarian cancer is not well-established. Ion Transport and Cancer: CFTR is involved in ion transport across cell membranes, and this function could potentially influence cellular processes relevant to cancer. Changes in ion transport can affect cell proliferation, apoptosis, and other cellular functions that are important in cancer development and progression. Potential Therapeutic Target: Some research suggests that modulating CFTR activity or expression could be a potential target for cancer therapy, including ovarian cancer. However, more research is needed to determine the feasibility and effectiveness of such approaches. It's important to note that the understanding of the role of CFTR in cancer, including ovarian cancer, is an evolving field, and new research findings may have emerged since my last update in January 2022. To get the most up-to-date information on this topic, I recommend consulting recent scientific literature and speaking with medical experts or researchers actively working in the field of ovarian cancer and CFTR biology.</p>"},{"location":"BioChatter/vignettes/rag/#references","title":"References","text":"<ul> <li> <p>[1] ABC transporters affects tumor immune microenvironment to regulate cancer immunotherapy and multidrug resistance</p> </li> <li> <p>[2] ABC transporters and the hallmarks of cancer: roles in cancer aggressiveness beyond multidrug resistance</p> </li> <li> <p>[3] Advances in the structure, mechanism and targeting of chemoresistance-linked ABC transporters</p> </li> <li> <p>[4] ATP-binding cassette (ABC) transporters in cancer: A review of recent updates</p> </li> </ul>"},{"location":"BioCypher/","title":"Democratising Knowledge Graphs","text":"Building a biomedical knowledge graph often takes months or even years.         What if you could do it in just weeks or days?      <p>We created BioCypher to revolutionise the process\u2014making it easier than ever while maintaining flexibility and transparency.</p> <p>At its core, BioCypher is designed around the principle of threefold modularity:</p> <ol> <li>Modular data sources \u2013 Seamlessly integrate diverse biomedical datasets.</li> <li>Modular ontology structures \u2013 Define flexible, structured knowledge representations.</li> <li>Modular output formats \u2013 Adapt results to various applications and tools.</li> </ol> <p>This modular approach maximises flexibility and reusability, empowering the biomedical community to accelerate research while streamlining efforts.</p> <p>Hot Topics</p> <p>BioCypher is the simplest way to create an AI-enabled knowledge graph for biomedical (or other) tasks. See BioCypher + LLMs for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models\". You can read it here.</p>"},{"location":"BioCypher/#new-to-knowledge-graphs","title":"New to Knowledge Graphs?","text":"<p>If you\u2019re new to knowledge graphs and want to familiarise with the concepts that drive BioCypher, we recommend to check out the graphical abstract below and read our paper (self-archived version here on Zenodo, online version at this https link)!</p>"},{"location":"BioCypher/#abstract","title":"Abstract","text":"<p> BioCypher simplifies biomedical data integration with reusable adapters, minimizing redundancy and ensuring consistency through ontologies. Its intuitive low-code setup enables effortless creation and sharing of reproducible knowledge graphs.</p>"},{"location":"BioCypher/#additional-resources","title":"Additional Resources","text":"<ul> <li> Questions? Join our Zulip channel</li> <li> Find us on GitHub</li> </ul>"},{"location":"BioCypher/installation/","title":"Installation guide","text":"<p>Before diving into developing wonderful use cases with BioCypher, we strongly recommend installing a few prerequisites to ensure a smooth experience. These prerequisites are:</p> <ol> <li>Python 3 (version &gt;= 3.10)<ul> <li>Install Python 3</li> </ul> </li> <li>Poetry (Python packaging and dependency manager)<ul> <li>Install Poetry</li> </ul> </li> <li>git (version control manager)<ul> <li>Install git</li> </ul> </li> <li>Docker (containerization technology) [optional]<ul> <li>Install Docker</li> </ul> </li> </ol> <p>Tip</p> <p>If you are missing any of those pre-requisites, please follow the installation guide in each resource before you continue.</p>"},{"location":"BioCypher/installation/#checking-prerequisites","title":"Checking prerequisites","text":"<p>You can verify access to these components in your terminal:</p> <ol> <li><code>python</code> version 3.10 or higher.    <pre><code>python --version\n</code></pre></li> <li><code>poetry</code> <pre><code>poetry --version\n</code></pre></li> <li><code>git</code> <pre><code>git --version\n</code></pre></li> <li><code>docker</code> <pre><code>docker --version\n</code></pre></li> </ol>"},{"location":"BioCypher/installation/#option-1-use-a-project-pre-configured-with-biocypher","title":"Option 1. Use a project pre-configured with BioCypher","text":"<p>The easiest way to start using BioCypher is with a pre-configured project that includes all the essential code, dependencies, environment settings, and the recommended directory structure. This setup allows you to focus solely on implementing your use case, with minimal modifications to a few existing files, depending on your needs. If this approach suits you, follow the instructions below to get started.</p> GitHub repo from template (recommended)Cloning the project template directly <p>Step  1: Go to Biocypher project template repository, click on \"Use this template\", then click on \"Create a new repository\".</p> <p></p> <p>Step 2: Complete the information on owner (GitHub username or organisation), name, description, and visibility of your repository.</p> <p></p> <p>Step 3:  Now, you can clone the repository and navigate into it. Replace user name (alternatively, GitHub organisation) and repository name with your own.</p> <pre><code>git clone https://github.com/&lt;your-github-username&gt;/&lt;your-project-name&gt;.git\ncd &lt;your-project-name&gt;\n</code></pre> <p>Step 4: Open the <code>pyproject.toml</code> file, change the following sections, and do not forget to save your changes.</p> <ul> <li><code>name</code>: replace the default project's name (<code>biocypher-project-template</code>) with the name you want to use for your project. This name does not need to be equal to the repository name (but often it is).</li> <li><code>description</code>: change the default description to a meaningful one based on your use case.</li> </ul> <p>Step 5: Install the dependencies using Poetry.</p> <pre><code>poetry install --no-root\n</code></pre> <p>Step 6: Run the script <code>create_knowledge_graph.py</code></p> <pre><code>poetry run python create_knowledge_graph.py\n</code></pre> <p>Step  1: Clone the project template repository, rename it, and navigate to the project folder.</p> <pre><code>git clone https://github.com/biocypher/project-template.git\nmv project-template my-knowledge-graph-project\ncd my-project\n</code></pre> <p>Step  2: Make the repository your own.</p> <pre><code>rm -rf .git\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n# (you can add your remote repository here)\n</code></pre> <p>Step 3: Open the <code>pyproject.toml</code> file, change the following sections, and do not forget to save your changes.</p> <ul> <li><code>name</code>: replace the default project's name (<code>biocypher-project-template</code>) with the name you want to use for your project. This name does not need to be equal to the repository name (but often it is).</li> <li><code>description</code>: change the default description to a meaningful one based on your use case.</li> </ul> <p>Step 4: Install the dependencies using Poetry.</p> <pre><code>poetry install --no-root\n</code></pre> <p>Step 5: Run the script <code>create_knowledge_graph.py</code></p> <pre><code>poetry run python create_knowledge_graph.py\n</code></pre>"},{"location":"BioCypher/installation/#option-2-install-from-a-package-manager","title":"Option 2. Install from a Package Manager","text":"poetry (recommended)pip <p>Note: about Poetry</p> <p>Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and will manage (install/update) them for you. Poetry offers a lock file to ensure reproducible environments and allows you to easily build your project for distribution. For information about the installation process, see here.</p> <pre><code># Create a new Poetry project, i.e. my-awesome-kg-project.\npoetry new &lt;name-of-the-project&gt;\n\n# Navigate into the recently created folder's project\ncd &lt;name-of-the-project&gt;\n\n# Install the BioCypher package with all the dependencies automatically\npoetry add biocypher\n</code></pre> <p>Note: Virtual environment and best practices</p> <p>To follow best practices in software engineering and prevent issues with your Python installation, we highly recommend installing packages in a separate virtual environment instead of directly in the base Python installation.</p> <ol> <li> <p>Create and activate a virtual environment. Replace <code>&lt;name-of-environment&gt;</code> with the name of the environment you desire, i.e. <code>biocypher_env</code></p> condavenv <pre><code># Create a conda environment with Python 3.10\nconda create --name &lt;name-of-environment&gt; python=3.10\n\n# Activate the new created environment\nconda activate &lt;name-of-environment&gt;\n</code></pre> <pre><code># Create a virtualenv environment\npython3 -m venv &lt;name-of-environment&gt;\n\n# Activate the new created environment\nsource ./&lt;name-of-environment&gt;/bin/activate\n</code></pre> </li> <li> <p>Install BioCypher package from <code>pip</code>. Type the following command to install BioCypher package. Note: do not forget to activate a virtual environment before do it.</p> <pre><code>pip install biocypher\n</code></pre> </li> </ol>"},{"location":"BioCypher/installation/#option-3-docker","title":"Option 3. Docker","text":"<p>Play with your data in Neo4j with this Docker container</p> <p>The project template includes a Docker compose workflow that allows to:</p> <ol> <li>Create an example database using BioCypher.</li> <li>Load the data into a dockerized Neo4j instance automatically.</li> </ol> <p>Once you have created your project using one of the template options (option 1), please follow the steps below:</p> <p>Step 1: Start a single detached Docker container running a Neo4j instance, which contains the knowledge graph built by BioCypher as the default Neo4j database.</p> <pre><code>docker compose up -d\n</code></pre> <p>Step 2: Open the Neo4j instance in a web browser by typing the address and port: localhost:7474.</p> <p>Authentication is deactivated by default and can be modified in the <code>docker_variables.env</code> file (in which case you need to provide the .env file to the deploy stage of the <code>docker-compose.yml</code>).</p>"},{"location":"BioCypher/installation/#docker-workflow","title":"Docker Workflow","text":"<p>The Docker Compose file creates three containers: build, import, and deploy. These containers share files using a Docker Volume. In the BioCypher build procedure, the <code>biocypher_docker_config.yaml</code> file is used instead of <code>biocypher_config.yaml</code>, as specified in <code>scripts/build.sh</code>.</p> <ul> <li>Containers and their functions<ul> <li>build: Installs and runs the BioCypher pipeline.</li> <li>import: installs Neo4j and executes the data import.</li> <li>deploy: deploys the Neo4j instance on localhost.</li> </ul> </li> </ul> <p>This three-stage setup strictly is not necessary for the mounting of a read-write instance of Neo4j, but is required if the purpose is to provide a read-only instance (e.g. for a web app) that is updated regularly;for an example, see the meta graph repository. The read-only setting is configured in the docker-compose.yml file (<code>NEO4J_dbms_databases_default__to__read__only: \"false\"</code>) and is deactivated by default.</p>"},{"location":"BioCypher/installation/#for-developers","title":"For Developers","text":"<p>If you want to directly install BioCypher, here are the steps (requires Poetry):</p> <p>Execute in bash<pre><code>git clone https://github.com/biocypher/biocypher\ncd BioCypher\npoetry install\n</code></pre> Poetry creates a virtual environment for you (starting with <code>biocypher-</code>; alternatively you can name it yourself) and installs all dependencies.</p> <p>If you want to run the tests that use a local Neo4j or PostgreSQL DBMS (database management system) instance:</p> <ul> <li> <p>Make sure that you have a Neo4j instance with the APOC plugin installed and a database named <code>test</code> running on standard bolt port <code>7687</code></p> </li> <li> <p>A PostgreSQL instance with the psql command line tool should be installed locally and running on standard port <code>5432</code></p> </li> <li> <p>Activate the virtual environment by running <code>poetry shell</code> and then run the tests by running <code>pytest</code> in the root directory of the repository with the command line argument <code>--password=&lt;your DBMS password&gt;</code>.</p> </li> </ul> <p>Once this is set up, you can go through the tutorial or use it in your project as a local dependency.</p>"},{"location":"BioCypher/biocypher-project/biochatter-integration/","title":"BioChatter (LLM on Knowledge Graphs)","text":""},{"location":"BioCypher/biocypher-project/biochatter-integration/#connect-your-knowledge-graph-to-large-language-models","title":"Connect your Knowledge Graph to Large Language Models","text":"<p>BioChatter is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this paper. BioChatter is part of the BioCypher ecosystem, connecting natively to BioCypher knowledge graphs. Find the docs here.</p>"},{"location":"BioCypher/biocypher-project/biochatter-integration/#abstract","title":"Abstract","text":"<ul> <li> <p> BioChatter Preview Web Apps</p> <p> To BioChatter Web</p> </li> <li> <p> BioChatter Repository</p> <p> To BioChatter Repository</p> </li> </ul>"},{"location":"BioCypher/biocypher-project/design-philosophy/","title":"BioCypher design philosophy","text":"<p>At its core, BioCypher is designed around the principle of threefold modularity:</p> <ol> <li>Modular data sources \u2013 Seamlessly integrate diverse biomedical datasets.</li> <li>Modular ontology structures \u2013 Define flexible, structured knowledge representations.</li> <li>Modular output formats \u2013 Adapt results to various applications and tools.</li> </ol>"},{"location":"BioCypher/biocypher-project/design-philosophy/#design-principles","title":"Design Principles","text":""},{"location":"BioCypher/biocypher-project/design-philosophy/#1-modular-data-sources","title":"1. Modular data sources","text":""},{"location":"BioCypher/biocypher-project/design-philosophy/#resources","title":"Resources","text":"<p>Resources are diverse data inputs and sources that feed into the knowledge graph through \"adapters.\" A Resource could be a file, a list of files, an API request, or a list of API requests. BioCypher can download resources from a given URL, cache them, and manage their lifecycle.</p>"},{"location":"BioCypher/biocypher-project/design-philosophy/#adapters","title":"Adapters","text":"<p>BioCypher is a modular framework, with the main purpose of avoiding redundant maintenance work for maintainers of secondary resources and end users alike. To achieve this, we use a collection of reusable \"adapters\" for the different sources of biomedical knowledge as well as for different ontologies.</p>"},{"location":"BioCypher/biocypher-project/design-philosophy/#2-modular-ontology-structures","title":"2. Modular ontology structures","text":""},{"location":"BioCypher/biocypher-project/design-philosophy/#ontologies","title":"Ontologies","text":"<p>An ontology is a formal, hierarchical representation of knowledge within a specific domain, organizing concepts and their relationships. It structures concepts into subclasses of more general categories, such as a wardrobe being a subclass of furniture. BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required.</p> <p>Philosophically, a lot has changed since the introduction of current-generation large language models (LLMs). For instance, LLMs bring a sophisticated world model without explicitly modelling concepts, which is in stark contrast to the modelling decisions of traditional ontologies. We need to critically re-evaluate the future role of ontologies in the modern scientific knowledge management ecosystem. They provide valuable context via the thousands of hours of human curation, but they also come with many intricacies and inconsistencies.</p> <p>Our Philosophy</p> <p>BioCypher aims to disrupt the traditional workflow to boost knowledge management into the AI era. While we hope to preserve the benefits of human curation, we also want to critically re-evaluate the role of all parts of the knowledge representation pipeline.</p>"},{"location":"BioCypher/biocypher-project/design-philosophy/#3-modular-output-formats","title":"3. Modular output formats","text":""},{"location":"BioCypher/biocypher-project/design-philosophy/#outputs","title":"Outputs","text":"<p>Initially focused on Neo4j due to OmniPath's migration, BioCypher now supports multiple output formats, including RDF, SQL, ArangoDB, CSV, PostgreSQL, SQLite, and NetworkX, specified via the dbms parameter in the <code>biocypher_config.yaml</code> file. Users can choose between online mode (manipulation of a running database) or offline mode.</p>"},{"location":"BioCypher/biocypher-project/design-philosophy/#configuration","title":"Configuration","text":"<p>Configuration in BioCypher involves setting up and customizing the system to meet specific needs. BioCypher provides default configuration parameters, which can be overridden by creating a <code>biocypher_config.yaml</code> file in your project's root or config directory, specifying the parameters you wish to change.</p>"},{"location":"BioCypher/biocypher-project/project/","title":"BioCypher Project","text":""},{"location":"BioCypher/biocypher-project/project/#our-mission","title":"Our Mission","text":"<p>We aim to enable access to versatile and powerful knowledge graphs for as many researchers as possible. Making biomedical knowledge \u201ctheir own\u201d is often a privilege of the companies and groups that can afford individuals or teams working on knowledge representation in a dedicated manner. With BioCypher, we aim to change that. Creating a knowledge graph should be \u201cas simple as possible, but not any simpler.\u201d To achieve this, we have developed a framework that facilitates the creation of knowledge graphs that are informed by the latest developments in the field of biomedical knowledge representation. However, to make this framework truly accessible and comprehensive, we need the input of the biomedical community. We are therefore inviting you to join us in this endeavour!</p>"},{"location":"BioCypher/biocypher-project/project/#our-vision","title":"Our Vision","text":"<p>The machine learning models we train are only as good as the data they are trained on. However, most developments today still rely on manually engineered and non-reproducible data processing. We envision a future where the creation of knowledge graphs is as easy as running a script, enabling researchers to build reliable knowledge representations with up-to-date information. We believe that making the knowledge representation process more agile and lifting it to the same level of attention as the process of algorithm development will lead to more robust and reliable machine learning models. We are convinced that this will be a crucial step towards the democratisation of AI in biomedicine and beyond.</p>"},{"location":"BioCypher/biocypher-project/project/#timeline","title":"Timeline","text":"<p>BioCypher Launch</p> 2021-09-10<p>Start of BioCypher development.</p> <p>Preprint publication</p> 2022-12-27<p>BioCypher preprint v1 published on arXiv.</p> <p>BioChatter Launch</p> 2023-04-03<p>BioChatter start of development.</p> <p>Preprint publication</p> 2023-05-10<p>BioChatter preprint v1 published on arXiv.</p> <p>Journal publication</p> 2023-06-19<p>BioCypher peer-reviewed in Nature Biotechnology.</p> <p>Journal publication</p> 2025-01-22<p>BioChatter peer-reviewed in Nature Biotechnology.</p> <p>BioGather Launch</p> 2025-02-10<p>BioGather start of development.</p> <p>Preprint publication</p> Soon<p>BioGather preprint v1 published on arXiv.</p>"},{"location":"BioCypher/biocypher-project/publications/","title":"Publications","text":""},{"location":"BioCypher/biocypher-project/publications/#APBLLM_2025","title":"[2025] A platform for the biomedical application of large language models","text":"<p>Publisher: Nature Biotechnology</p> <p> Read the paper in Nature Biotechnology</p>"},{"location":"BioCypher/biocypher-project/publications/#MCAFM_2024","title":"[2024] Molecular causality in the advent of foundation models","text":"<p>Publisher: EMBO Molecular Systems Biology</p> <p> Read the paper in EMBO Molecular Systems Biology</p>"},{"location":"BioCypher/biocypher-project/publications/#DKRWB_2023","title":"[2023] Democratizing knowledge representation with BioCypher","text":"<p>Publisher: Nature Biotechnology</p> <p> Read the paper in Nature Biotechnology  Read the self-archived version on Zenodo  Read the online version</p>"},{"location":"BioCypher/biocypher-project/sponsors/","title":"Sponsors","text":""},{"location":"BioCypher/biocypher-project/sponsors/#institutional-partners","title":"Institutional partners","text":"<p>We are fortunate to have received support from the following institutions and companies:</p> <ul> <li>Heidelberg University and University Hospital Heidelberg</li> <li>German Research Foundation (DFG)</li> <li>Open Targets (EMBL-EBI)</li> <li>Helmholtz Association</li> <li>German Centre for Diabetes Research (DZD)</li> <li>GitHub</li> <li>Docker</li> <li>Zulip</li> </ul>"},{"location":"BioCypher/biocypher-project/sponsors/#become-a-sponsor","title":"Become a sponsor","text":"<p>As a free and open source project, Biocypher relies on the support of the community of users for its development. If you work for an organization that uses and benefits from BioCypher, please consider supporting us.</p>"},{"location":"BioCypher/biocypher-project/use-cases/","title":"Use Cases","text":""},{"location":"BioCypher/biocypher-project/use-cases/#how-researchers-are-leveraging-biocypher","title":"How Researchers Are Leveraging BioCypher","text":"<p>BioCypher has been instrumental in advancing research around Knowledge Graphs. Teams have primarily used BioCypher for:</p> <ol> <li> <p>Creation and maintenance of knowledge repositories (\"storage\"): Ensuring structured, scalable, and easily accessible data storage.</p> </li> <li> <p>Project-specific knowledge graph creation (\"analysis\"): Facilitating streamlined data integration and insightful analysis tailored to research needs.</p> </li> </ol> <p>We are excited to showcase some real-world use cases where BioCypher has made a significant impact. If BioCypher has helped your research, we\u2019d love to hear about it\u2014your use case could be the next one featured here!</p>"},{"location":"BioCypher/biocypher-project/use-cases/#a-knowledge-graph-for-impact-of-genomic-variation-on-function-igvf","title":"A Knowledge Graph for Impact of Genomic Variation on Function (IGVF)","text":"<ul> <li> <p>Impact of Genomic Variation on Function (IGVF)</p> <p>Description: The impact of Genomic Variation on Function (IGVF) project aims to provide a comprenhensive and integrated view of the impact of genomic variation on human health and disease.</p> <p>Resources: The BioCypher pipeline used to build the knowledge graph uses several adapters for genetics data sources; an overview is available in our meta-graph and on the GitHub Components Board (pipelines column). The pipeline boasts a Docker Compose workflow that builds the graph and the API (using tRPC), and is available on GitHub.</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"Our project, Impact of Genomic Variation on Function (IGVF, https://igvf.org), is building a massive biological knowledge graph to attempt to link human variation and disease with genomic datasets at the single-cell level. We are creating a user-facing API (and eventually UI) that will access this graph. BioCypher, which acts as an intermediary between Biolink and graph databases (we are using ArangoDB) has been instrumental in helping us design the schema and move our project forward. Specifically, it provides a framework we can use to parse the dozens of data files and formats into a Biolink-inspired schema\".</p> <ul> <li>Ben Hitz, Director of Genomics Data Resources, Project Manager ENCODE, Stanford University</li> </ul>"},{"location":"BioCypher/biocypher-project/use-cases/#drug-repurposing-with-crossbar","title":"Drug Repurposing with CROssBAR","text":"<ul> <li> <p>CROssBAR</p> <p>Description: CROssBAR is a biomedical data integration and representation project. CROssBAR knowledge graphs incorporate relevant genes-proteins, molecular interactions, pathways, phenotypes, diseases, as well as known/predicted drugs and bioactive compounds, and they are constructred on-the-fly based on simple non-programmatic user queries.</p> <p>Resources: Using BioCypher, CROssBAR v2 will be a flexible property graph database comprised of single input adapters for each data source. As above, you can see its current state in the meta-graph and on the GitHub Components Board (pipelines column).</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"We built CROssBAR v1 on NoSQL since property graph databases were quite new at the time and there was no framework to help us establish the system. We used an available NoSQL solution to house different layers of biological/biomedical data as independent collections. CROssBAR\u2019s \u201csmall-scale knowledge graph (KG) construction module\u201d queries each collection separately, collects the data, and merges the data points according to their mappings (which are held in the database as well, as cross-references), eliminates redundancy, queries each and every collection again with the entries retrieved in the previous step, and repeats all subsequent steps. Given that user queries can start with a single or multiple genes/proteins, compounds/drugs, diseases, phenotypes, pathways, or any combination of those, this procedure gets extremely complicated, requiring an average of 64 NoSQL queries to construct one single user-specific KG. The total number of lines of code required for this procedure alone is around 8000. This task could have been achieved significantly faster and more efficiently if we had had BioCypher five years ago\".</p> <ul> <li>Tunca Do\u011fan, Department of Computer Engineering and Artificial Intelligence Engineering, Hacettepe University and Protein Function Development Team (UniProt database), European Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI)</li> </ul>"},{"location":"BioCypher/biocypher-project/use-cases/#building-a-knowledge-graph-for-contextualised-metabolic-enzymatic-interactions","title":"Building a Knowledge Graph for Contextualised Metabolic-Enzymatic Interactions","text":"<ul> <li> <p>Metalinks</p> <p>Description: The metalinks project aims to build a knowledge graph for contextualised metabolic-enzymatic interactions.</p> <p>Resources: The BioCypher pipeline used to build the knowledge graph uses several adapters, some of which overlap with the CROssBAR project, which helps synergising maintenance efforts. An overview is available in our meta-graph and on the GitHub Components Board (pipelines column).</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"In the metalinks project, we build a knowledge graph (KG) that incorporates attributes of metabolites, proteins and their interactions to ultimately study cell-cell communication. We use two types of interaction between metabolites and proteins, I) production and degradation of metabolites by enzymes and II) interaction of metabolites with protein receptors. During the KG assembly we access multiple databases that provide information in diverse formats. BioCypher takes all of these inputs, gives them a reasonable, reproducible structure, and facilitates proper versioning. The KG produced by BioCypher can be easily contextualized to biological questions aiming for specific tissues, diseases or metabolite properties, which facilitates downstream analysis and interpretability. While spending 2.5 months to create a loose collection of scripts and directories for the initial project, I was able to obtain a structured result with BioCypher within 2 weeks.\".</p> <ul> <li>Elias Farr, Institute for Computational Biomedicine, University Hospital Heidelberg</li> </ul>"},{"location":"BioCypher/community/","title":"Join Us","text":"<p>Welcome to the BioCypher community! We follow open-source principles and encourage any sort of contribution. We communicate on GitHub, where we also organise our projects. For more informal discussions and updates, we have a Zulip channel. Everybody is welcome!</p> <ul> <li> <p> Where to Start</p> <p>If you'd like to learn how to contribute to our projects, please follow the steps outlined in the contribution guide.</p> <p> To the contribution guide</p> </li> </ul>"},{"location":"BioCypher/community/#our-community-zulip-channel","title":"Our Community Zulip Channel","text":"<p>Some channels are public, others can be seen after joining the Zulip community (free of charge).</p> <ul> <li><code>#announcements</code>: Stay informed about seminars, workshops, and other events.</li> <li><code>#help</code>: Ask questions and get help related to BioCypher ecosystem projects.</li> <li><code>#general</code>: You can introduce yourselves, discuss ideas and share your work.</li> <li><code>#development</code>: Exchange ideas about development of BioCypher ecosystem projects.</li> </ul> <p>Why Zulip? We chose Zulip because it is a modern, open-source alternative to Slack. It is organised by topics, which helps in keeping the discussions focused. It also is free of charge for open-source and academic projects, which means that they sponsor the cloud hosting for our channel (as for many other open-source projects) - thanks!</p> <p>Sign up here.</p>"},{"location":"BioCypher/community/#contributing-guidelines-github-links","title":"Contributing Guidelines GitHub Links","text":"<ul> <li> <p>Contribution guidelines</p> </li> <li> <p>Code of Conduct</p> </li> <li> <p>Developer Guide</p> </li> </ul>"},{"location":"BioCypher/community/biocypher-docstring-guide/","title":"BioCypher docstring guide","text":""},{"location":"BioCypher/community/biocypher-docstring-guide/#about-docstrings-and-standards","title":"About docstrings and standards","text":"<p>A Python docstring is a string used to document a Python module, class, function or method, so programmers can understand what it does without having to read the details of the implementation.</p> <p>Also, it is a common practice to generate online (html) documentation automatically from docstrings. <code>Sphinx &lt;https://www.sphinx-doc.org&gt;</code>_ serves this purpose.</p> <p>The next example gives an idea of what a docstring looks like:</p> <p>.. code-block:: python</p> <pre><code>def add(num1, num2):\n    \"\"\"\n    Add up two integer numbers.\n\n    This function simply wraps the ``+`` operator, and does not\n    do anything interesting, except for illustrating what\n    the docstring of a very simple function looks like.\n\n    Parameters\n    ----------\n    num1 : int\n        First number to add.\n    num2 : int\n        Second number to add.\n\n    Returns\n    -------\n    int\n        The sum of ``num1`` and ``num2``.\n\n    See Also\n    --------\n    subtract : Subtract one integer from another.\n\n    Examples\n    --------\n    &gt;&gt;&gt; add(2, 2)\n    4\n    &gt;&gt;&gt; add(25, 0)\n    25\n    &gt;&gt;&gt; add(10, -10)\n    0\n    \"\"\"\n    return num1 + num2\n</code></pre> <p>Some standards regarding docstrings exist, which make them easier to read, and allow them be easily exported to other formats such as html or pdf.</p> <p>The first conventions every Python docstring should follow are defined in <code>PEP-257 &lt;https://www.python.org/dev/peps/pep-0257/&gt;</code>_.</p> <p>As PEP-257 is quite broad, other more specific standards also exist. In the case of biocypher, the NumPy docstring convention is followed. These conventions are explained in this document:</p> <ul> <li><code>numpydoc docstring guide &lt;https://numpydoc.readthedocs.io/en/latest/format.html&gt;</code>_</li> </ul> <p>numpydoc is a Sphinx extension to support the NumPy docstring convention.</p> <p>The standard uses reStructuredText (reST). reStructuredText is a markup language that allows encoding styles in plain text files. Documentation about reStructuredText can be found in:</p> <ul> <li><code>Sphinx reStructuredText primer &lt;https://www.sphinx-doc.org/en/stable/rest.html&gt;</code>_</li> <li><code>Quick reStructuredText reference &lt;https://docutils.sourceforge.io/docs/user/rst/quickref.html&gt;</code>_</li> <li><code>Full reStructuredText specification &lt;https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html&gt;</code>_</li> </ul> <p>biocypher has some helpers for sharing docstrings between related classes, see :ref:<code>docstring.sharing</code>.</p> <p>The rest of this document will summarize all the above guidelines, and will provide additional conventions specific to the biocypher project.</p> <p>.. _docstring.tutorial:</p>"},{"location":"BioCypher/community/biocypher-docstring-guide/#writing-a-docstring","title":"Writing a docstring","text":"<p>.. _docstring.general:</p> <p>General rules ~~~~~~~~~~~~~</p> <p>Docstrings must be defined with three double-quotes. No blank lines should be left before or after the docstring. The text starts in the next line after the opening quotes. The closing quotes have their own line (meaning that they are not at the end of the last sentence).</p> <p>On rare occasions reST styles like bold text or italics will be used in docstrings, but is it common to have inline code, which is presented between backticks. The following are considered inline code:</p> <ul> <li>The name of a parameter</li> <li>Python code, a module, function, built-in, type, literal... (e.g. <code>os</code>,   <code>list</code>, <code>numpy.abs</code>, <code>datetime.date</code>, <code>True</code>)</li> <li>A biocypher class (in the form <code>`:class:</code>biocypher.Series```)</li> <li>A biocypher method (in the form <code>`:meth:</code>biocypher.Series.sum```)</li> <li>A biocypher function (in the form <code>`:func:</code>biocypher.to_datetime```)</li> </ul> <p>.. note::     To display only the last component of the linked class, method or     function, prefix it with <code>~</code>. For example, <code>:class:`~biocypher.Series```     will link to</code>biocypher.Series<code>but only display the last part,</code>Series<code>`     as the link text. See</code>Sphinx cross-referencing syntax     https://www.sphinx-doc.org/en/stable/domains.html#cross-referencing-syntax`_     for details.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>def add_values(arr):\n    \"\"\"\n    Add the values in ``arr``.\n\n    This is equivalent to Python ``sum`` of :meth:`biocypher.Series.sum`.\n\n    Some sections are omitted here for simplicity.\n    \"\"\"\n    return sum(arr)\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def func():\n\n    \"\"\"Some function.\n\n    With several mistakes in the docstring.\n\n    It has a blank like after the signature ``def func():``.\n\n    The text 'Some function' should go in the line after the\n    opening quotes of the docstring, not in the same line.\n\n    There is a blank line between the docstring and the first line\n    of code ``foo = 1``.\n\n    The closing quotes should be in the next line, not in this one.\"\"\"\n\n    foo = 1\n    bar = 2\n    return foo + bar\n</code></pre> <p>.. _docstring.short_summary:</p> <p>Section 1: short summary ~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The short summary is a single sentence that expresses what the function does in a concise way.</p> <p>The short summary must start with a capital letter, end with a dot, and fit in a single line. It needs to express what the object does without providing details. For functions and methods, the short summary must start with an infinitive verb.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type.\n\n    This section will provide further details.\n    \"\"\"\n    pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Casts Series type.\n\n    Verb in third-person of the present simple, should be infinitive.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Method to cast Series type.\n\n    Does not start with verb.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type\n\n    Missing dot at the end.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type from its current type to the new type defined in\n    the parameter dtype.\n\n    Summary is too verbose and doesn't fit in a single line.\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.extended_summary:</p> <p>Section 2: extended summary ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The extended summary provides details on what the function does. It should not go into the details of the parameters, or discuss implementation notes, which go in other sections.</p> <p>A blank line is left between the short summary and the extended summary. Every paragraph in the extended summary ends with a dot.</p> <p>The extended summary should provide details on why the function is useful and their use cases, if it is not too generic.</p> <p>.. code-block:: python</p> <pre><code>def unstack():\n    \"\"\"\n    Pivot a row index to columns.\n\n    When using a MultiIndex, a level can be pivoted so each value in\n    the index becomes a column. This is especially useful when a subindex\n    is repeated for the main index, and data is easier to visualize as a\n    pivot table.\n\n    The index level will be automatically removed from the index when added\n    as columns.\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.parameters:</p> <p>Section 3: parameters ~~~~~~~~~~~~~~~~~~~~~</p> <p>The details of the parameters will be added in this section. This section has the title \"Parameters\", followed by a line with a hyphen under each letter of the word \"Parameters\". A blank line is left before the section title, but not after, and not between the line with the word \"Parameters\" and the one with the hyphens.</p> <p>After the title, each parameter in the signature must be documented, including <code>*args</code> and <code>**kwargs</code>, but not <code>self</code>.</p> <p>The parameters are defined by their name, followed by a space, a colon, another space, and the type (or types). Note that the space between the name and the colon is important. Types are not defined for <code>*args</code> and <code>**kwargs</code>, but must be defined for all other parameters. After the parameter definition, it is required to have a line with the parameter description, which is indented, and can have multiple lines. The description must start with a capital letter, and finish with a dot.</p> <p>For keyword arguments with a default value, the default will be listed after a comma at the end of the type. The exact form of the type in this case will be \"int, default 0\". In some cases it may be useful to explain what the default argument means, which can be added after a comma \"int, default -1, meaning all cpus\".</p> <p>In cases where the default value is <code>None</code>, meaning that the value will not be used. Instead of <code>\"str, default None\"</code>, it is preferred to write <code>\"str, optional\"</code>. When <code>None</code> is a value being used, we will keep the form \"str, default None\". For example, in <code>df.to_csv(compression=None)</code>, <code>None</code> is not a value being used, but means that compression is optional, and no compression is being used if not provided. In this case we will use <code>\"str, optional\"</code>. Only in cases like <code>func(value=None)</code> and <code>None</code> is being used in the same way as <code>0</code> or <code>foo</code> would be used, then we will specify \"str, int or None, default None\".</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self, kind, color='blue', **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Parameters\n        ----------\n        kind : str\n            Kind of matplotlib plot.\n        color : str, default 'blue'\n            Color name or rgb code.\n        **kwargs\n            These parameters will be passed to the matplotlib plotting\n            function.\n        \"\"\"\n        pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self, kind, **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Note the blank line between the parameters title and the first\n        parameter. Also, note that after the name of the parameter ``kind``\n        and before the colon, a space is missing.\n\n        Also, note that the parameter descriptions do not start with a\n        capital letter, and do not finish with a dot.\n\n        Finally, the ``**kwargs`` parameter is missing.\n\n        Parameters\n        ----------\n\n        kind: str\n            kind of matplotlib plot\n        \"\"\"\n        pass\n</code></pre> <p>.. _docstring.parameter_types:</p> <p>Parameter types ^^^^^^^^^^^^^^^</p> <p>When specifying the parameter types, Python built-in data types can be used directly (the Python type is preferred to the more verbose string, integer, boolean, etc):</p> <ul> <li>int</li> <li>float</li> <li>str</li> <li>bool</li> </ul> <p>For complex types, define the subtypes. For <code>dict</code> and <code>tuple</code>, as more than one type is present, we use the brackets to help read the type (curly brackets for <code>dict</code> and normal brackets for <code>tuple</code>):</p> <ul> <li>list of int</li> <li>dict of {str : int}</li> <li>tuple of (str, int, int)</li> <li>tuple of (str,)</li> <li>set of str</li> </ul> <p>In case where there are just a set of values allowed, list them in curly brackets and separated by commas (followed by a space). If the values are ordinal and they have an order, list them in this order. Otherwise, list the default value first, if there is one:</p> <ul> <li>{0, 10, 25}</li> <li>{'simple', 'advanced'}</li> <li>{'low', 'medium', 'high'}</li> <li>{'cat', 'dog', 'bird'}</li> </ul> <p>If the type is defined in a Python module, the module must be specified:</p> <ul> <li>datetime.date</li> <li>datetime.datetime</li> <li>decimal.Decimal</li> </ul> <p>If the type is in a package, the module must be also specified:</p> <ul> <li>numpy.ndarray</li> <li>scipy.sparse.coo_matrix</li> </ul> <p>If the type is a biocypher type, also specify biocypher except for Series and DataFrame:</p> <ul> <li>Series</li> <li>DataFrame</li> <li>biocypher.Index</li> <li>biocypher.Categorical</li> <li>biocypher.arrays.SparseArray</li> </ul> <p>If the exact type is not relevant, but must be compatible with a NumPy array, array-like can be specified. If Any type that can be iterated is accepted, iterable can be used:</p> <ul> <li>array-like</li> <li>iterable</li> </ul> <p>If more than one type is accepted, separate them by commas, except the last two types, that need to be separated by the word 'or':</p> <ul> <li>int or float</li> <li>float, decimal.Decimal or None</li> <li>str or list of str</li> </ul> <p>If <code>None</code> is one of the accepted values, it always needs to be the last in the list.</p> <p>For axis, the convention is to use something like:</p> <ul> <li>axis : {0 or 'index', 1 or 'columns', None}, default None</li> </ul> <p>.. _docstring.returns:</p> <p>Section 4: returns or yields ~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>If the method returns a value, it will be documented in this section. Also if the method yields its output.</p> <p>The title of the section will be defined in the same way as the \"Parameters\". With the names \"Returns\" or \"Yields\" followed by a line with as many hyphens as the letters in the preceding word.</p> <p>The documentation of the return is also similar to the parameters. But in this case, no name will be provided, unless the method returns or yields more than one value (a tuple of values).</p> <p>The types for \"Returns\" and \"Yields\" are the same as the ones for the \"Parameters\". Also, the description must finish with a dot.</p> <p>For example, with a single value:</p> <p>.. code-block:: python</p> <pre><code>def sample():\n    \"\"\"\n    Generate and return a random number.\n\n    The value is sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Returns\n    -------\n    float\n        Random number generated.\n    \"\"\"\n    return np.random.random()\n</code></pre> <p>With more than one value:</p> <p>.. code-block:: python</p> <pre><code>import string\n\ndef random_letters():\n    \"\"\"\n    Generate and return a sequence of random letters.\n\n    The length of the returned string is also random, and is also\n    returned.\n\n    Returns\n    -------\n    length : int\n        Length of the returned string.\n    letters : str\n        String of random letters.\n    \"\"\"\n    length = np.random.randint(1, 10)\n    letters = ''.join(np.random.choice(string.ascii_lowercase)\n                      for i in range(length))\n    return length, letters\n</code></pre> <p>If the method yields its value:</p> <p>.. code-block:: python</p> <pre><code>def sample_values():\n    \"\"\"\n    Generate an infinite sequence of random numbers.\n\n    The values are sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Yields\n    ------\n    float\n        Random number generated.\n    \"\"\"\n    while True:\n        yield np.random.random()\n</code></pre> <p>.. _docstring.see_also:</p> <p>Section 5: see also ~~~~~~~~~~~~~~~~~~~</p> <p>This section is used to let users know about biocypher functionality related to the one being documented. In rare cases, if no related methods or functions can be found at all, this section can be skipped.</p> <p>An obvious example would be the <code>head()</code> and <code>tail()</code> methods. As <code>tail()</code> does the equivalent as <code>head()</code> but at the end of the <code>Series</code> or <code>DataFrame</code> instead of at the beginning, it is good to let the users know about it.</p> <p>To give an intuition on what can be considered related, here there are some examples:</p> <ul> <li><code>loc</code> and <code>iloc</code>, as they do the same, but in one case providing indices   and in the other positions</li> <li><code>max</code> and <code>min</code>, as they do the opposite</li> <li><code>iterrows</code>, <code>itertuples</code> and <code>items</code>, as it is easy that a user   looking for the method to iterate over columns ends up in the method to   iterate over rows, and vice-versa</li> <li><code>fillna</code> and <code>dropna</code>, as both methods are used to handle missing values</li> <li><code>read_csv</code> and <code>to_csv</code>, as they are complementary</li> <li><code>merge</code> and <code>join</code>, as one is a generalization of the other</li> <li><code>astype</code> and <code>biocypher.to_datetime</code>, as users may be reading the   documentation of <code>astype</code> to know how to cast as a date, and the way to do   it is with <code>biocypher.to_datetime</code></li> <li><code>where</code> is related to <code>numpy.where</code>, as its functionality is based on it</li> </ul> <p>When deciding what is related, you should mainly use your common sense and think about what can be useful for the users reading the documentation, especially the less experienced ones.</p> <p>When relating to other libraries (mainly <code>numpy</code>), use the name of the module first (not an alias like <code>np</code>). If the function is in a module which is not the main one, like <code>scipy.sparse</code>, list the full module (e.g. <code>scipy.sparse.coo_matrix</code>).</p> <p>This section has a header, \"See Also\" (note the capital S and A), followed by the line with hyphens and preceded by a blank line.</p> <p>After the header, we will add a line for each related method or function, followed by a space, a colon, another space, and a short description that illustrates what this method or function does, why is it relevant in this context, and what the key differences are between the documented function and the one being referenced. The description must also end with a dot.</p> <p>Note that in \"Returns\" and \"Yields\", the description is located on the line after the type. In this section, however, it is located on the same line, with a colon in between. If the description does not fit on the same line, it can continue onto other lines which must be further indented.</p> <p>For example:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def head(self):\n        \"\"\"\n        Return the first 5 elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying the whole of it.\n\n        Returns\n        -------\n        Series\n            Subset of the original series with the 5 first values.\n\n        See Also\n        --------\n        Series.tail : Return the last 5 elements of the Series.\n        Series.iloc : Return a slice of the elements in the Series,\n            which can also be used to return the first or last n.\n        \"\"\"\n        return self.iloc[:5]\n</code></pre> <p>.. _docstring.notes:</p> <p>Section 6: notes ~~~~~~~~~~~~~~~~</p> <p>This is an optional section used for notes about the implementation of the algorithm, or to document technical aspects of the function behavior.</p> <p>Feel free to skip it, unless you are familiar with the implementation of the algorithm, or you discover some counter-intuitive behavior while writing the examples for the function.</p> <p>This section follows the same format as the extended summary section.</p> <p>.. _docstring.examples:</p> <p>Section 7: examples ~~~~~~~~~~~~~~~~~~~</p> <p>This is one of the most important sections of a docstring, despite being placed in the last position, as often people understand concepts better by example than through accurate explanations.</p> <p>Examples in docstrings, besides illustrating the usage of the function or method, must be valid Python code, that returns the given output in a deterministic way, and that can be copied and run by users.</p> <p>Examples are presented as a session in the Python terminal. <code>&gt;&gt;&gt;</code> is used to present code. <code>...</code> is used for code continuing from the previous line. Output is presented immediately after the last line of code generating the output (no blank lines in between). Comments describing the examples can be added with blank lines before and after them.</p> <p>The way to present examples is as follows:</p> <ol> <li> <p>Import required libraries (except <code>numpy</code> and <code>biocypher</code>)</p> </li> <li> <p>Create the data required for the example</p> </li> <li> <p>Show a very basic example that gives an idea of the most common use case</p> </li> <li> <p>Add examples with explanations that illustrate how the parameters can be    used for extended functionality</p> </li> </ol> <p>A simple example could be:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n\n    def head(self, n=5):\n        \"\"\"\n        Return the first elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying all of it.\n\n        Parameters\n        ----------\n        n : int\n            Number of values to return.\n\n        Return\n        ------\n        biocypher.Series\n            Subset of the original series with the n first values.\n\n        See Also\n        --------\n        tail : Return the last n elements of the Series.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series(['Ant', 'Bear', 'Cow', 'Dog', 'Falcon',\n        ...                'Lion', 'Monkey', 'Rabbit', 'Zebra'])\n        &gt;&gt;&gt; ser.head()\n        0   Ant\n        1   Bear\n        2   Cow\n        3   Dog\n        4   Falcon\n        dtype: object\n\n        With the ``n`` parameter, we can change the number of returned rows:\n\n        &gt;&gt;&gt; ser.head(n=3)\n        0   Ant\n        1   Bear\n        2   Cow\n        dtype: object\n        \"\"\"\n        return self.iloc[:n]\n</code></pre> <p>The examples should be as concise as possible. In cases where the complexity of the function requires long examples, is recommended to use blocks with headers in bold. Use double star <code>**</code> to make a text bold, like in <code>**this example**</code>.</p> <p>.. _docstring.example_conventions:</p> <p>Conventions for the examples ^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Code in examples is assumed to always start with these two lines which are not shown:</p> <p>.. code-block:: python</p> <pre><code>import numpy as np\nimport biocypher as pd\n</code></pre> <p>Any other module used in the examples must be explicitly imported, one per line (as recommended in :pep:<code>8#imports</code>) and avoiding aliases. Avoid excessive imports, but if needed, imports from the standard library go first, followed by third-party libraries (like matplotlib).</p> <p>When illustrating examples with a single <code>Series</code> use the name <code>ser</code>, and if illustrating with a single <code>DataFrame</code> use the name <code>df</code>. For indices, <code>idx</code> is the preferred name. If a set of homogeneous <code>Series</code> or <code>DataFrame</code> is used, name them <code>ser1</code>, <code>ser2</code>, <code>ser3</code>...  or <code>df1</code>, <code>df2</code>, <code>df3</code>... If the data is not homogeneous, and more than one structure is needed, name them with something meaningful, for example <code>df_main</code> and <code>df_to_join</code>.</p> <p>Data used in the example should be as compact as possible. The number of rows is recommended to be around 4, but make it a number that makes sense for the specific example. For example in the <code>head</code> method, it requires to be higher than 5, to show the example with the default values. If doing the <code>mean</code>, we could use something like <code>[1, 2, 3]</code>, so it is easy to see that the value returned is the mean.</p> <p>For more complex examples (grouping for example), avoid using data without interpretation, like a matrix of random numbers with columns A, B, C, D... And instead use a meaningful example, which makes it easier to understand the concept. Unless required by the example, use names of animals, to keep examples consistent. And numerical properties of them.</p> <p>When calling the method, keywords arguments <code>head(n=3)</code> are preferred to positional arguments <code>head(3)</code>.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n\n    def mean(self):\n        \"\"\"\n        Compute the mean of the input.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; ser.mean()\n        2\n        \"\"\"\n        pass\n\n\n    def fillna(self, value):\n        \"\"\"\n        Replace missing values by ``value``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([1, np.nan, 3])\n        &gt;&gt;&gt; ser.fillna(0)\n        [1, 0, 3]\n        \"\"\"\n        pass\n\n    def groupby_mean(self):\n        \"\"\"\n        Group by index and return mean.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([380., 370., 24., 26],\n        ...               name='max_speed',\n        ...               index=['falcon', 'falcon', 'parrot', 'parrot'])\n        &gt;&gt;&gt; ser.groupby_mean()\n        index\n        falcon    375.0\n        parrot     25.0\n        Name: max_speed, dtype: float64\n        \"\"\"\n        pass\n\n    def contains(self, pattern, case_sensitive=True, na=numpy.nan):\n        \"\"\"\n        Return whether each value contains ``pattern``.\n\n        In this case, we are illustrating how to use sections, even\n        if the example is simple enough and does not require them.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series('Antelope', 'Lion', 'Zebra', np.nan)\n        &gt;&gt;&gt; ser.contains(pattern='a')\n        0    False\n        1    False\n        2     True\n        3      NaN\n        dtype: bool\n\n        **Case sensitivity**\n\n        With ``case_sensitive`` set to ``False`` we can match ``a`` with both\n        ``a`` and ``A``:\n\n        &gt;&gt;&gt; s.contains(pattern='a', case_sensitive=False)\n        0     True\n        1    False\n        2     True\n        3      NaN\n        dtype: bool\n\n        **Missing values**\n\n        We can fill missing values in the output using the ``na`` parameter:\n\n        &gt;&gt;&gt; ser.contains(pattern='a', na=False)\n        0    False\n        1    False\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def method(foo=None, bar=None):\n    \"\"\"\n    A sample DataFrame method.\n\n    Do not import NumPy and biocypher.\n\n    Try to use meaningful data, when it makes the example easier\n    to understand.\n\n    Try to avoid positional arguments like in ``df.method(1)``. They\n    can be all right if previously defined with a meaningful name,\n    like in ``present_value(interest_rate)``, but avoid them otherwise.\n\n    When presenting the behavior with different parameters, do not place\n    all the calls one next to the other. Instead, add a short sentence\n    explaining what the example shows.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import biocypher as pd\n    &gt;&gt;&gt; df = pd.DataFrame(np.random.randn(3, 3),\n    ...                   columns=('a', 'b', 'c'))\n    &gt;&gt;&gt; df.method(1)\n    21\n    &gt;&gt;&gt; df.method(bar=14)\n    123\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.doctest_tips:</p> <p>Tips for getting your examples pass the doctests ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Getting the examples pass the doctests in the validation script can sometimes be tricky. Here are some attention points:</p> <ul> <li> <p>Import all needed libraries (except for biocypher and NumPy, those are already   imported as <code>import biocypher as pd</code> and <code>import numpy as np</code>) and define   all variables you use in the example.</p> </li> <li> <p>Try to avoid using random data. However random data might be OK in some   cases, like if the function you are documenting deals with probability   distributions, or if the amount of data needed to make the function result   meaningful is too much, such that creating it manually is very cumbersome.   In those cases, always use a fixed random seed to make the generated examples   predictable. Example::</p> <p>np.random.seed(42) df = pd.DataFrame({'normal': np.random.normal(100, 5, 20)})</p> </li> <li> <p>If you have a code snippet that wraps multiple lines, you need to use '...'   on the continued lines: ::</p> <p>df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['a', 'b', 'c'], ...                   columns=['A', 'B'])</p> </li> <li> <p>If you want to show a case where an exception is raised, you can do::</p> <p>pd.to_datetime([\"712-01-01\"]) Traceback (most recent call last): OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 712-01-01 00:00:00</p> </li> </ul> <p>It is essential to include the \"Traceback (most recent call last):\", but for   the actual error only the error name is sufficient.</p> <ul> <li>If there is a small part of the result that can vary (e.g. a hash in an object   representation), you can use <code>...</code> to represent this part.</li> </ul> <p>If you want to show that <code>s.plot()</code> returns a matplotlib AxesSubplot object,   this will fail the doctest ::</p> <pre><code>&gt;&gt;&gt; s.plot()\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7efd0c0b0690&gt;\n</code></pre> <p>However, you can do (notice the comment that needs to be added) ::</p> <pre><code>&gt;&gt;&gt; s.plot()  # doctest: +ELLIPSIS\n&lt;matplotlib.axes._subplots.AxesSubplot at ...&gt;\n</code></pre> <p>.. _docstring.example_plots:</p> <p>Plots in examples ^^^^^^^^^^^^^^^^^</p> <p>There are some methods in biocypher returning plots. To render the plots generated by the examples in the documentation, the <code>.. plot::</code> directive exists.</p> <p>To use it, place the next code after the \"Examples\" header as shown below. The plot will be generated automatically when building the documentation.</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self):\n        \"\"\"\n        Generate a plot with the ``Series`` data.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n            &gt;&gt;&gt; ser.plot()\n        \"\"\"\n        pass\n</code></pre> <p>.. _docstring.sharing:</p>"},{"location":"BioCypher/community/biocypher-docstring-guide/#sharing-docstrings","title":"Sharing docstrings","text":"<p>biocypher has a system for sharing docstrings, with slight variations, between classes. This helps us keep docstrings consistent, while keeping things clear for the user reading. It comes at the cost of some complexity when writing.</p> <p>Each shared docstring will have a base template with variables, like <code>{klass}</code>. The variables filled in later on using the <code>doc</code> decorator. Finally, docstrings can also be appended to with the <code>doc</code> decorator.</p> <p>In this example, we'll create a parent docstring normally (this is like <code>biocypher.core.generic.NDFrame</code>. Then we'll have two children (like <code>biocypher.core.series.Series</code> and <code>biocypher.core.frame.DataFrame</code>). We'll substitute the class names in this docstring.</p> <p>.. code-block:: python</p> <p>class Parent:        @doc(klass=\"Parent\")        def my_function(self):            \"\"\"Apply my function to {klass}.\"\"\"            ...</p> <p>class ChildA(Parent):        @doc(Parent.my_function, klass=\"ChildA\")        def my_function(self):            ...</p> <p>class ChildB(Parent):        @doc(Parent.my_function, klass=\"ChildB\")        def my_function(self):            ...</p> <p>The resulting docstrings are</p> <p>.. code-block:: python</p> <p>print(Parent.my_function.doc)    Apply my function to Parent. print(ChildA.my_function.doc)    Apply my function to ChildA. print(ChildB.my_function.doc)    Apply my function to ChildB.</p> <p>Notice:</p> <ol> <li>We \"append\" the parent docstring to the children docstrings, which are    initially empty.</li> </ol> <p>Our files will often contain a module-level <code>_shared_doc_kwargs</code> with some common substitution values (things like <code>klass</code>, <code>axes</code>, etc).</p> <p>You can substitute and append in one shot with something like</p> <p>.. code-block:: python</p> <p>@doc(template, **_shared_doc_kwargs)    def my_function(self):        ...</p> <p>where <code>template</code> may come from a module-level <code>_shared_docs</code> dictionary mapping function names to docstrings. Wherever possible, we prefer using <code>doc</code>, since the docstring-writing processes is slightly closer to normal.</p> <p>See <code>biocypher.core.generic.NDFrame.fillna</code> for an example template, and <code>biocypher.core.series.Series.fillna</code> and <code>biocypher.core.generic.frame.fillna</code> for the filled versions.</p>"},{"location":"BioCypher/community/contribute-codebase/","title":"Developer Guide","text":"<p>Thank you for considering to contribute to the project! This guide will help you to get started with the development of the project. If you have any questions, please feel free to ask them in the issue tracker or on Zulip.</p>"},{"location":"BioCypher/community/contribute-codebase/#small-contributions","title":"Small Contributions","text":"<p>If you want to contribute a small change (e.g., a bugfix), you can probably immediately go ahead and create a pull request. For more substantial changes or additions, please read on.</p>"},{"location":"BioCypher/community/contribute-codebase/#larger-contributions","title":"Larger Contributions","text":"<p>If you want to contribute a larger change, please create an issue first. This will allow us to discuss the change and make sure that it fits into the project.  It can happen that development for a feature is already in progress, so it is important to check first to avoid duplicate work. If you have any questions, feel free to approach us in any way you like.</p>"},{"location":"BioCypher/community/contribute-codebase/#dependency-management","title":"Dependency management","text":"<p>We use Poetry for dependency management. Please make sure that you have installed Poetry and set up the environment correctly before starting development.</p>"},{"location":"BioCypher/community/contribute-codebase/#setting-up-the-environment","title":"Setting up the environment","text":"<ul> <li> <p>Install dependencies from the lock file: <code>poetry install</code></p> </li> <li> <p>Use the environment: You can either run commands directly with <code>poetry run &lt;command&gt;</code> or open a shell with <code>poetry shell</code> and then run commands directly.</p> </li> </ul>"},{"location":"BioCypher/community/contribute-codebase/#updating-the-environment","title":"Updating the environment","text":"<p>If you want to fix dependency issues, please do so in the Poetry framework. If Poetry does not work for you for some reason, please let us know.</p> <p>The Poetry dependencies are organized in groups. There are groups with dependencies needed for running BioCypher (<code>[tool.poetry.dependencies]</code> with the group name <code>main</code>) and a group with dependencies needed for development (<code>[tool.poetry.group.dev.dependencies]</code> with the group name <code>dev</code>).</p> <p>For adding new dependencies:</p> <ul> <li> <p>Add new dependencies via <code>poetry add</code>: <code>poetry add &lt;dependency&gt; --group &lt;group&gt;</code>. This will update the <code>pyproject.toml</code> and lock file automatically.</p> </li> <li> <p>Add new dependencies via <code>pyproject.toml</code>: Add the dependency to the <code>pyproject.toml</code> file in the correct group, including version. Then update the lock file: <code>poetry lock</code> and install the dependencies: <code>poetry install</code>.</p> </li> </ul>"},{"location":"BioCypher/community/contribute-codebase/#code-quality-and-formal-requirements","title":"Code quality and formal requirements","text":"<p>For ensuring code quality, the following tools are used:</p> <ul> <li> <p>isort for sorting imports</p> </li> <li> <p>black for automated code formatting</p> </li> <li> <p>pre-commit-hooks for ensuring some general rules</p> </li> <li> <p>pep585-upgrade for automatically upgrading type hints to the new native types defined in PEP 585</p> </li> <li> <p>pygrep-hooks for ensuring some general naming rules</p> </li> <li> <p>NEW Ruff An extremely fast Python linter and code formatter, written in Rust</p> </li> </ul> <p>We recommend configuring your IDE to execute Ruff on save/type, which will automatically keep your code clean and fix some linting errors as you type. This is made possible by the fast execution of Ruff and removes the need to run a dedicated pre-commit step. For instance, in VSCode or Cursor, you can add this to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"editor.formatOnType\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.fixAll.ruff\": \"explicit\",\n        \"source.organizeImports.ruff\": \"explicit\"\n    },\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n}\n</code></pre> <p>Alternatively, pre-commit hooks can be used to automatically or manually run these tools before each commit. They are defined in <code>.pre-commit-config.yaml</code>. To install the hooks run <code>poetry run pre-commit install</code>. The hooks are then executed before each commit. For running the hook for all project files (not only the changed ones) run <code>poetry run pre-commit run --all-files</code>. Our CI runs the pre-commit hooks, so running them locally is a good way to check if your code conforms to the formatting rules.</p>"},{"location":"BioCypher/community/contribute-codebase/#testing","title":"Testing","text":"<p>The project uses pytest for testing. To run the tests, please run <code>pytest</code> in the root directory of the project. We are developing BioCypher using test-driven development. Please make sure that you add tests for your code before submitting a pull request.</p> <p>The existing tests can also help you to understand how the code works. If you have any questions, please feel free to ask them in the issue tracker or on Zulip.</p> <p>Before submitting a pull request, please make sure that all tests pass and that the documentation builds correctly.</p>"},{"location":"BioCypher/community/contribute-codebase/#versioning","title":"Versioning","text":"<p>We use semantic versioning for the project. This means that the version number is incremented according to the following scheme:</p> <ul> <li> <p>Increment the major version number if you make incompatible API changes.</p> </li> <li> <p>Increment the minor version number if you add functionality in a backwards-   compatible manner. Since we are still in the 0.x.y version range, most of the   significant changes will increase the minor version number.</p> </li> <li> <p>Increment the patch version number if you make backwards-compatible bug fixes.</p> </li> </ul> <p>We use the <code>bumpversion</code> tool to update the version number in the <code>pyproject.toml</code> file. This will create a new git tag automatically. Usually, versioning is done by the maintainers, so please do not increment versions in pull requests by default.</p>"},{"location":"BioCypher/community/contribute-codebase/#finding-an-issue-to-contribute-to","title":"Finding an issue to contribute to","text":"<p>If you are brand new to BioCypher or open-source development, we recommend searching the GitHub \"Issues\" tab to find issues that interest you. Unassigned issues labeled <code>Docs</code> and good first issue are typically good for newer contributors.</p> <p>Once you've found an interesting issue, it's a good idea to assign the issue to yourself, so nobody else duplicates the work on it.</p> <p>If for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it's available again. If you want to work on an issue that is currently assigned but you're unsure whether work is actually being done, feel free to kindly ask the current assignee if you can take over (please allow at least a week of inactivity before getting in touch).</p>"},{"location":"BioCypher/community/contribute-codebase/#submitting-a-pull-request","title":"Submitting a Pull Request","text":""},{"location":"BioCypher/community/contribute-codebase/#tips-for-a-successful-pull-request","title":"Tips for a successful pull request","text":"<p>To improve the chances of your pull request being reviewed, you should:</p> <ul> <li>Reference an open issue for non-trivial changes to clarify the PR's purpose.</li> <li>Ensure you have appropriate tests. Tests should be the focus of any PR (apart from documentation changes).</li> <li>Keep your pull requests as simple as possible. Larger PRs take longer to review.</li> <li>Ensure that CI is in a green state. Reviewers may tell you to fix the CI before looking at anything else.</li> </ul>"},{"location":"BioCypher/community/contribute-codebase/#version-control-git-and-github","title":"Version control, Git, and GitHub","text":"<p>BioCypher is hosted on GitHub, and to contribute, you will need to sign up for a free GitHub account. We use Git for version control to allow many people to work together on the project.</p> <p>If you are new to Git, you can reference some of these resources for learning Git. Feel free to reach out to the contributor community for help if needed:</p> <ul> <li>Git documentation.</li> </ul> <p>The project follows a forking workflow further described on this page whereby contributors fork the repository, make changes and then create a Pull Request. So please be sure to read and follow all the instructions in this guide.</p> <p>If you are new to contributing to projects through forking on GitHub, take a look at the GitHub documentation for contributing to projects. GitHub provides a quick tutorial using a test repository that may help you become more familiar with forking a repository, cloning a fork, creating a feature branch, pushing changes and making Pull Requests.</p> <p>Below are some useful resources for learning more about forking and Pull Requests on GitHub:</p> <ul> <li> <p>the GitHub documentation for forking a repo.</p> </li> <li> <p>the GitHub documentation for collaborating with Pull Requests.</p> </li> <li> <p>the GitHub documentation for working with forks.</p> </li> </ul> <p>There are also many unwritten rules and conventions that are helpful in interacting with other open-source contributors. These lessons from PyOpenSci are a good resource for learning more about how to interact with other open-source contributors in scientific computing.</p>"},{"location":"BioCypher/community/contribute-codebase/#getting-started-with-git","title":"Getting started with Git","text":"<p>GitHub has instructions for installing git, setting up your SSH key, and configuring git. All these steps need to be completed before you can work seamlessly between your local repository and GitHub.</p>"},{"location":"BioCypher/community/contribute-codebase/#create-a-fork-of-biocypher","title":"Create a fork of BioCypher","text":"<p>You will need your own fork of BioCypher in order to eventually open a Pull Request. Go to the BioCypher project page and hit the Fork button. Please uncheck the box to copy only the main branch before selecting Create Fork. You will then want to clone your fork to your machine.</p> <pre><code>git clone https://github.com/your-user-name/biocypher.git\ncd biocypher\ngit remote add upstream https://github.com/biocypher/biocypher.git\ngit fetch upstream\n</code></pre> <p>This creates the directory <code>biocypher</code> and connects your repository to the upstream (main project) biocypher repository. They have the same name, but your local repository and fork are separate from the upstream repository.</p>"},{"location":"BioCypher/community/contribute-codebase/#creating-a-feature-branch","title":"Creating a feature branch","text":"<p>Your local <code>main</code> branch should always reflect the current state of BioCypher repository. First ensure it's up-to-date with the main BioCypher repository.</p> <pre><code>git checkout main\ngit pull upstream main --ff-only\n</code></pre> <p>Then, create a feature branch for making your changes. For example, we are going to create a branch called <code>my-new-feature-for-biocypher</code></p> <pre><code>git checkout -b my-new-feature-for-biocypher\n</code></pre> <p>This changes your working branch from <code>main</code> to the <code>my-new-feature-for-biocypher</code> branch. Keep any changes in this branch specific to one bug or feature so it is clear what the branch brings to BioCypher. You can have many feature branches and switch between them using the <code>git checkout</code> command.</p>"},{"location":"BioCypher/community/contribute-codebase/#making-code-changes","title":"Making code changes","text":"<p>Before modifying any code, ensure you follow the contributing environment guidelines to set up an appropriate development environment.</p> <p>When making changes, follow these BioCypher-specific guidelines:</p> <ol> <li> <p>Keep changes of that branch/PR focused on a single feature or bug fix.</p> </li> <li> <p>Follow roughly the conventional commit message conventions.</p> </li> </ol>"},{"location":"BioCypher/community/contribute-codebase/#pushing-your-changes","title":"Pushing your changes","text":"<p>When you want your committed changes to appear publicly on your GitHub page, you can push your forked feature branch's commits to your forked repository on GitHub.</p> <p>Now your code is on GitHub, but it is not yet a part of the BioCypher project. For that to happen, a Pull Request (PR) needs to be submitted.</p>"},{"location":"BioCypher/community/contribute-codebase/#opening-a-pull-request-pr","title":"Opening a Pull Request (PR)","text":"<p>If everything looks good according to the general guidelines, you are ready to make a Pull Request. A Pull Request is how code from your fork becomes available to the project maintainers to review and merge into the project to appear in the next release. To submit a Pull Request:</p> <ol> <li> <p>Navigate to your repository on GitHub.</p> </li> <li> <p>Click on the Compare &amp; Pull Request button.</p> </li> <li> <p>You can then click on Commits and Files Changed to make sure everything looks okay one last time.</p> </li> <li> <p>Write a descriptive title that includes prefixes. BioCypher uses a convention for title prefixes, most commonly, <code>feat:</code> for features, <code>fix:</code> for bug fixes, and <code>refactor:</code> for refactoring.</p> </li> <li> <p>Write a description of your changes in the <code>Preview Discussion</code> tab. This description will inform the reviewers about the changes you made, so please include all relevant information, including the motivation, implementation details, and references to any issues that you are addressing.</p> </li> <li> <p>Make sure to <code>Allow edits from maintainers</code>; this allows the maintainers to make changes to your PR directly, which is useful if you are not sure how to fix the PR.</p> </li> <li> <p>Click <code>Send Pull Request</code>.</p> </li> <li> <p>Optionally, you can assign reviewers to your PR, if you know who should review it.</p> </li> </ol> <p>This request then goes to the repository maintainers, and they will review the code.</p>"},{"location":"BioCypher/community/contribute-codebase/#updating-your-pull-request","title":"Updating your Pull Request","text":"<p>Based on the review you get on your pull request, you will probably need to make some changes to the code. You can follow the steps above again to address any feedback and update your pull request.</p>"},{"location":"BioCypher/community/contribute-codebase/#parallel-changes-in-the-upstream-main-branch","title":"Parallel changes in the upstream <code>main</code> branch","text":"<p>In case of simultaneous changes to the upstream code, it is important that these changes are reflected in your pull request. To update your feature branch with changes in the biocypher <code>main</code> branch, run:</p> <pre><code>    git checkout my-new-feature-for-biocypher\n    git fetch upstream\n    git merge upstream/main\n</code></pre> <p>If there are no conflicts (or they could be fixed automatically), a file with a default commit message will open, and you can simply save and quit this file.</p> <p>If there are merge conflicts, you need to resolve those conflicts. See here for an explanation on how to do this.</p> <p>Once the conflicts are resolved, run:</p> <ol> <li><code>git add -u</code> to stage any files you've updated;</li> <li><code>git commit</code> to finish the merge.</li> </ol> <p>After the feature branch has been updated locally, you can now update your pull request by pushing to the branch on GitHub:</p> <pre><code>    git push origin my-new-feature-for-biocypher\n</code></pre> <p>Any <code>git push</code> will automatically update your pull request with your branch's changes and restart the <code>Continuous Integration</code> checks.</p>"},{"location":"BioCypher/community/contribute-docs/","title":"Contributing to the documentation","text":"<p>Contributing to the documentation benefits everyone who uses BioCypher. We encourage you to help us improve the documentation, and you don't have to be an expert on BioCypher to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn't make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person.</p>"},{"location":"BioCypher/community/contribute-docs/#how-to-contribute-to-the-documentation","title":"How to contribute to the documentation","text":"<p>The documentation is written in Markdown, which is almost like writing in plain English, and built using Material for MkDocs. The simplest way to contribute to the docs is to click on the <code>Edit</code> button (pen and paper) at the top right of any page. This will take you to the source file on GitHub, where you can make your changes and create a pull request using GitHub's web interface (the <code>Commit changes...</code> button).</p> <p>Some other important things to know about the docs:</p> <ul> <li> <p>The BioCypher documentation consists of two parts: the docstrings in the code   itself and the docs in the <code>docs/</code> folder. The docstrings provide a clear   explanation of the usage of the individual functions, while the documentation   website you are looking at is built from the <code>docs/</code> folder.</p> </li> <li> <p>The docstrings follow a BioCypher convention, based on the Google Docstring   Standard.</p> </li> <li> <p>Our API documentation files in <code>docs/reference/source</code> contain the   instructions for the auto-generated documentation from the docstrings. For   classes, there are a few subtleties around controlling which methods and   attributes have pages auto-generated.</p> </li> </ul>"},{"location":"BioCypher/community/contribute/","title":"How to Start Contributing","text":"<p>There are many valuable ways to contribute besides writing code. Thank you for dedicating your time to improve our projects!</p>"},{"location":"BioCypher/community/contribute/#bug-reports-and-enhancement-requests","title":"Bug reports and enhancement requests","text":"<p>Bug reports and enhancement requests are an important part of making any software more stable. We curate them though Github issues. When opening an issue or request, please select the appropriate category and fill out the issue form fully to ensure others and the core development team can fully understand the scope of the issue. If your category is not listed, you can create a blank issue.</p> <p>The issue will then show up to the BioCypher community and be open to comments/ideas from others.</p>"},{"location":"BioCypher/community/contribute/#categories","title":"Categories","text":"<ul> <li>Bug Report: Report incorrect behavior in the BioCypher library</li> <li>Register New Component: Register a new component in the BioCypher ecosystem, either one you have created, or one that you would like to see added</li> <li>Documentation Improvement: Report wrong or missing documentation</li> <li>Feature Request: Suggest an idea for BioCypher</li> </ul>"},{"location":"BioCypher/community/contribute/#detailed-guides","title":"Detailed Guides","text":"<ul> <li> <p> Contributing to the Documentation</p> <p>A simple way to get started is to contribute to the documentation. Please follow the guide here to learn how to do so.</p> <p> To the contribution guide</p> </li> </ul> <ul> <li> <p> Contributing to the Code Base</p> <p>The best way to contribute code is to open a pull request on Github. Please follow the guide here to learn how to do so.</p> <p> To the contribution guide</p> </li> </ul>"},{"location":"BioCypher/community/maintenance/","title":"Biocypher Maintenance","text":"<p>This guide is for BioCypher's maintainers. It may also be interesting to contributors looking to understand the BioCypher development process and what steps are necessary to become a maintainer.</p>"},{"location":"BioCypher/community/maintenance/#roles","title":"Roles","text":"<p>GitHub publishes the full list of permissions.</p>"},{"location":"BioCypher/community/maintenance/#tasks","title":"Tasks","text":"<p>biocypher is largely a volunteer project, so these tasks shouldn't be read as \"expectations\" of triage and maintainers. Rather, they're general descriptions of what it means to be a maintainer.</p> <ul> <li>Triage newly filed issues (see :ref:<code>maintaining.triage</code>)</li> <li>Review newly opened pull requests</li> <li>Respond to updates on existing issues and pull requests</li> <li>Drive discussion and decisions on stalled issues and pull requests</li> <li>Provide experience / wisdom on API design questions to ensure consistency and maintainability</li> <li>Project organization (run / attend developer meetings, represent biocypher)</li> </ul> <p>https://matthewrocklin.com/blog/2019/05/18/maintainer may be interesting background reading.</p>"},{"location":"BioCypher/community/maintenance/#issue-triage","title":"Issue triage","text":"<p>Triage is an important first step in addressing issues reported by the community, and even partial contributions are a great way to help maintain biocypher. Only remove the \"Needs Triage\" tag once all of the steps below have been completed.</p> <p>Here's a typical workflow for triaging a newly opened issue.</p> <ol> <li> <p>Thank the reporter for opening an issue    The issue tracker is many people's first interaction with the biocypher project itself,    beyond just using the library. As such, we want it to be a welcoming, pleasant    experience.</p> </li> <li> <p>Is the necessary information provided?    Ideally reporters would fill out the issue template, but many don't.    If crucial information (like the version of biocypher they used), is missing    feel free to ask for that and label the issue with \"Needs info\". The    report should follow the guidelines in :ref:<code>contributing.bug_reports</code>.    You may want to link to that if they didn't follow the template.    Make sure that the title accurately reflects the issue. Edit it yourself    if it's not clear.</p> </li> <li> <p>Is this a duplicate issue?    We have many open issues. If a new issue is clearly a duplicate, label the    new issue as \"Duplicate\" and close the issue with a link to the original issue.    Make sure to still thank the reporter, and encourage them to chime in on the    original issue, and perhaps try to fix it.    If the new issue provides relevant information, such as a better or slightly    different example, add it to the original issue as a comment or an edit to    the original post.</p> </li> <li> <p>Is the issue minimal and reproducible?    For bug reports, we ask that the reporter provide a minimal reproducible    example. See https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports    for a good explanation. If the example is not reproducible, or if it's    clearly not minimal, feel free to ask the reporter if they can provide    and example or simplify the provided one. Do acknowledge that writing    minimal reproducible examples is hard work. If the reporter is struggling,    you can try to write one yourself and we'll edit the original post to include it.    If a reproducible example can't be provided, add the \"Needs info\" label.    If a reproducible example is provided, but you see a simplification,    edit the original post with your simpler reproducible example.    Ensure the issue exists on the main branch and that it has the \"Needs Triage\" tag    until all steps have been completed. Add a comment to the issue once you have    verified it exists on the main branch, so others know it has been confirmed.</p> </li> <li> <p>Is this a clearly defined feature request?    Generally, biocypher prefers to discuss and design new features in issues,    before a pull request is made. Encourage the submitter to include a proposed    API for the new feature. Having them write a full docstring is a good way to    pin down specifics.    Tag new feature requests with \"Needs Discussion\", as we'll need a discussion    from several biocypher maintainers before deciding whether the proposal is in    scope for biocypher.</p> </li> <li> <p>Is this a usage question?    We prefer that usage questions are asked on StackOverflow with the biocypher    tag. https://stackoverflow.com/questions/tagged/biocypher    If it's easy to answer, feel free to link to the relevant documentation section,    let them know that in the future this kind of question should be on    StackOverflow, and close the issue.</p> </li> <li> <p>What labels and milestones should I add?    Apply the relevant labels. This is a bit of an art, and comes with experience.    Look at similar issues to get a feel for how things are labeled.    If the issue is clearly defined and the fix seems relatively straightforward,    label the issue as \"Good first issue\".    Once you have completed the above, make sure to remove the \"needs triage\" label.</p> </li> </ol>"},{"location":"BioCypher/community/maintenance/#closing-issues","title":"Closing issues","text":"<p>Be delicate here: many people interpret closing an issue as us saying that the conversation is over. It's typically best to give the reporter some time to respond or self-close their issue if it's determined that the behavior is not a bug, or the feature is out of scope. Sometimes reporters just go away though, and we'll close the issue after the conversation has died. If you think an issue should be closed but are not completely sure, please apply the \"closing candidate\" label and wait for other maintainers to take a look.</p>"},{"location":"BioCypher/community/maintenance/#reviewing-pull-requests","title":"Reviewing pull requests","text":"<p>Anybody can review a pull request: regular contributors, triagers, or core-team members. But only core-team members can merge pull requests when they're ready.</p> <p>Here are some things to check when reviewing a pull request.</p> <ul> <li>Tests should be in a sensible location: in the same file as closely related tests.</li> <li>New public APIs should be included somewhere in <code>doc/source/reference/</code>.</li> <li>New / changed API should use the <code>versionadded</code> or <code>versionchanged</code> directives in the docstring.</li> <li>User-facing changes should have a whatsnew in the appropriate file.</li> <li>Regression tests should reference the original GitHub issue number like <code># GH-1234</code>.</li> <li>The pull request should be labeled and assigned the appropriate milestone (the next patch release   for regression fixes and small bug fixes, the next minor milestone otherwise)</li> <li>Changes should comply with our :ref:<code>policies.version</code>.</li> </ul>"},{"location":"BioCypher/community/maintenance/#cleaning-up-old-issues","title":"Cleaning up old issues","text":"<p>Every open issue in biocypher has a cost. Open issues make finding duplicates harder, and can make it harder to know what needs to be done in biocypher. That said, closing issues isn't a goal on its own. Our goal is to make biocypher the best it can be, and that's best done by ensuring that the quality of our open issues is high.</p> <p>Occasionally, bugs are fixed but the issue isn't linked to in the Pull Request. In these cases, comment that \"This has been fixed, but could use a test.\" and label the issue as \"Good First Issue\" and \"Needs Test\".</p> <p>If an older issue doesn't follow our issue template, edit the original post to include a minimal example, the actual output, and the expected output. Uniformity in issue reports is valuable.</p> <p>If an older issue lacks a reproducible example, label it as \"Needs Info\" and ask them to provide one (or write one yourself if possible). If one isn't provide reasonably soon, close it according to the policies in :ref:<code>maintaining.closing</code>.</p>"},{"location":"BioCypher/community/maintenance/#cleaning-up-old-pull-requests","title":"Cleaning up old pull requests","text":"<p>Occasionally, contributors are unable to finish off a pull request. If some time has passed (two weeks, say) since the last review requesting changes, gently ask if they're still interested in working on this. If another two weeks or so passes with no response, thank them for their work and then either:</p> <ul> <li>close the pull request;</li> <li>push to the contributor's branch to push their work over the finish line (if   you're part of <code>biocypher-core</code>). This can be helpful for pushing an important PR   across the line, or for fixing a small merge conflict.</li> </ul> <p>If closing the pull request, then please comment on the original issue that \"There's a stalled PR at #1234 that may be helpful.\", and perhaps label the issue as \"Good first issue\" if the PR was relatively close to being accepted.</p>"},{"location":"BioCypher/community/maintenance/#becoming-a-biocypher-maintainer","title":"Becoming a biocypher maintainer","text":"<p>The full process is outlined in our <code>governance documents</code>_. In summary, we're happy to give triage permissions to anyone who shows interest by being helpful on the issue tracker.</p> <p>The required steps for adding a maintainer are:</p> <ol> <li>Contact the contributor and ask their interest to join.</li> <li> <p>Add the contributor to the appropriate <code>GitHub Team &lt;https://github.com/orgs/biocypher-dev/teams&gt;</code>_ if accepted the invitation.</p> </li> <li> <p><code>biocypher-core</code> is for core team members</p> </li> <li><code>biocypher-triage</code> is for biocypher triage members</li> </ol> <p>If adding to <code>biocypher-core</code>, there are two additional steps:</p> <ol> <li>Add the contributor to the biocypher Google group.</li> <li>Create a pull request to add the contributor's GitHub handle to <code>biocypher-dev/biocypher/web/biocypher/config.yml</code>.</li> </ol> <p>The current list of core-team members is at https://github.com/biocypher-dev/biocypher/blob/main/web/biocypher/config.yml</p>"},{"location":"BioCypher/community/maintenance/#merging-pull-requests","title":"Merging pull requests","text":"<p>Only core team members can merge pull requests. We have a few guidelines.</p> <ol> <li>You should typically not self-merge your own pull requests without approval.    Exceptions include things like small changes to fix CI    (e.g. pinning a package version). Self-merging with approval from other    core team members is fine if the change is something you're very confident    about.</li> <li>You should not merge pull requests that have an active discussion, or pull    requests that has any <code>-1</code> votes from a core maintainer. biocypher operates    by consensus.</li> <li>For larger changes, it's good to have a +1 from at least two core team members.</li> </ol> <p>In addition to the items listed in :ref:<code>maintaining.closing</code>, you should verify that the pull request is assigned the correct milestone.</p> <p>Pull requests merged with a patch-release milestone will typically be backported by our bot. Verify that the bot noticed the merge (it will leave a comment within a minute typically). If a manual backport is needed please do that, and remove the \"Needs backport\" label once you've done it manually. If you forget to assign a milestone before tagging, you can request the bot to backport it with:</p> <p>.. code-block:: console</p> <p>@Meeseeksdev backport  <p>.. _maintaining.asv-machine:</p>"},{"location":"BioCypher/community/maintenance/#benchmark-machine","title":"Benchmark machine","text":"<p>The team currently owns dedicated hardware for hosting a website for biocypher' ASV performance benchmark. The results are published to https://asv-runner.github.io/asv-collection/biocypher/</p>"},{"location":"BioCypher/community/maintenance/#configuration","title":"Configuration","text":"<p>The machine can be configured with the <code>Ansible &lt;http://docs.ansible.com/ansible/latest/index.html&gt;</code>_ playbook in https://github.com/tomaugspurger/asv-runner.</p>"},{"location":"BioCypher/community/maintenance/#publishing","title":"Publishing","text":"<p>The results are published to another GitHub repository, https://github.com/tomaugspurger/asv-collection. Finally, we have a cron job on our docs server to pull from https://github.com/tomaugspurger/asv-collection, to serve them from <code>/speed</code>. Ask Tom or Joris for access to the webserver.</p>"},{"location":"BioCypher/community/maintenance/#debugging","title":"Debugging","text":"<p>The benchmarks are scheduled by Airflow. It has a dashboard for viewing and debugging the results. You'll need to setup an SSH tunnel to view them <pre><code>ssh -L 8080:localhost:8080 biocypher@panda.likescandy.com\n</code></pre></p>"},{"location":"BioCypher/community/maintenance/#release-process","title":"Release process","text":"<p>The release process makes a snapshot of biocypher (a git commit) available to users with a particular version number. After the release the new biocypher version will be available in the next places:</p> <ul> <li>Git repo with a <code>new tag &lt;https://github.com/biocypher-dev/biocypher/tags&gt;</code>_</li> <li>Source distribution in a <code>GitHub release &lt;https://github.com/biocypher-dev/biocypher/releases&gt;</code>_</li> <li>Pip packages in the <code>PyPI &lt;https://pypi.org/project/biocypher/&gt;</code>_</li> <li>Conda/Mamba packages in <code>conda-forge &lt;https://anaconda.org/conda-forge/biocypher&gt;</code>_</li> </ul> <p>The process for releasing a new version of biocypher is detailed next section.</p> <p>The instructions contain <code>&lt;version&gt;</code> which needs to be replaced with the version to be released (e.g. <code>1.5.2</code>). Also the branch to be released <code>&lt;branch&gt;</code>, which depends on whether the version being released is the release candidate of a new version, or any other version. Release candidates are released from <code>main</code>, while other versions are released from their branch (e.g. <code>1.5.x</code>).</p>"},{"location":"BioCypher/community/maintenance/#prerequisites","title":"Prerequisites","text":"<p>In order to be able to release a new biocypher version, the next permissions are needed:</p> <ul> <li>Merge rights to the <code>biocypher &lt;https://github.com/biocypher-dev/biocypher/&gt;</code> and   <code>biocypher-feedstock &lt;https://github.com/conda-forge/biocypher-feedstock/&gt;</code> repositories.   For the latter, open a PR adding your GitHub username to the conda-forge recipe.</li> <li>Permissions to push to <code>main</code> in the biocypher repository, to push the new tags.</li> <li><code>Write permissions to PyPI &lt;https://github.com/conda-forge/biocypher-feedstock/pulls&gt;</code>_.</li> <li>Access to our website / documentation server. Share your public key with the   infrastructure committee to be added to the <code>authorized_keys</code> file of the main   server user.</li> <li>Access to the social media accounts, to publish the announcements.</li> </ul>"},{"location":"BioCypher/community/maintenance/#pre-release","title":"Pre-release","text":"<ol> <li> <p>Agree with the core team on the next topics:</p> </li> <li> <p>Release date (major/minor releases happen usually every 6 months, and patch releases      monthly until x.x.5, just before the next major/minor)</p> </li> <li>Blockers (issues and PRs that must be part of the release)</li> <li> <p>Next version after the one being released</p> </li> <li> <p>Update and clean release notes for the version to be released, including:</p> </li> <li> <p>Set the final date of the release</p> </li> <li>Remove any unused bullet point</li> <li> <p>Make sure there are no formatting issues, typos, etc.</p> </li> <li> <p>Make sure the CI is green for the last commit of the branch being released.</p> </li> <li> <p>If not a release candidate, make sure all backporting pull requests to the branch    being released are merged.</p> </li> <li> <p>Create a new issue and milestone for the version after the one being released.    If the release was a release candidate, we would usually want to create issues and    milestones for both the next major/minor, and the next patch release. In the    milestone of a patch release, we add the description <code>on-merge: backport to &lt;branch&gt;</code>,    so tagged PRs are automatically backported to the release branch by our bot.</p> </li> <li> <p>Change the milestone of all issues and PRs in the milestone being released to the    next milestone.</p> </li> </ol>"},{"location":"BioCypher/community/maintenance/#release","title":"Release","text":"<ol> <li>Create an empty commit and a tag in the last commit of the branch to be released:: <pre><code>git checkout &lt;branch&gt;\ngit pull --ff-only upstream &lt;branch&gt;\ngit clean -xdf\ngit commit --allow-empty --author=\"biocypher Development Team &lt;biocypher-dev@python.org&gt;\" -m \"RLS: &lt;version&gt;\"\ngit tag -a v&lt;version&gt; -m \"Version &lt;version&gt;\"  # NOTE that the tag is v1.5.2 with \"v\" not 1.5.2\ngit push upstream &lt;branch&gt; --follow-tags\n</code></pre></li> </ol> <p>The docs for the new version will be built and published automatically with the docs job in the CI, which will be triggered when the tag is pushed.</p> <ol> <li>Only if the release is a release candidate, we want to create a new branch for it, immediately    after creating the tag. For example, if we are releasing biocypher 1.4.0rc0, we would like to    create the branch 1.4.x to backport commits to the 1.4 versions. As well as create a tag to    mark the start of the development of 1.5.0 (assuming it is the next version)::</li> </ol> <p><pre><code>git checkout -b 1.4.x\ngit push upstream 1.4.x\ngit checkout main\ngit commit --allow-empty -m \"Start 1.5.0\"\ngit tag -a v1.5.0.dev0 -m \"DEV: Start 1.5.0\"\ngit push upstream main --follow-tags\n</code></pre> 3. Download the source distribution and wheels from the <code>wheel staging area &lt;https://anaconda.org/scientific-python-nightly-wheels/biocypher&gt;</code>_.    Be careful to make sure that no wheels are missing (e.g. due to failed builds).</p> <p>Running scripts/download_wheels.sh with the version that you want to download wheels/the sdist for should do the trick.    This script will make a <code>dist</code> folder inside your clone of biocypher and put the downloaded wheels and sdist there::</p> <p><pre><code>scripts/download_wheels.sh &lt;VERSION&gt;\n</code></pre> 4. Create a <code>new GitHub release &lt;https://github.com/biocypher-dev/biocypher/releases/new&gt;</code>_:</p> <ul> <li>Tag: <code>&lt;version&gt;</code></li> <li>Title: <code>biocypher &lt;version&gt;</code></li> <li>Description: Copy the description of the last release of the same kind (release candidate, major/minor or patch release)</li> <li>Files: <code>biocypher-&lt;version&gt;.tar.gz</code> source distribution just generated</li> <li>Set as a pre-release: Only check for a release candidate</li> <li> <p>Set as the latest release: Leave checked, unless releasing a patch release for an older version      (e.g. releasing 1.4.5 after 1.5 has been released)</p> </li> <li> <p>Upload wheels to PyPI:: <pre><code>twine upload biocypher/dist/biocypher-&lt;version&gt;*.{whl,tar.gz} --skip-existing\n</code></pre></p> </li> <li>The GitHub release will after some hours trigger an    <code>automated conda-forge PR &lt;https://github.com/conda-forge/biocypher-feedstock/pulls&gt;</code>_.    (If you don't want to wait, you can open an issue titled <code>@conda-forge-admin, please update version</code> to trigger the bot.)    Merge it once the CI is green, and it will generate the conda-forge packages.</li> </ul> <p>In case a manual PR needs to be done, the version, sha256 and build fields are the    ones that usually need to be changed. If anything else in the recipe has changed since    the last release, those changes should be available in <code>ci/meta.yaml</code>.</p>"},{"location":"BioCypher/community/maintenance/#post-release","title":"Post-Release","text":"<ol> <li> <p>Update symlinks to stable documentation by logging in to our web server, and    editing <code>/var/www/html/biocypher-docs/stable</code> to point to <code>version/&lt;latest-version&gt;</code>    for major and minor releases, or <code>version/&lt;minor&gt;</code> to <code>version/&lt;patch&gt;</code> for    patch releases. The exact instructions are (replace the example version numbers by    the appropriate ones for the version you are releasing):</p> <ul> <li>Log in to the server and use the correct user.</li> <li><code>cd /var/www/html/biocypher-docs/</code></li> <li><code>ln -sfn version/2.1 stable</code> (for a major or minor release)</li> <li><code>ln -sfn version/2.0.3 version/2.0</code> (for a patch release)</li> </ul> </li> <li> <p>If releasing a major or minor release, open a PR in our source code to update    <code>web/biocypher/versions.json</code>, to have the desired versions in the documentation    dropdown menu.</p> </li> <li> <p>Close the milestone and the issue for the released version.</p> </li> <li> <p>Create a new issue for the next release, with the estimated date of release.</p> </li> <li> <p>Open a PR with the placeholder for the release notes of the next version. See    for example <code>the PR for 1.5.3 &lt;https://github.com/biocypher-dev/biocypher/pull/49843/files&gt;</code>_.    Note that the template to use depends on whether it is a major, minor or patch release.</p> </li> <li> <p>Announce the new release in the official channels (use previous announcements    for reference):</p> <ul> <li>The biocypher-dev and pydata mailing lists</li> <li>Twitter, Mastodon, Telegram and LinkedIn</li> </ul> </li> <li> <p>Update this release instructions to fix anything incorrect and to update about any    change since the last release.</p> </li> </ol> <p>.. _governance documents: https://github.com/biocypher-dev/biocypher/blob/main/web/biocypher/about/governance.md .. _list of permissions: https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/repository-roles-for-an-organization</p>"},{"location":"BioCypher/community/release-notes/","title":"Release notes","text":""},{"location":"BioCypher/community/release-notes/#version-080","title":"Version 0.8.0","text":""},{"location":"BioCypher/community/release-notes/#enhancements","title":"Enhancements","text":"<ul> <li>Labels can now be sorted in various way before being written to CSV, in order to reflect the ontological hierarchy flexibly. The default has changed to \"ascending\", meaning more specific to more generic labels (it was alphabetic before).</li> </ul>"},{"location":"BioCypher/community/release-notes/#contributors","title":"Contributors","text":"<ul> <li>Johann Dreo</li> </ul>"},{"location":"BioCypher/community/release-notes/#version-070","title":"Version 0.7.0","text":""},{"location":"BioCypher/community/release-notes/#enhancements_1","title":"Enhancements","text":"<ul> <li>Added <code>Ruff</code> as a tool for linting/formatting, including CI.</li> <li>BioCypher has support for NetworkX.</li> <li>BioChatter has improved token statistics handling (@shaohong feng) and has received a prototypic Python API calling module as a result of the German BioHackathon 3 in December. In this hackathon, we worked on integrating BioChatter-driven API calling with the scverse ecosystem (starting with core packages anndata and scanpy). Many thanks to all who contributed in the hackathon!</li> </ul>"},{"location":"BioCypher/community/release-notes/#contributors_1","title":"Contributors","text":"<ul> <li>Paul To</li> <li>Yaxi Liu</li> <li>Shaohong Feng</li> <li>Sebastian Lobentanzer</li> <li>Edwin Carre\u00f1o</li> </ul>"},{"location":"BioCypher/learn/quickstart/","title":"Quickstart","text":"<ul> <li> <p> Already Familiar?</p> <p>We have a project template (batteries included!)</p> <p> To the template</p> </li> <li> <p> New to BioCypher?</p> <p>Follow our detailed tutorial for on-boarding BioCypher.</p> <p> To the tutorial</p> </li> </ul> <p>Note</p> <p>If you already know how BioCypher works, we provide here a quickstart into the knowledge graph build process. We provide a template repository on GitHub, which you can use to get started with your own project. You can get it here. To set up a new project, simply follow the instructions in the README.</p> <p>If you are new to BioCypher and would like a step-by-step introduction to the package, please follow the tutorial.</p> <p>The BioCypher workflow of creating your own knowledge graph consists of three consecutive steps:</p> <ol> <li> <p>Clearly define the scope of your project, including the data sources you want to use, the entities and relationships you want to represent, and the ontologies that should inform these entities.</p> </li> <li> <p>Using these definitions, find existing adapters of data sources or, if necessary, create your own. For the data yielded by these adapters, create a schema configuration file that tells BioCypher how to represent the entities and relationships in the graph.</p> </li> <li> <p>Run BioCypher using the adapters and schema config to create the knowledge graph. If necessary, iterate over KG construction and configuration until you are satisfied with the result.</p> </li> </ol> <p>Graphical Interface Support</p> <p>We are currently working on a graphical user interface to support the complex process of defining and building a knowledge graph. Get in touch if you'd like to test or contribute to the development of this tool.</p>"},{"location":"BioCypher/learn/quickstart/#the-input-adapter","title":"The input adapter","text":"<p>BioCypher follows a modular approach to data inputs; to create a knowledge graph, we use at least one adapter module that provides a data stream to build the graph from. Examples for current adapters can be found on the GitHub project adapter view. This is the first place to look when creating your own KG; BioCypher adapters are meant to be reusable and a centralised way of maintaining access to data sources.</p> <p>Adapters can ingest data from many different input formats, including Python modules as in the CROssBAR adapter (which uses the OmniPath backend software, PyPath, for downloading and caching data), advanced file management formats such as Parquet as in the Open Targets adapter, or simple CSV files as in the Dependency Map adapter.</p> <p>The main function of the adapter is to pass data into BioCypher, usually as some form of iterable (commonly a list or generator of items). As a minimal example, we load a list of proteins with identifiers, trivial names, and molecular masses from a (fictional) CSV:</p> Adapter yielding nodes<pre><code># read data into df\nwith open(\"file.csv\", \"r\") as f:\n    proteins = pd.read_csv(f)\n\n# yield proteins from data frame\ndef node_generator():\n    for p in proteins:\n        _id = p[\"uniprot_id\"]\n        _type = \"protein\"\n        _props = {\n            \"name\": p[\"trivial_name\"],\n            \"mm\": p[\"molecular_mass\"]\n        }\n\n        yield (_id, _type, _props)\n</code></pre> <p>For nodes, BioCypher expects a tuple containing three entries; the preferred identifier of the node, the type of entity, and a dictionary containing all other properties (can be empty). What BioCypher does with the received information is determined largely by the schema configuration detailed below.</p> Adapter yielding edges<pre><code># read data into df\nwith open(\"file.csv\", \"r\") as f:\n    interactions = pd.read_csv(f)\n\n# yield interactions from data frame\ndef edge_generator():\n    for i in interactions:\n        _id = i[\"id\"]\n        _source = i[\"source\"]\n        _target = i[\"target\"]\n        _type = \"interaction\"\n        _props = {\n            \"type\": i[\"relationship_type\"],\n            \"score\": i[\"score\"],\n        }\n\n        yield (_id, _source, _target, _type, _props)\n</code></pre> <p>For edges, BioCypher expects a tuple containing five entries; the preferred identifier of the edge (can be <code>None</code>), the identifier of the source node (non-optional), the identifier of the target node (non-optional), the type of relationship, and a dictionary containing all other properties (can be empty).</p> <p>For advanced usage, the type of node or edge can be determined programatically. Properties do not need to be explicitly called one by one; they can be passed in as a complete dictionary of all entries and filtered inside BioCypher by detailing the desired properties per node type in the schema configuration file.</p>"},{"location":"BioCypher/learn/quickstart/#the-schema-configuration-yaml-file","title":"The schema configuration YAML file","text":"<p>The second important component of translation into a BioCypher-compatible knowledge graph is the specification of graph constituents and their mode of representation in the graph. To make this known to the BioCypher module, we use the schema-config.yaml, which details only the immediate constituents of the desired graph as the top-level entries in the YAML file. While each of these top-level entries is required to be found in the underlying ontology (for instance, the Biolink model), the <code>input_label</code> field is arbitrary and has to match the <code>_type</code> yielded by the adapter (compare above).</p> <p>Other fields of each entry can refer to the representation of the entity in the KG (<code>represented_as: node</code>), and the identifier namespace chosen for each entity type. For instance, a protein could be represented by a UniProt identifier, the corresponding ENSEMBL identifier, or an HGNC gene symbol. We prefer the CURIE prefix for unambiguous identification of entities. The CURIE prefix for \"Uniprot Protein\" is <code>uniprot</code>, so a consistent protein schema definition would be:</p> <pre><code>protein:                    # top-level entry, has to match ontology\n  represented_as: node      # mode of representation: node or edge\n  preferred_id: uniprot     # preferred identifier namespace\n  input_label: protein      # label that identifies members of this class (_type)\n</code></pre> <p>Note</p> <p>For BioCypher classes, similar to the internal representation in the Biolink model, we use lower sentence-case notation, e.g., <code>protein</code> and <code>small molecule</code>. For file names and Neo4j labels, these are converted to PascalCase. For more information, see the Ontology tutorial.</p> <p>The above configuration of the protein class specifies its representation as a node, that we wish to use the UniProt identifier as the main identifier for proteins, and that proteins in the data stream from the adapter carry the label (<code>_type</code>) <code>protein</code> (in lowercase). Should we want to use the ENSEMBL namespace instead of UniProt IDs, the corresponding CURIE prefix, in this case, <code>ensembl</code>, can be substituted:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: ensembl\n  input_label: protein\n</code></pre> <p>If there exists no identifier system that is suitable for coverage of the data (which is fairly common when it comes to relationships), <code>preferred_id</code> field can be omitted. This will lead to the creation of a generic <code>id</code> property on this node or edge type.</p> <p>Rename incoming</p> <p>To denote the namespace of identifiers less ambiguously, we will rename the <code>preferred_id</code> field to <code>namespace</code>. The legacy field will still be supported and deprecated in a future release.</p>"},{"location":"BioCypher/learn/quickstart/#biocypher-api-documentation","title":"BioCypher API documentation","text":"<p>BioCypher is instantiated using the <code>BioCypher()</code> class, which can be called without arguments, given that the configuration files are either present in the working directory, or the pipeline should be run with default settings.</p> <pre><code>from biocypher import BioCypher\nbc = BioCypher()\n</code></pre> <p>BioCypher's main functionality is writing the graph (nodes and edges) to a database or files for database import. We exemplarise this using the Neo4j output format, writing CSV files formatted for the Neo4j admin import. In this example, <code>node_generator()</code> and <code>edge_generator()</code> are the adapter functions that yield nodes and edges, respectively (see above).</p> <pre><code>bc.write_nodes(node_generator())\nbc.write_edges(edge_generator())\n</code></pre> <p>Node and edge generators can contain arbitrarily many types of nodes and edges, which will be mapped via the schema configuration and sorted by BioCypher. One instance of the BioCypher class keeps track of the nodes and edges that have been written to the database, so that multiple calls to <code>write_nodes()</code> and <code>write_edges()</code> will not lead to duplicate entries in the database.</p> <p>For on-line writing to a database or a Pandas dataframe, we use the functions with <code>add</code> instead of <code>write</code>. For instance, to add nodes and edges to a Pandas dataframe, we can use:</p> <pre><code>bc.add_nodes(node_generator())\nbc.add_edges(edge_generator())\n</code></pre> <p>To retrieve the dataframe once all entities are in the graph, we can call <code>to_df()</code>:</p> <pre><code>df = bc.to_df()\n</code></pre> <p>For more information on the usage of these functions, please refer to the Tutorial section and the full API documentation.</p>"},{"location":"BioCypher/learn/quickstart/#the-biocypher-configuration-yaml-file","title":"The Biocypher configuration YAML file","text":"<p>Most of the configuration options for BioCypher can and should be specified in the configuration YAML file, <code>biocypher_config.yaml</code>. While BioCypher comes with default settings (the ones you can see in the Configuration section), we can override them by specifying the desired settings in the local configuration in the root or the <code>config</code> directory of the project. The primary BioCypher settings are found in the top-level entry <code>biocypher</code>. For instance, you can select your output format (<code>dbms</code>) and output path, the location of the schema configuration file, and the ontology to be used.</p> biocypher_config.yaml<pre><code>biocypher:\n  dbms: postgresql\n  output_path: postgres_out/\n  schema_config: config/schema-config.yaml\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n</code></pre> <p>You can currently select between <code>postgresql</code>, <code>neo4j</code>, <code>rdf</code> (beta), and <code>arangodb</code> (beta) as your output format; more options will be added in the future. The <code>output_path</code> is relative to your working directory, as is the schema-config path. The <code>ontology</code> should be specified as a (preferably persistent) URL to the ontology file, and a <code>root_node</code> to specify the node from which the ontology should be traversed.  We recommend using a URL that specifies the exact version of the ontology, as in the example above.</p>"},{"location":"BioCypher/learn/quickstart/#dbms-specific-settings","title":"DBMS-specific settings","text":"<p>In addition to the general settings, you can specify settings specific to each DBMS (database management system) in the configuration file under the <code>postgresql</code>, <code>arangodb</code>, <code>rdf</code>, or <code>neo4j</code> entry. For instance, you can specify the database name, the host, the port, and the credentials for your database. You can also set delimiters for the entities and arrays in your import files. For a list of all settings, please refer to the Configuration section.</p> biocypher_config.yaml<pre><code>neo4j:\n  database_name: biocypher\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n  delimiter: ','\n</code></pre>"},{"location":"BioCypher/learn/quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>BioCypher API Reference</li> <li>BioCypher Configuration Reference</li> <li>BioCypher Schema Reference</li> </ul>"},{"location":"BioCypher/learn/explanation/","title":"Explanation:","text":""},{"location":"BioCypher/learn/explanation/#purpose","title":"Purpose","text":"<p>The purpose of this documentation is to provide a comprehensive guide to understanding and working with knowledge graphs within the context of BioCypher.  It aims to explain the fundamental concepts of knowledge graphs and how they are represented in BioCypher.</p>"},{"location":"BioCypher/learn/explanation/#catalog","title":"Catalog","text":""},{"location":"BioCypher/learn/explanation/#basics-of-knowledge-graphs","title":"Basics of Knowledge Graphs.","text":"<ul> <li>Here goes a simple explanation about edges, nodes, and graphs. TODO: Create a page for this, including images.</li> </ul>"},{"location":"BioCypher/learn/explanation/#how-a-graph-is-represented-in-biocypher","title":"How a graph is represented in BioCypher?","text":"<ul> <li>Here goes a simple explanation about how the graphs are represented in BioCypher.</li> </ul>"},{"location":"BioCypher/learn/explanation/#about-this-documentation","title":"About this documentation","text":"<p>TO DO: delete once the webpage is live in production.</p>"},{"location":"BioCypher/learn/explanation/about-documentation/","title":"About the structure: tutorials, how-to guides, technical reference and explanations","text":"<p>Our documentation follows the \"Diataxis documentation framework\" proposed by Daniel Procida. This framework divide the documentation universe in four parts that represent different purposes or functions. They are: Tutorials, How-to guides, Technical Reference and Explanations.</p>"},{"location":"BioCypher/learn/explanation/about-documentation/#brief-summary","title":"Brief Summary:","text":"<p>Table 1. Diataxis documentation framework (extracted from link)</p> Tutorials How-to Guides Reference Explanation what they do introduce, educate, lead guide state, describe, inform explain, clarify, discuss answers the question Can you teach me to...? How do I...? \"What is...?\" \"Why...?\" oriented to learning a goal information understanding purpose to provide a learning experience to help achieve a particular goal to describe the machinery to illuminate a topic form a lesson a series of steps dry description discursive explanation analogy teaching a small child how to cook a recipe in a cookery book a reference encyclopedia article an article on culinary social history"},{"location":"BioCypher/learn/explanation/exp001_basics_knowledge_graphs/","title":"Basics of Knowledge Graphs","text":"<ul> <li> <p>Nodes</p> </li> <li> <p>Edges</p> </li> <li> <p>Operations</p> </li> </ul>"},{"location":"BioCypher/learn/guides/","title":"Index","text":"<ol> <li> <p>How to create a standalone Docker image for a BioCypher KG?</p> </li> <li> <p>How to use or create an Adapter to read data?</p> </li> <li> <p>How to combine data from different Adapters?</p> </li> <li> <p>How to create a graph from a CSV file</p> </li> <li> <p>How to create convert a graph to Pandas Dataframe.</p> </li> <li> <p>How to create a complete pipeline (from CSV file to Neo4j)</p> </li> <li> <p>How to configure BioCypher</p> </li> <li> <p>How to define a Schema for your Graph</p> </li> </ol>"},{"location":"BioCypher/learn/guides/adapters/","title":"Adapters","text":""},{"location":"BioCypher/learn/guides/adapters/#introduction","title":"Introduction","text":"<p>BioCypher is a modular framework, with the main purpose of avoiding redundant maintenance work for maintainers of secondary resources and end users alike. To achieve this, we use a collection of reusable \u201cadapters\u201d for the different sources of biomedical knowledge as well as for different ontologies. To see whether your favourite resource is already supported or currently in development, or if you would like to contribute to the development of a new adapter, please refer to this GitHub projects view (check the tabs for different views) or the meta-graph instance.</p> <p>Note</p> <p>We are currently working on adapter documentation, so the collection in the GitHub Projects view may be less than complete. Please get in touch if you want to make sure that your favourite resource is supported.</p> <ul> <li> <p> Adapter Tutorial</p> <p>For more information on developing your own adapters, please refer to this tutorial</p> <p> To the Adpaters Tutorial</p> </li> </ul> <p>The project view is built from issues in the BioCypher GitHub repository, which carry <code>Fields</code> (a GitHub Projects-specific attribute) to describe their category and features. In detail, these are as follows:</p> <ul> <li> <p><code>Component Type</code>: This refers to the class of component and can be one of <code>Adapter</code>, <code>Ontology</code>, or <code>Pipeline</code>.</p> </li> <li> <p><code>Adapter Granularity</code>: This is only applicable to adapters and can be either <code>Primary</code> (denoting an atomic resource that is represented by the adapter) or <code>Secondary</code> (denoting a composite resource, often pre-harmonised).</p> </li> <li> <p><code>Adapter Input Format</code> provides a drop-down menu of the different formats that can be ingested, such as <code>Flat File</code>, <code>API</code>, or <code>OWL</code>. Select the one that applies to the resource.</p> </li> <li> <p><code>Resource URL</code>: A free-text field to provide a link to the resource, also used for identification purposes.</p> </li> <li> <p><code>Resource Type</code>: Currently only <code>Database</code> or <code>Ontology</code>, but more granular reporting is planned.</p> </li> <li> <p><code>Data Type</code> provides a drop-down menu of the different data types that can be ingested, such as <code>Proteomics</code>, <code>Genetics</code>, or <code>Clinical</code>. Select the one that applies to the resource. This field primarily makes sense for primary adapters, but is interesting information particularly for the pipelines that use the adapters. For that reason, when building the meta-graph (see below), we propagate this information from the adapters to the pipelines.</p> </li> </ul> <p>Caution</p> <p>There is currently one type of meta-information that needs to be provided via free-text annotation in the text body of the issue: the links of pipelines to the input adapters and ontologies they use. For the meta-graph pipeline to work correctly, this information needs to be provided in the issue of the pipeline, in a line that starts with <code>Uses:</code>, followed by a space-separated list of issue numbers representing the used components.</p> <p>To make this annotation less error-prone, we use the auto-completion GitHub provides for referencing issues. Typing a <code>#</code> character and then a few characters of the title of the issue to be linked to the pipeline will show a list of possible matches. Select the correct one and the issue number will be inserted automatically.</p> <p>The meta-graph pipeline extracts this information and uses it to build the meta-graph described below.</p>"},{"location":"BioCypher/learn/guides/adapters/#biocypher-meta-graph","title":"BioCypher meta-graph","text":"<ul> <li> <p> Repo with Docker setup</p> <p>For more information on developing your own adapters, please refer to this tutorial</p> <p> To the BioCypher Meta-graph</p> </li> </ul> <p>We have built a BioCypher pipeline (from the template repository) that fetches information about all adapters from the BioCypher GitHub repository via the GitHub API and builds a graph of all adapters and their dependencies.  Browsing this graph can give an overview of the current state of the adapters supported by BioCypher and the pipelines they are used in. The graph can be built locally by cloning the repository and running the pipeline using <code>docker compose up</code>. The graph is then available at <code>localhost:7474/browser/</code> in the Neo4j Browser.</p> <p>If you're unfamiliar with Neo4j, you can use the following Cypher query to retrieve an overview of all graph contents:</p> <pre><code>MATCH (n)\nRETURN n\n</code></pre> <p>For more information on how to use the graph, please refer to the Neo4j documentation.</p>"},{"location":"BioCypher/learn/guides/htg001_standalone_docker_biocypher/","title":"Standalone Docker Image","text":"<p>In order to build a standalone Docker image for a BioCypher KG, you only need small modifications to the Docker image of the template. We will render the data that ususally is stored in the <code>biocypher_neo4j_volume</code> to our local disk by exchanging <code>biocypher_neo4j_volume</code> with a local directory, <code>./biocypher_neo4j_volume</code>. Then, we use a Dockerfile to build an image that contains the final database. This image can be used to deploy the database anywhere, without the need to run the BioCypher code. This process is demonstrated in the drug-interactions example repository.</p> <ol> <li> <p>Clone the example repository</p> <pre><code>git clone https://github.com/biocypher/drug-interactions.git\ncd drug-interactions\n</code></pre> </li> <li> <p>Attach volumes to disk by modifying the docker-compose.yml. In the example repository, we have created a dedicated compose file for the standalone image. You can see the differences between the standard and standalone compose files here. IMPORTANT: only run the standalone compose file once, as the data in the <code>./biocypher_neo4j_volume</code> directory is persistent and interferes with subsequent runs. If you want to run it again, you need to delete the <code>./biocypher_neo4j_volume</code> directory.</p> </li> <li> <p>Run the standalone compose file. This will create the <code>./biocypher_neo4j_volume</code> directory and store the data in it. You can stop the container after the database has been created.</p> <pre><code>docker compose -f docker-compose-local-disk.yml up -d\ndocker compose -f docker-compose-local-disk.yml down\n</code></pre> </li> <li> <p>Create standalone <code>Dockerfile</code> (example here):</p> <pre><code># Dockerfile\nFROM neo4j:4.4-enterprise\nCOPY ./biocypher_neo4j_volume /data\nRUN chown -R 7474:7474 /data\nEXPOSE 7474\nEXPOSE 7687\n</code></pre> </li> <li> <p>Build the standalone image.</p> <pre><code>docker build -t drug-interactions:latest .\n</code></pre> <p>This image can be deployed anywhere, without the need to run the BioCypher code. For example, you can add it to a Docker Compose file (example here):</p> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n[other services ...]\n\nbiocypher:\n    container_name: biocypher\n    image: biocypher/drug-interactions:latest\n    environment:\n    NEO4J_dbms_security_auth__enabled: \"false\"\n    NEO4J_dbms_databases_default__to__read__only: \"false\"\n    NEO4J_ACCEPT_LICENSE_AGREEMENT: \"yes\"\n    ports:\n    - \"0.0.0.0:7474:7474\"\n    - \"0.0.0.0:7687:7687\"\n</code></pre> </li> </ol>"},{"location":"BioCypher/learn/guides/ontologies/","title":"Handling Ontologies","text":"<p>BioCypher relies on ontologies to ground the knowledge graph contents in biology. This has the advantages of providing machine readability and therefore automation capabilities as well as making working with BioCypher accessible to biologically oriented researchers. However, it also means that BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required. In the following we will cover the basics of ontologies and how to use them in BioCypher.</p>"},{"location":"BioCypher/learn/guides/ontologies/#what-is-an-ontology","title":"What is an ontology?","text":"<p>An ontology is a formal representation of a domain of knowledge. It is a hierarchical structure of concepts and relations. The concepts are organized into a hierarchy, where each concept is a subclass of a more general concept. For instance, a wardrobe is a subclass of a piece of furniture. Individual wardrobes, such as yours or mine, are instances of the concept wardrobe, and as such would be represented as Wardrobe nodes in a knowledge graph. In BioCypher, these nodes would additionally inherit the PieceOfFurniture label from the ontological hierarchy of things.</p> <p>Note</p> <p>Why is the class called piece of furniture but the label is PieceOfFurniture?</p> <p>The Biolink model uses two different case notations for its labels: the \"internal\" designation of classes is in lower sentence case (\"protein\", \"pairwise molecular interaction\"), while the \"external\" designation is in PascalCase (\"Protein\", \"PairwiseMolecularInteraction\"). BioCypher uses the same paradigm: in most cases (input, schema configuration, internally), the lower sentence case is used, while in the output (Neo4j labels, file system names) the PascalCase is more suitable; Neo4j labels and system file names don't deal well with spaces and special characters. Therefore, we check the output file names for their compliance with the Neo4j naming rules. All non compliant characters are removed from the file name (e.g. if the ontology class is called \"desk (piece of furniture)\", the brackets would be removed and the file name will be \"DeskPieceOfFurniture\"). We also remove the \"biolink:\" CURIE prefix for use in file names and Neo4j labels.</p> <p>The relations between concepts can also be organized into a hierarchy. In the specific case of a Neo4j graph, however, relationships cannot possess multiple labels; therefore, if concept inheritance is desired for relationships, they need to be \"reified\", i.e., turned into nodes. BioCypher provides a simple way of converting edges to nodes and vice versa (using the <code>represented_as</code> field). For a more in-depth explanation of ontologies, we recommend this introduction.</p>"},{"location":"BioCypher/learn/guides/ontologies/#how-biocypher-uses-ontologies","title":"How BioCypher uses ontologies","text":"<p>BioCypher is agnostic to the choice of ontology. Practically, we have built our initial projects around the Biolink model, because it provides a large but shallow collection of concepts that are relevant to the biomedical domain. Other examples of generalist ontologies are the Experimental Factor Ontology and the Basic Formal Ontology. To account for the specific requirements of expert systems, it is possible to use multiple ontologies in the same project. For instance, one might want to extend the rather basic classes relating to molecular interactions in Biolink (the most specific being <code>pairwise molecular interaction</code>) with more specific classes from a more domain-specific ontology, such as the EBI molecular interactions ontology (PSI-MI). A different project may need to define very specific genetics concepts, and thus extend the Biolink model at the terminal node <code>sequence variant</code> with the corresponding subtree of the Sequence Ontology. The OBO Foundry and the BioPortal collect many such specialised ontologies.</p> <p>The default format for ingesting ontology definitions into BioCypher is the Web Ontology Language (OWL); BioCypher can read <code>.owl</code>, <code>.rdf</code>, and <code>.ttl</code> files. The preferred way to specify the ontology or ontologies to be used in a project is to specify them in the biocypher configuration file (<code>biocypher_config.yaml</code>). This file is used to specify the location of the ontology files, as well as the root node of the main (\"head\") ontology and join nodes as fusion points for all \"tail\" ontologies. For more info, see the section on hybridising ontologies.</p>"},{"location":"BioCypher/learn/guides/ontologies/#visualising-ontologies","title":"Visualising ontologies","text":"<p>BioCypher provides a simple way of visualising the ontology hierarchy. This is useful for debugging and for getting a quick overview of the ontology and which parts are actually used in the knowledge graph to be created. Depending on your use case you can either visualise the parts of the ontology used in the knowledge graph (sufficient for most use cases) or the full ontology. If the used ontology is more complex and contains multiple inheritance please refer to the section on visualising complex ontologies.</p>"},{"location":"BioCypher/learn/guides/ontologies/#visualise-only-the-parts-of-the-ontology-used-in-the-knowledge-graph","title":"Visualise only the parts of the ontology used in the knowledge graph","text":"<p>To get an overview of the structure of our project, we can run the following command via the interface:</p> Visualising the ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure()\n</code></pre> <p>This will build the ontology scaffold and print a tree visualisation of its hierarchy to the console using the treelib library. You can see this in action in tutorial part 6 (<code>tutorial/06_relationships.py</code>). The output will look something like this:</p> <pre><code>Showing ontology structure, based on Biolink 3.0.3:\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u251c\u2500\u2500 mixin\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre> <p>Note</p> <p>BioCypher will only show the parts of the ontology that are actually used in the knowledge graph with the exception of intermediary nodes that are needed to build a complete tree. For instance, the <code>protein</code> class is linked to the root class <code>entity</code> via <code>polypeptide</code>, <code>biological entity</code>, and <code>named thing</code>, all of which are not part of the input data.</p>"},{"location":"BioCypher/learn/guides/ontologies/#visualise-the-full-ontology","title":"Visualise the full ontology","text":"<p>If you want to see the complete ontology tree, you can call <code>show_ontology_structure</code> with the parameter <code>full=True</code>.</p> Visualising the full ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure(full=True)\n</code></pre>"},{"location":"BioCypher/learn/guides/ontologies/#visualise-complex-ontologies","title":"Visualise complex ontologies","text":"<p>Not all ontologies can be easily visualised as a tree, such as ontologies with multiple inheritance, where classes in the ontology can have multiple parent classes. This violates the definition of a tree, where each node can only have one parent node. Consequently, ontologies with multiple inheritance cannot be visualised as a tree.</p> <p>BioCypher can still handle these ontologies, and you can call <code>show_ontology_structure()</code> to get a visualisation of the ontology. However, each ontology class will only be added to the hierarchy tree once (a class with multiple parent classes is only placed under one parent in the hierarchy tree). Since this will occur the first time the class is seen, the ontology class might not be placed where you would expect it. This only applies to the visualisation; the underlying ontology is still correct and contains all ontology classes and their relationships.</p> <p>Note</p> <p>When calling <code>show_ontology_structure()</code>, BioCypher automatically checks if the ontology contains multiple inheritance and logs a warning message if so.</p> <p>If you need to get a visualisation of the ontology with multiple inheritance, you can call <code>show_ontology_structure()</code> with the parameter <code>to_disk=/some/path/where_to_store_the_file</code>. This creates a <code>GraphML</code> file and stores it at the specified location.</p>"},{"location":"BioCypher/learn/guides/ontologies/#using-ontologies-plain-biolink","title":"Using ontologies: plain Biolink","text":"<p>BioCypher maps any input data to the underlying ontology; in the basic case, the Biolink model. This mapping is defined in the schema configuration (<code>schema_config.yaml</code>, see also here). In the simplest case, the representation of a concept in the knowledge graph to be built and the Biolink model class representing this concept are synonymous. For instance, the concept protein is represented by the Biolink class protein. To introduce proteins into the knowledge graph, one would simply define a node constituent with the class label protein. This is the mechanism we implicitly used for proteins in the basic tutorial (part 1); to reiterate:</p> schema_config.yaml<pre><code>protein:\n  represented_as: node\n  # ...\n</code></pre>"},{"location":"BioCypher/learn/guides/ontologies/#model-extensions","title":"Model extensions","text":"<p>There are multiple reasons why a user might want to modify the basic model of the ontology or ontologies used. A class that is relevant to the user's task might be missing (Explicit inheritance). A class might not be granular enough, and the user would like to split it into subclasses based on distinct inputs (Implicit inheritance). For some very common use cases, we recommend going one step further and, maybe after some testing using the above \"soft\" model extensions, proposing the introduction of a new class to the model itself. For instance, Biolink is an open source community project, and new classes can be requested by opening an issue or filing a pull request directly on the Biolink model GitHub repository. Similar mechanisms apply for OBO Foundry ontologies.</p> <p>BioCypher provides further methods for ontology manipulation. The name of a class of the model may be too unwieldy for the use inside the desired knowledge graph, and the user would like to introduce a synonym/alias (Synonyms). Finally, the user might want to extend the basic model with another, more specialised ontology (Hybridising ontologies).</p>"},{"location":"BioCypher/learn/guides/ontologies/#explicit-inheritance","title":"Explicit inheritance","text":"<p>Explicit inheritance is the most straightforward way of extending the basic model. It is also the most common use case. For instance, the Biolink model does not contain a class for <code>protein isoform</code>, and neither does it contain a relationship class for <code>protein protein interaction</code>, both of which we have already used in the basic tutorial. Since protein isoforms are specific types of protein, it makes sense to extend the existing Biolink model class <code>protein</code> with the concept of protein isoforms. To do this, we simply add a new class <code>protein isoform</code> to the schema configuration, and specify that it is a subclass of <code>protein</code> using the (optional) <code>is_a</code> field:</p> schema_config.yaml<pre><code>protein isoform:\n  is_a: protein\n  represented_as: node\n  # ...\n</code></pre> <p>Explicit inheritance can also be used to introduce new relationship classes. However, if the output is a Neo4j graph, these relationships must be represented as nodes to provide full functionality, since edges do not allow multiple labels. This does not mean that explicit inheritance cannot be used in edges; it is even recommended to do so to situate all components of the knowledge graph in the ontological hierarchy. However, to have the ancestry represented in the resulting Neo4j graph DB, multiple labels are required. For instance, we have already used the <code>protein protein interaction</code> relationship in the basic tutorial (part 6), making it a child of the Biolink model class <code>pairwise molecular interaction</code>. To reiterate:</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  # ...\n</code></pre> <p>The <code>is_a</code> field can be used to specify multiple inheritance, i.e., multiple ancestor classes and their direct parent-child relationships can be created by specifying multiple classes (as a list) in the <code>is_a</code> field. For instance, if we wanted to further extend the protein-protein interaction with a more specific <code>enzymatic interaction</code> class, we could do so as follows:</p> schema_config.yaml<pre><code>enzymatic interaction:\n  is_a: [protein protein interaction, pairwise molecular interaction]\n  represented_as: node\n  # ...\n</code></pre> <p>Note</p> <p>To create this multiple inheritance chain, we do not require the creation of a <code>protein protein interaction</code> class as shown above; all intermediary classes are automatically created by BioCypher and inserted into the ontological hierarchy. To obtain a continuous ontology tree, the target class (i.e., the last in the list) must be a real Biolink model class.</p>"},{"location":"BioCypher/learn/guides/ontologies/#implicit-inheritance","title":"Implicit inheritance","text":"<p>The base model (in the standard case, Biolink) can also be extended without specifying an explicit <code>is_a</code> field. This \"implicit\" inheritance happens when a class has multiple input labels that each refer to a distinct preferred identifier. In other words, if both the <code>input_label</code> and the <code>preferred_id</code> fields of a schema configuration class are lists, BioCypher will automatically create a subclass for each of the preferred identifiers. This is demonstrated in part 3 of the basic tutorial.</p> <p>Caution</p> <p>If only the <code>input_label</code> field - but not the <code>preferred_id</code> field - is a list, BioCypher will merge the inputs instead. This is useful for cases where different input streams should be unified under the same class label. See part 2 of the basic tutorial for more information.</p> <p>To make this more concrete, let's consider the example of <code>pathway</code> annotations. There are multiple projects that provide pathway annotations, such as Reactome and Wikipathways, and, in contrast to proteins, pathways are not easily mapped one-to-one. For classes where mapping is difficult or even impossible, we can use implicit subclassing instead. The Biolink model contains a <code>pathway</code> class, which we can use as a parent class of the Reactome and Wikipathways classes; we simply need to provide the pathways as two separate inputs with their own labels (e.g., \"react\" and \"wiki\"), and specify a corresponding list of preferred identifiers in the <code>preferred_id</code> field:</p> schema_config.yaml<pre><code>pathway:\n  represented_as: node\n  preferred_id: [reactome, wikipathways]\n  input_label: [react, wiki]\n  # ...\n</code></pre> <p>This will prompt BioCypher to create two subclasses of <code>pathway</code>, one for each input, and to map the input data to these subclasses. In the resulting knowledge graph, the Reactome and Wikipathways pathways will be represented as distinct classes by prepending the preferred identifier to the class label: <code>Reactome.Pathway</code> and <code>Wikipathways.Pathway</code>. By virtue of BioCypher's multiple labelling paradigm, those nodes will also inherit the <code>Pathway</code> class label as well as all parent labels and mixins of <code>Pathway</code> (<code>BiologicalProcess</code>, etc.). This allows us to query the graph for all <code>Pathway</code> nodes as well as for specific datasets depending on the desired granularity.</p> <p>Note</p> <p>This also works for relationships, but in this case, not the preferred identifiers but the sources (defined in the <code>source</code> field) are used to create the subclasses.</p>"},{"location":"BioCypher/learn/guides/ontologies/#synonyms","title":"Synonyms","text":"<p>Note: Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/07__synonyms.py</code>. Schema files are at <code>tutorial/07_schema_config.yaml</code>, configuration in <code>tutorial/07_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In some cases, an ontology may contain a biological concept, but the name of the concept does for some reason not agree with the users desired knowledge graph structure. For instance, the user may not want to represent protein complexes in the graph as <code>macromolecular complex</code> nodes due to ease of use and/or readability criteria and rather call these nodes <code>complex</code>. In such cases, the user can introduce a synonym for the ontology class. This is done by selecting another, more desirable name for the respective class(es) and specifying the <code>synonym_for</code> field in their schema configuration. In this case, as we would like to represent protein complexes as <code>complex</code> nodes, we can do so as follows:</p> schema_config.yaml<pre><code>complex:\n  synonym_for: macromolecular complex\n  represented_as: node\n  # ...\n</code></pre> <p>Importantly, BioCypher preserves these mappings to enable compatibility between different structural instantiations of the ontology (or combination of ontologies). All entities that are mapped to ontology classes in any way can be harmonised even between different types of concrete representations.</p> <p>Note</p> <p>It is essential that the desired class name is used as the main class key in the schema configuration, and the ontology class name is given in the <code>synonym_for</code> field. The name given in the <code>synonym_for</code> field must be an existing class name (in this example, a real Biolink class).</p> <p>We can visualise the structure of the ontology as we have before. Instead of using <code>bc.show_ontology_structure()</code> however, we can use the <code>bc.summary()</code> method to show the structure and simultaneously check for duplicates and missing labels. This is useful for debugging purposes, and we can see that the import was completed without encountering duplicates, and all labels in the input are accounted for in the schema configuration. We also observe in the tree that the <code>complex</code> class is now a synonym for the <code>macromolecular complex</code> class (their being synonyms indicated as an equals sign):</p> <pre><code>Showing ontology structure based on https://raw.githubusercontent.com/biolink/biolink-model/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u251c\u2500\u2500 complex = macromolecular complex\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre>"},{"location":"BioCypher/learn/guides/ontologies/#hybridising-ontologies","title":"Hybridising ontologies","text":"<p>A broad, general ontology is a useful tool for knowledge representation, but often the task at hand requires more specific and granular concepts. In such cases, it is possible to hybridise the general ontology with a more specific one. For instance, there are many different types of sequence variants in biology, but Biolink only provides a generic \"sequence variant\" class (and it clearly exceeds the scope of Biolink to provide granular classes for all thinkable cases). However, there are many specialist ontologies, such as the Sequence Ontology (SO), which provides a more granular representation of sequence variants, and MONDO, which provides a more granular representation of diseases.</p> <p>To hybridise the Biolink model with the SO and MONDO, we can use the generic ontology adapter class of BioCypher by providing \"tail ontologies\" as dictionaries consisting of an OWL format ontology file and a set of nodes, one in the head ontology (which by default is Biolink), and one in the tail ontology. Each of the tail ontologies will then be joined to the head ontology to form the hybridised ontology at the specified nodes. It is up to the user to make sure that the concept at which the ontologies shall be joined makes sense as a point of contact between the ontologies; ideally, it is the exact same concept.</p> <p>Hint</p> <p>If the concept does not exist in the head ontology, but is a feasible child class of an existing concept, you can set the <code>merge_nodes</code> option to <code>False</code> to prevent the merging of head and tail join nodes, but instead adding the tail join node as a child of the head join node you have specified. For instance, in the example below, we merge <code>sequence variant</code> from Biolink and <code>sequence_variant</code> from Sequence Ontology into a single node, but we add the MONDO subtree of <code>human disease</code> as a child of <code>disease</code> in Biolink.</p> <p><code>merge_nodes</code> is set to <code>True</code> by default, so there is no need to specify it in the configuration file if you want to merge the nodes.</p> <p>The ontology adapter also accepts any arbitrary \"head ontology\" as a base ontology, but if none is provided, the Biolink model is used as the default head ontology. However, it is strongly recommended to explicitly specify your desired ontology version here. These options can be provided to the BioCypher interface as parameters, or as options in the BioCypher configuration file, which is the preferred method for transparency reasons:</p> Using biocypher_config.yaml<pre><code># ...\n\nbiocypher:  # biocypher settings\n\n  # Ontology configuration\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n\n  tail_ontologies:\n\n    so:\n      url: data/so.owl\n      head_join_node: sequence variant\n      tail_join_node: sequence_variant\n\n    mondo:\n      url: http://purl.obolibrary.org/obo/mondo.owl\n      head_join_node: disease\n      tail_join_node: human disease\n      merge_nodes: false\n\n# ...\n</code></pre> <p>Note</p> <p>The <code>url</code> parameter can be either a local path or a URL to a remote resource.</p> <p>If you need to pass the ontology configuration programmatically, you can do so as follows at BioCypher interface instantiation:</p> Programmatic usage<pre><code>bc = BioCypher(\n    # ...\n\n    head_ontology={\n      'url': 'https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl',\n      'root_node': 'entity',\n    },\n\n    tail_ontologies={\n        'so':\n            {\n                'url': 'test/ontologies/so.owl',\n                'head_join_node': 'sequence variant',\n                'tail_join_node': 'sequence_variant',\n            },\n        'mondo':\n            {\n                'url': 'test/ontologies/mondo.owl',\n                'head_join_node': 'disease',\n                'tail_join_node': 'human disease',\n                'merge_nodes': False,\n            }\n    },\n\n    # ...\n)\n</code></pre>"},{"location":"BioCypher/learn/tutorials/","title":"Tutorials","text":"<p>The BioCypher tutorials are written as Jupyter notebooks and run directly in Google Colab\u2014a hosted notebook environment that requires no setup. At the top of each tutorial, you'll see a Run in Google Colab button. Click the button to open the notebook and run the code yourself.</p>"},{"location":"BioCypher/learn/tutorials/#list-of-tutorials-current-status","title":"List of Tutorials (current status)","text":"<ol> <li>Getting started</li> <li>Example Notebook: BioCypher and Pandas</li> <li>Tutorial - Basics</li> <li>Tutorial - Handling Ontologies</li> <li>Tutorial - Adapters</li> </ol>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/","title":"Example Notebook: BioCypher and Pandas","text":"Tip: Run the tutorial interactively in      Google Colab.  <p>While BioCypher was designed as a graph-focused framework, due to commonalities in bioinformatics workflows, BioCypher also supports Pandas DataFrames. This allows integration with methods that use tabular data, such as machine learning and statistical analysis, for instance in the scVerse framework.</p> <p>To run this tutorial interactively, you will first need to install perform some setup steps specific to running on Google Colab. You can collapse this section and run the setup steps with one click, as they are not required for the explanation of BioCyper's functionality. You can of course also run the steps one by one, if you want to see what is happening. The real tutorial starts with section 1, \"Adding data\" (do not follow this link on colab, as you will be taken back to the website; please scroll down instead).</p> In\u00a0[\u00a0]: Copied! <pre>!pip install biocypher\n</pre> !pip install biocypher In\u00a0[\u00a0]: Copied! <pre>import yaml\nimport requests\nimport subprocess\n\nschema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\"\n</pre> import yaml import requests import subprocess  schema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\" In\u00a0[\u00a0]: Copied! <pre>!wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\"\n</pre> !wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\" In\u00a0[\u00a0]: Copied! <pre>owner = \"biocypher\"\nrepo = \"biocypher\"\npath = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory)\ngithub_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n\napi_url = github_url.format(owner=owner, repo=repo, path=path)\nresponse = requests.get(api_url)\n\n# Get list of yaml files from the repo\nfiles = response.json()\nyamls = []\nfor file in files:\n    if file[\"type\"] == \"file\":\n        if file[\"name\"].endswith(\".yaml\"):\n            yamls.append(file[\"name\"])\n\n# wget all yaml files\nfor yaml in yamls:\n    url_path = schema_path + yaml\n    subprocess.run([\"wget\", url_path])\n</pre> owner = \"biocypher\" repo = \"biocypher\" path = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory) github_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"  api_url = github_url.format(owner=owner, repo=repo, path=path) response = requests.get(api_url)  # Get list of yaml files from the repo files = response.json() yamls = [] for file in files:     if file[\"type\"] == \"file\":         if file[\"name\"].endswith(\".yaml\"):             yamls.append(file[\"name\"])  # wget all yaml files for yaml in yamls:     url_path = schema_path + yaml     subprocess.run([\"wget\", url_path]) <p>Let's also define functions with which we can visualize those</p> In\u00a0[\u00a0]: Copied! <pre># helper function to print yaml files\nimport yaml\ndef print_yaml(file_path):\n    with open(file_path, 'r') as file:\n        yaml_data = yaml.safe_load(file)\n\n    print(\"--------------\")\n    print(yaml.dump(yaml_data, sort_keys=False, indent=4))\n    print(\"--------------\")\n</pre> # helper function to print yaml files import yaml def print_yaml(file_path):     with open(file_path, 'r') as file:         yaml_data = yaml.safe_load(file)      print(\"--------------\")     print(yaml.dump(yaml_data, sort_keys=False, indent=4))     print(\"--------------\") In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nfrom data_generator import Protein\nn_proteins = 3\nproteins = [Protein() for _ in range(n_proteins)]\n</pre> # create a list of proteins to be imported from data_generator import Protein n_proteins = 3 proteins = [Protein() for _ in range(n_proteins)] <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is - purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> In\u00a0[\u00a0]: Copied! <pre>def node_generator(proteins):\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties(),\n        )\nentities = node_generator(proteins)\n</pre> def node_generator(proteins):     for protein in proteins:         yield (             protein.get_id(),             protein.get_label(),             protein.get_properties(),         ) entities = node_generator(proteins) <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('01_schema_config.yaml')\n</pre> print_yaml('01_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_protein\n\n--------------\n</pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p> In\u00a0[\u00a0]: Copied! <pre>from biocypher import BioCypher\nbc = BioCypher(\n    biocypher_config_path='01_biocypher_config.yaml',\n    schema_config_path='01_schema_config.yaml',\n)\n# Add the entities that we generated above to the graph\nbc.add(entities)\n</pre> from biocypher import BioCypher bc = BioCypher(     biocypher_config_path='01_biocypher_config.yaml',     schema_config_path='01_schema_config.yaml', ) # Add the entities that we generated above to the graph bc.add(entities) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Print the graph as a dictionary of pandas DataFrame(s) per node label\nbc.to_df()[\"protein\"]\n</pre> # Print the graph as a dictionary of pandas DataFrame(s) per node label bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  F7V4U2  RMFDDRFPVELRICTGSLVIINLGEFAEQHDKQDGSKPSHQPMFAT...   \n 1  K2Y8U3  HWPPSGVSCGVFPECWYRWRDEQWACFGPHIKYNKDNTWSWAQWMH...   \n 2  L1V6V9  QAEPKYKLAQENCRVQIKLPKIVGTCRPHWMTKTYHVLHTCVLWKS...   \n \n            description taxon      id preferred_id  \n 0  i f c m m q e o o s  9606  F7V4U2      uniprot  \n 1  e y p g j t j y r x  9606  K2Y8U3      uniprot  \n 2  a i b t l j e g n j  9606  L1V6V9      uniprot  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import Protein, EntrezProtein\n</pre> from data_generator import Protein, EntrezProtein In\u00a0[\u00a0]: Copied! <pre>print_yaml('02_schema_config.yaml')\n</pre> print_yaml('02_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [Protein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# Create a new BioCypher instance\nbc = BioCypher(\n    biocypher_config_path='02_biocypher_config.yaml',\n    schema_config_path='02_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n</pre> # Create a list of proteins to be imported proteins = [     p for sublist in zip(         [Protein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # Create a new BioCypher instance bc = BioCypher(     biocypher_config_path='02_biocypher_config.yaml',     schema_config_path='02_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins)) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein\"]\n</pre> bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1  186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 2  S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 3  926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 4  Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n 5  362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  b c q m l d a u u g  9606  186009      uniprot  \n 2  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 3  t n a j d l j a t a  9606  926766      uniprot  \n 4  h d m k q n r e h r  9606  Z1F6R2      uniprot  \n 5  l m x k h m v g p y  9606  362641      uniprot  }</pre> <p>This again creates a single DataFrame, now for both protein types, but now including both input streams (you should note both uniprot &amp; entrez style IDs in the id column). However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('03_schema_config.yaml')\n</pre> print_yaml('03_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p>  The only change affected upon the code from the previous section is the referral to the updated schema configuration file.   In the output, we now generate two separate files for the `protein` class, one for each subclass (with names in PascalCase).  <p>Let's create a DataFrame with the same nodes as above, but with a different schema configuration:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='03_biocypher_config.yaml',\n    schema_config_path='03_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> bc = BioCypher(     biocypher_config_path='03_biocypher_config.yaml',     schema_config_path='03_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1          S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 2          Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 2  h d m k q n r e h r  9606  Z1F6R2      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 1         926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 2         362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  b c q m l d a u u g  9606  186009       entrez  \n 1  t n a j d l j a t a  9606  926766       entrez  \n 2  l m x k h m v g p y  9606  362641       entrez  }</pre> <p>Now we see two separate DataFrames, one for each subclass of the <code>protein</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('04_schema_config.yaml')\n</pre> print_yaml('04_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\n\n--------------\n</pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p>  If we wanted to ignore the mass value for all properties, we could simply remove the `mass` key from the `properties` dictionary.  In\u00a0[\u00a0]: Copied! <pre>from data_generator import EntrezProtein, RandomPropertyProtein\n</pre> from data_generator import EntrezProtein, RandomPropertyProtein In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported (now with properties)\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# New instance, populated, and to DataFrame\nbc = BioCypher(\n    biocypher_config_path='04_biocypher_config.yaml',\n    schema_config_path='04_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # Create a list of proteins to be imported (now with properties) proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # New instance, populated, and to DataFrame bc = BioCypher(     biocypher_config_path='04_biocypher_config.yaml',     schema_config_path='04_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          S1Z9L5  RHLRGDVMQEDHHTSSERMVYNVLPQDYKVVSCEYWNTQVTALWVI...   \n 1          W9J5F1  IPFSQSAWAQQRIGPKGTKAHGVTQPAPMDIKNLCNLTDLTLILDF...   \n 2          T1J3U0  WFGCCHKQYVSHVIDRQDPQSPSDNPSLVSQLQFFMWGIQIQNGEI...   \n \n            description taxon  mass      id preferred_id  \n 0  u x e o k m a i o s  3899  None  S1Z9L5      uniprot  \n 1  i x k c r b p d d p  8873  None  W9J5F1      uniprot  \n 2  m a w r r u x c w o  1966  9364  T1J3U0      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         405878  RMTDGFEWQLDFHAFIWCNQAAWQLPLEVHISQGNGGWRMGLYGNM...   \n 1         154167  CGMNYDNGYFSVAYQSYDLWYHQQLKTRGVKPAEKDSDKDLGIDVI...   \n 2         234189  GQWQECIQGFTPQQMCVDCCAETKLANKSYYHSWMTWRLSGLCFNM...   \n \n            description taxon  mass      id preferred_id  \n 0  y c s v s n e c h o  9606  None  405878       entrez  \n 1  i k n c e n r n c d  9606  None  154167       entrez  \n 2  o v w y g h y e v y  9606  None  234189       entrez  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import RandomPropertyProteinIsoform\n</pre> from data_generator import RandomPropertyProteinIsoform In\u00a0[\u00a0]: Copied! <pre>print_yaml('05_schema_config.yaml')\n</pre> print_yaml('05_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\n\n--------------\n</pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>We now create three separate DataFrames, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p> In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [RandomPropertyProteinIsoform() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n\n# Create BioCypher driver\nbc = BioCypher(\n    biocypher_config_path='05_biocypher_config.yaml',\n    schema_config_path='05_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # create a list of proteins to be imported proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [RandomPropertyProteinIsoform() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ]  # Create BioCypher driver bc = BioCypher(     biocypher_config_path='05_biocypher_config.yaml',     schema_config_path='05_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins))  for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n  uniprot.protein                                           sequence  \\\n0          A9L6G4  SWIVVGQPDSHNKRLVNYHWMRCEHPLRCWRPIYVVRVSFQSQCEQ...   \n1          E4N2H2  PGVMILDNMQHKCSKELSTRQIITNHWICNSAPISWSSGMDRSCLD...   \n2          V4F1T1  DQCHNLCPGSSFQCPENAFGNDWIDHMPQETGLMQYDDPQSGMWFT...   \n\n           description taxon  mass      id preferred_id  \n0  m o k j a f w v w r  4220  None  A9L6G4      uniprot  \n1  n v i r s f m f d w  6339  6481  E4N2H2      uniprot  \n2  w e v v a b o b b u  9176  6510  V4F1T1      uniprot  \nprotein isoform\n  protein isoform                                           sequence  \\\n0          F0N9A4  QDVVLVEGCGDEGWIHMPEKRPGQAYKWCERFRPIPDFTNSIKIAY...   \n1          B1W6O2  SQKHFRRWWTNDCFGQELMSIYYNVKFWDNLIEMTGGPASRVCLGQ...   \n2          G6V5R9  ASAITPFSYEKPHTVTLDATEVFPKMQDAQAIEREIHFSKSTLVYG...   \n\n           description taxon  mass      id preferred_id  \n0  r f e a v a a g w r  8061  None  F0N9A4      uniprot  \n1  a c a v v k v k c w  6786  None  B1W6O2      uniprot  \n2  c k g d a l f r t v  6868  1323  G6V5R9      uniprot  \nentrez.protein\n  entrez.protein                                           sequence  \\\n0          52329  DYRSMAPTFILMKIYPACDAITKRRWSVATVKDGEFIWWSAVKIFP...   \n1         581107  LLVFNMGQLAVAGYGNTMVSAMMCFCCDVKARMGMSWLPKITTMQW...   \n2         270569  MVCSHHELAVAFQTMCPIQGDAATAKANAHRTTDKQNWMVVKWFRT...   \n\n           description taxon  mass      id preferred_id  \n0  q k r b h g t q x x  9606  None   52329       entrez  \n1  h f g z j r b g m w  9606  None  581107       entrez  \n2  s b p v f u t y g v  9606  None  270569       entrez  \n</pre> In\u00a0[\u00a0]: Copied! <pre>print_yaml('06_schema_config_pandas.yaml')\n</pre> print_yaml('06_schema_config_pandas.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\nprotein protein interaction:\n    is_a: pairwise molecular interaction\n    represented_as: edge\n    preferred_id: intact\n    input_label: interacts_with\n    properties:\n        method: str\n        source: str\n\n--------------\n</pre> <p>Now that we have added <code>protein protein interaction</code> as an edge, we have to simulate some interactions:</p> In\u00a0[\u00a0]: Copied! <pre>from data_generator import InteractionGenerator\n\n# Simulate edges for proteins we defined above\nppi = InteractionGenerator(\n    interactors=[p.get_id() for p in proteins],\n    interaction_probability=0.05,\n).generate_interactions()\n</pre> from data_generator import InteractionGenerator  # Simulate edges for proteins we defined above ppi = InteractionGenerator(     interactors=[p.get_id() for p in proteins],     interaction_probability=0.05, ).generate_interactions() In\u00a0[\u00a0]: Copied! <pre># naturally interactions/edges contain information about the interacting source and target nodes\n# let's look at the first one in the list\ninteraction = ppi[0]\nf\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\"\n</pre> # naturally interactions/edges contain information about the interacting source and target nodes # let's look at the first one in the list interaction = ppi[0] f\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\" Out[\u00a0]: <pre>'A9L6G4 interacts_with V4F1T1'</pre> In\u00a0[\u00a0]: Copied! <pre># similarly to nodes, it also has a dictionary of properties\ninteraction.get_properties()\n</pre> # similarly to nodes, it also has a dictionary of properties interaction.get_properties() Out[\u00a0]: <pre>{'source': 'signor', 'method': 'u z c x m d c u g s'}</pre> <p>As with nodes, we add first createa a new BioCypher instance, and then populate it with nodes as well as edges:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='06_biocypher_config.yaml',\n    schema_config_path='06_schema_config_pandas.yaml',\n)\n</pre> bc = BioCypher(     biocypher_config_path='06_biocypher_config.yaml',     schema_config_path='06_schema_config_pandas.yaml', ) In\u00a0[\u00a0]: Copied! <pre># Extract id, source, target, label, and property dictionary\ndef edge_generator(ppi):\n    for interaction in ppi:\n        yield (\n            interaction.get_id(),\n            interaction.get_source_id(),\n            interaction.get_target_id(),\n            interaction.get_label(),\n            interaction.get_properties(),\n        )\n\nbc.add(node_generator(proteins))\nbc.add(edge_generator(ppi))\n</pre> # Extract id, source, target, label, and property dictionary def edge_generator(ppi):     for interaction in ppi:         yield (             interaction.get_id(),             interaction.get_source_id(),             interaction.get_target_id(),             interaction.get_label(),             interaction.get_properties(),         )  bc.add(node_generator(proteins)) bc.add(edge_generator(ppi))  <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <p>Let's look at the interaction DataFrame:</p> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein protein interaction\"]\n</pre> bc.to_df()[\"protein protein interaction\"] Out[\u00a0]: protein protein interaction _from _to source method 0 intact703256 A9L6G4 V4F1T1 signor u z c x m d c u g s 1 None E4N2H2 F0N9A4 intact None <p>Finally, it is worth noting that BioCypher relies on ontologies, which are machine readable representations of domains of knowledge that we use to ground the contents of our knowledge graphs. While details about ontologies are out of scope for this tutorial, and are described in detail in the BioCypher documentation, we can still have a glimpse at the ontology that we used implicitly in this tutorial:</p> In\u00a0[\u00a0]: Copied! <pre>bc.show_ontology_structure()\n</pre> bc.show_ontology_structure() <pre>Showing ontology structure based on https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</pre> Out[\u00a0]: <pre>&lt;treelib.tree.Tree at 0x7f7327b3a880&gt;</pre>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#example-notebook-biocypher-and-pandas","title":"Example Notebook: BioCypher and Pandas\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#introduction","title":"Introduction\u00b6","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data, and thus save development time in the maintenance of curated knowledge graphs, while allowing simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two other versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find the former in the tutorial directory of the BioCypher repository. This tutorial simply takes the latter, in-memory approach to a Jupyter notebook.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#setup","title":"Setup\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#tutorial-files","title":"Tutorial files\u00b6","text":"<p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files, specifically the <code>.yaml</code> files, are named according to the tutorial step they are used in.</p> <p>Let's download these:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#configuration","title":"Configuration\u00b6","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#section-1-adding-data","title":"Section 1: Adding data\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#input-data-stream-adapter","title":"Input data stream (\"adapter\")\u00b6","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#schema-configuration","title":"Schema configuration\u00b6","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)\u00b6","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#section-2-merging-data","title":"Section 2: Merging data\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#plain-merge","title":"Plain merge\u00b6","text":"<p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#ad-hoc-subclassing","title":"Ad hoc subclassing\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#section-3-handling-properties","title":"Section 3: Handling properties\u00b6","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#designated-properties","title":"Designated properties\u00b6","text":"<p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li><p><code>str</code> (or <code>string</code>),</p> </li> <li><p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li><p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li><p><code>bool</code> (or <code>boolean</code>),</p> </li> <li><p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#inheriting-properties","title":"Inheriting properties\u00b6","text":"<p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial/#section-4-handling-relationships","title":"Section 4: Handling relationships\u00b6","text":"<p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can modify the schema configuration above to the following:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/","title":"Example Notebook: BioCypher and Pandas","text":"Tip: Run the tutorial interactively in      Google Colab.  <p>While BioCypher was designed as a graph-focused framework, due to commonalities in bioinformatics workflows, BioCypher also supports Pandas DataFrames. This allows integration with methods that use tabular data, such as machine learning and statistical analysis, for instance in the scVerse framework.</p> <p>To run this tutorial interactively, you will first need to install perform some setup steps specific to running on Google Colab. You can collapse this section and run the setup steps with one click, as they are not required for the explanation of BioCyper's functionality. You can of course also run the steps one by one, if you want to see what is happening. The real tutorial starts with section 1, \"Adding data\" (do not follow this link on colab, as you will be taken back to the website; please scroll down instead).</p> In\u00a0[\u00a0]: Copied! <pre>!pip install biocypher\n</pre> !pip install biocypher In\u00a0[\u00a0]: Copied! <pre>import yaml\nimport requests\nimport subprocess\n\nschema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\"\n</pre> import yaml import requests import subprocess  schema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\" In\u00a0[\u00a0]: Copied! <pre>!wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\"\n</pre> !wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\" In\u00a0[\u00a0]: Copied! <pre>owner = \"biocypher\"\nrepo = \"biocypher\"\npath = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory)\ngithub_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n\napi_url = github_url.format(owner=owner, repo=repo, path=path)\nresponse = requests.get(api_url)\n\n# Get list of yaml files from the repo\nfiles = response.json()\nyamls = []\nfor file in files:\n    if file[\"type\"] == \"file\":\n        if file[\"name\"].endswith(\".yaml\"):\n            yamls.append(file[\"name\"])\n\n# wget all yaml files\nfor yaml in yamls:\n    url_path = schema_path + yaml\n    subprocess.run([\"wget\", url_path])\n</pre> owner = \"biocypher\" repo = \"biocypher\" path = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory) github_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"  api_url = github_url.format(owner=owner, repo=repo, path=path) response = requests.get(api_url)  # Get list of yaml files from the repo files = response.json() yamls = [] for file in files:     if file[\"type\"] == \"file\":         if file[\"name\"].endswith(\".yaml\"):             yamls.append(file[\"name\"])  # wget all yaml files for yaml in yamls:     url_path = schema_path + yaml     subprocess.run([\"wget\", url_path]) <p>Let's also define functions with which we can visualize those</p> In\u00a0[\u00a0]: Copied! <pre># helper function to print yaml files\nimport yaml\ndef print_yaml(file_path):\n    with open(file_path, 'r') as file:\n        yaml_data = yaml.safe_load(file)\n\n    print(\"--------------\")\n    print(yaml.dump(yaml_data, sort_keys=False, indent=4))\n    print(\"--------------\")\n</pre> # helper function to print yaml files import yaml def print_yaml(file_path):     with open(file_path, 'r') as file:         yaml_data = yaml.safe_load(file)      print(\"--------------\")     print(yaml.dump(yaml_data, sort_keys=False, indent=4))     print(\"--------------\") In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nfrom data_generator import Protein\nn_proteins = 3\nproteins = [Protein() for _ in range(n_proteins)]\n</pre> # create a list of proteins to be imported from data_generator import Protein n_proteins = 3 proteins = [Protein() for _ in range(n_proteins)] <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is - purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> In\u00a0[\u00a0]: Copied! <pre>def node_generator(proteins):\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties(),\n        )\nentities = node_generator(proteins)\n</pre> def node_generator(proteins):     for protein in proteins:         yield (             protein.get_id(),             protein.get_label(),             protein.get_properties(),         ) entities = node_generator(proteins) <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('01_schema_config.yaml')\n</pre> print_yaml('01_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_protein\n\n--------------\n</pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p> In\u00a0[\u00a0]: Copied! <pre>from biocypher import BioCypher\nbc = BioCypher(\n    biocypher_config_path='01_biocypher_config.yaml',\n    schema_config_path='01_schema_config.yaml',\n)\n# Add the entities that we generated above to the graph\nbc.add(entities)\n</pre> from biocypher import BioCypher bc = BioCypher(     biocypher_config_path='01_biocypher_config.yaml',     schema_config_path='01_schema_config.yaml', ) # Add the entities that we generated above to the graph bc.add(entities) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Print the graph as a dictionary of pandas DataFrame(s) per node label\nbc.to_df()[\"protein\"]\n</pre> # Print the graph as a dictionary of pandas DataFrame(s) per node label bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  F7V4U2  RMFDDRFPVELRICTGSLVIINLGEFAEQHDKQDGSKPSHQPMFAT...   \n 1  K2Y8U3  HWPPSGVSCGVFPECWYRWRDEQWACFGPHIKYNKDNTWSWAQWMH...   \n 2  L1V6V9  QAEPKYKLAQENCRVQIKLPKIVGTCRPHWMTKTYHVLHTCVLWKS...   \n \n            description taxon      id preferred_id  \n 0  i f c m m q e o o s  9606  F7V4U2      uniprot  \n 1  e y p g j t j y r x  9606  K2Y8U3      uniprot  \n 2  a i b t l j e g n j  9606  L1V6V9      uniprot  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import Protein, EntrezProtein\n</pre> from data_generator import Protein, EntrezProtein In\u00a0[\u00a0]: Copied! <pre>print_yaml('02_schema_config.yaml')\n</pre> print_yaml('02_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [Protein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# Create a new BioCypher instance\nbc = BioCypher(\n    biocypher_config_path='02_biocypher_config.yaml',\n    schema_config_path='02_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n</pre> # Create a list of proteins to be imported proteins = [     p for sublist in zip(         [Protein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # Create a new BioCypher instance bc = BioCypher(     biocypher_config_path='02_biocypher_config.yaml',     schema_config_path='02_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins)) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein\"]\n</pre> bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1  186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 2  S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 3  926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 4  Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n 5  362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  b c q m l d a u u g  9606  186009      uniprot  \n 2  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 3  t n a j d l j a t a  9606  926766      uniprot  \n 4  h d m k q n r e h r  9606  Z1F6R2      uniprot  \n 5  l m x k h m v g p y  9606  362641      uniprot  }</pre> <p>This again creates a single DataFrame, now for both protein types, but now including both input streams (you should note both uniprot &amp; entrez style IDs in the id column). However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('03_schema_config.yaml')\n</pre> print_yaml('03_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p>  The only change affected upon the code from the previous section is the referral to the updated schema configuration file.   In the output, we now generate two separate files for the `protein` class, one for each subclass (with names in PascalCase).  <p>Let's create a DataFrame with the same nodes as above, but with a different schema configuration:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='03_biocypher_config.yaml',\n    schema_config_path='03_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> bc = BioCypher(     biocypher_config_path='03_biocypher_config.yaml',     schema_config_path='03_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1          S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 2          Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 2  h d m k q n r e h r  9606  Z1F6R2      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 1         926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 2         362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  b c q m l d a u u g  9606  186009       entrez  \n 1  t n a j d l j a t a  9606  926766       entrez  \n 2  l m x k h m v g p y  9606  362641       entrez  }</pre> <p>Now we see two separate DataFrames, one for each subclass of the <code>protein</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('04_schema_config.yaml')\n</pre> print_yaml('04_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\n\n--------------\n</pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p>  If we wanted to ignore the mass value for all properties, we could simply remove the `mass` key from the `properties` dictionary.  In\u00a0[\u00a0]: Copied! <pre>from data_generator import EntrezProtein, RandomPropertyProtein\n</pre> from data_generator import EntrezProtein, RandomPropertyProtein In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported (now with properties)\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# New instance, populated, and to DataFrame\nbc = BioCypher(\n    biocypher_config_path='04_biocypher_config.yaml',\n    schema_config_path='04_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # Create a list of proteins to be imported (now with properties) proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # New instance, populated, and to DataFrame bc = BioCypher(     biocypher_config_path='04_biocypher_config.yaml',     schema_config_path='04_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          S1Z9L5  RHLRGDVMQEDHHTSSERMVYNVLPQDYKVVSCEYWNTQVTALWVI...   \n 1          W9J5F1  IPFSQSAWAQQRIGPKGTKAHGVTQPAPMDIKNLCNLTDLTLILDF...   \n 2          T1J3U0  WFGCCHKQYVSHVIDRQDPQSPSDNPSLVSQLQFFMWGIQIQNGEI...   \n \n            description taxon  mass      id preferred_id  \n 0  u x e o k m a i o s  3899  None  S1Z9L5      uniprot  \n 1  i x k c r b p d d p  8873  None  W9J5F1      uniprot  \n 2  m a w r r u x c w o  1966  9364  T1J3U0      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         405878  RMTDGFEWQLDFHAFIWCNQAAWQLPLEVHISQGNGGWRMGLYGNM...   \n 1         154167  CGMNYDNGYFSVAYQSYDLWYHQQLKTRGVKPAEKDSDKDLGIDVI...   \n 2         234189  GQWQECIQGFTPQQMCVDCCAETKLANKSYYHSWMTWRLSGLCFNM...   \n \n            description taxon  mass      id preferred_id  \n 0  y c s v s n e c h o  9606  None  405878       entrez  \n 1  i k n c e n r n c d  9606  None  154167       entrez  \n 2  o v w y g h y e v y  9606  None  234189       entrez  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import RandomPropertyProteinIsoform\n</pre> from data_generator import RandomPropertyProteinIsoform In\u00a0[\u00a0]: Copied! <pre>print_yaml('05_schema_config.yaml')\n</pre> print_yaml('05_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\n\n--------------\n</pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>We now create three separate DataFrames, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p> In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [RandomPropertyProteinIsoform() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n\n# Create BioCypher driver\nbc = BioCypher(\n    biocypher_config_path='05_biocypher_config.yaml',\n    schema_config_path='05_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # create a list of proteins to be imported proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [RandomPropertyProteinIsoform() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ]  # Create BioCypher driver bc = BioCypher(     biocypher_config_path='05_biocypher_config.yaml',     schema_config_path='05_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins))  for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n  uniprot.protein                                           sequence  \\\n0          A9L6G4  SWIVVGQPDSHNKRLVNYHWMRCEHPLRCWRPIYVVRVSFQSQCEQ...   \n1          E4N2H2  PGVMILDNMQHKCSKELSTRQIITNHWICNSAPISWSSGMDRSCLD...   \n2          V4F1T1  DQCHNLCPGSSFQCPENAFGNDWIDHMPQETGLMQYDDPQSGMWFT...   \n\n           description taxon  mass      id preferred_id  \n0  m o k j a f w v w r  4220  None  A9L6G4      uniprot  \n1  n v i r s f m f d w  6339  6481  E4N2H2      uniprot  \n2  w e v v a b o b b u  9176  6510  V4F1T1      uniprot  \nprotein isoform\n  protein isoform                                           sequence  \\\n0          F0N9A4  QDVVLVEGCGDEGWIHMPEKRPGQAYKWCERFRPIPDFTNSIKIAY...   \n1          B1W6O2  SQKHFRRWWTNDCFGQELMSIYYNVKFWDNLIEMTGGPASRVCLGQ...   \n2          G6V5R9  ASAITPFSYEKPHTVTLDATEVFPKMQDAQAIEREIHFSKSTLVYG...   \n\n           description taxon  mass      id preferred_id  \n0  r f e a v a a g w r  8061  None  F0N9A4      uniprot  \n1  a c a v v k v k c w  6786  None  B1W6O2      uniprot  \n2  c k g d a l f r t v  6868  1323  G6V5R9      uniprot  \nentrez.protein\n  entrez.protein                                           sequence  \\\n0          52329  DYRSMAPTFILMKIYPACDAITKRRWSVATVKDGEFIWWSAVKIFP...   \n1         581107  LLVFNMGQLAVAGYGNTMVSAMMCFCCDVKARMGMSWLPKITTMQW...   \n2         270569  MVCSHHELAVAFQTMCPIQGDAATAKANAHRTTDKQNWMVVKWFRT...   \n\n           description taxon  mass      id preferred_id  \n0  q k r b h g t q x x  9606  None   52329       entrez  \n1  h f g z j r b g m w  9606  None  581107       entrez  \n2  s b p v f u t y g v  9606  None  270569       entrez  \n</pre> In\u00a0[\u00a0]: Copied! <pre>print_yaml('06_schema_config_pandas.yaml')\n</pre> print_yaml('06_schema_config_pandas.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\nprotein protein interaction:\n    is_a: pairwise molecular interaction\n    represented_as: edge\n    preferred_id: intact\n    input_label: interacts_with\n    properties:\n        method: str\n        source: str\n\n--------------\n</pre> <p>Now that we have added <code>protein protein interaction</code> as an edge, we have to simulate some interactions:</p> In\u00a0[\u00a0]: Copied! <pre>from data_generator import InteractionGenerator\n\n# Simulate edges for proteins we defined above\nppi = InteractionGenerator(\n    interactors=[p.get_id() for p in proteins],\n    interaction_probability=0.05,\n).generate_interactions()\n</pre> from data_generator import InteractionGenerator  # Simulate edges for proteins we defined above ppi = InteractionGenerator(     interactors=[p.get_id() for p in proteins],     interaction_probability=0.05, ).generate_interactions() In\u00a0[\u00a0]: Copied! <pre># naturally interactions/edges contain information about the interacting source and target nodes\n# let's look at the first one in the list\ninteraction = ppi[0]\nf\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\"\n</pre> # naturally interactions/edges contain information about the interacting source and target nodes # let's look at the first one in the list interaction = ppi[0] f\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\" Out[\u00a0]: <pre>'A9L6G4 interacts_with V4F1T1'</pre> In\u00a0[\u00a0]: Copied! <pre># similarly to nodes, it also has a dictionary of properties\ninteraction.get_properties()\n</pre> # similarly to nodes, it also has a dictionary of properties interaction.get_properties() Out[\u00a0]: <pre>{'source': 'signor', 'method': 'u z c x m d c u g s'}</pre> <p>As with nodes, we add first createa a new BioCypher instance, and then populate it with nodes as well as edges:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='06_biocypher_config.yaml',\n    schema_config_path='06_schema_config_pandas.yaml',\n)\n</pre> bc = BioCypher(     biocypher_config_path='06_biocypher_config.yaml',     schema_config_path='06_schema_config_pandas.yaml', ) In\u00a0[\u00a0]: Copied! <pre># Extract id, source, target, label, and property dictionary\ndef edge_generator(ppi):\n    for interaction in ppi:\n        yield (\n            interaction.get_id(),\n            interaction.get_source_id(),\n            interaction.get_target_id(),\n            interaction.get_label(),\n            interaction.get_properties(),\n        )\n\nbc.add(node_generator(proteins))\nbc.add(edge_generator(ppi))\n</pre> # Extract id, source, target, label, and property dictionary def edge_generator(ppi):     for interaction in ppi:         yield (             interaction.get_id(),             interaction.get_source_id(),             interaction.get_target_id(),             interaction.get_label(),             interaction.get_properties(),         )  bc.add(node_generator(proteins)) bc.add(edge_generator(ppi))  <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <p>Let's look at the interaction DataFrame:</p> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein protein interaction\"]\n</pre> bc.to_df()[\"protein protein interaction\"] Out[\u00a0]: protein protein interaction _from _to source method 0 intact703256 A9L6G4 V4F1T1 signor u z c x m d c u g s 1 None E4N2H2 F0N9A4 intact None <p>Finally, it is worth noting that BioCypher relies on ontologies, which are machine readable representations of domains of knowledge that we use to ground the contents of our knowledge graphs. While details about ontologies are out of scope for this tutorial, and are described in detail in the BioCypher documentation, we can still have a glimpse at the ontology that we used implicitly in this tutorial:</p> In\u00a0[\u00a0]: Copied! <pre>bc.show_ontology_structure()\n</pre> bc.show_ontology_structure() <pre>Showing ontology structure based on https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</pre> Out[\u00a0]: <pre>&lt;treelib.tree.Tree at 0x7f7327b3a880&gt;</pre>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#example-notebook-biocypher-and-pandas","title":"Example Notebook: BioCypher and Pandas\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#introduction","title":"Introduction\u00b6","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data, and thus save development time in the maintenance of curated knowledge graphs, while allowing simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two other versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find the former in the tutorial directory of the BioCypher repository. This tutorial simply takes the latter, in-memory approach to a Jupyter notebook.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#setup","title":"Setup\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#tutorial-files","title":"Tutorial files\u00b6","text":"<p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files, specifically the <code>.yaml</code> files, are named according to the tutorial step they are used in.</p> <p>Let's download these:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#configuration","title":"Configuration\u00b6","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#section-1-adding-data","title":"Section 1: Adding data\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#input-data-stream-adapter","title":"Input data stream (\"adapter\")\u00b6","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#schema-configuration","title":"Schema configuration\u00b6","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)\u00b6","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#section-2-merging-data","title":"Section 2: Merging data\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#plain-merge","title":"Plain merge\u00b6","text":"<p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#ad-hoc-subclassing","title":"Ad hoc subclassing\u00b6","text":""},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#section-3-handling-properties","title":"Section 3: Handling properties\u00b6","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#designated-properties","title":"Designated properties\u00b6","text":"<p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li><p><code>str</code> (or <code>string</code>),</p> </li> <li><p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li><p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li><p><code>bool</code> (or <code>boolean</code>),</p> </li> <li><p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#inheriting-properties","title":"Inheriting properties\u00b6","text":"<p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p>"},{"location":"BioCypher/learn/tutorials/pandas_tutorial_refactored/#section-4-handling-relationships","title":"Section 4: Handling relationships\u00b6","text":"<p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can modify the schema configuration above to the following:</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/","title":"Tutorial - Basics","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data to save development time in the maintenance of curated knowledge graphs and to allow the simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find both in the <code>tutorial</code> directory of the BioCypher repository; the Pandas version of each tutorial step is suffixed with <code>_pandas</code>.</p> <p>Neo4j</p> <p>While you can use the files generated to create an actual Neo4j database, it is not required for this tutorial. For checking the output, you can simply open the CSV files in a text editor or your IDE; by default, they will be written to the <code>biocypher-out</code> directory. If you simply want to run the tutorial to see how it works, you can also run the Pandas version.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#setup","title":"Setup","text":"<p>To run this tutorial, you will need to have cloned and installed the BioCypher repository on your machine. We recommend using Poetry:</p> <pre><code>git clone https://github.com/biocypher/biocypher.git\ncd biocypher\npoetry install\n</code></pre> <p>Poetry environment</p> <p>In order to run the tutorial code, you will need to activate the Poetry environment. This can be done by running <code>poetry shell</code> in the <code>biocypher</code> directory. Alternatively, you can run the code from within the Poetry environment by prepending <code>poetry run</code> to the command. For example, to run the tutorial code, you can run <code>poetry run python tutorial/01__basic_import.py</code>.</p> <p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files are named according to the tutorial step they are used in. The <code>biocypher-out</code> directory will be created automatically when you run the tutorial code.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#configuration","title":"Configuration","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#section-1-adding-data","title":"Section 1: Adding data","text":"<p>Note: Poetry environment</p> <p>The code for this tutorial can be found at <code>tutorial/01__basic_import.py</code>. The schema is at <code>tutorial/01_schema_config.yaml</code>, configuration in <code>tutorial/01_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#input-data-stream-adapter","title":"Input data stream (\"adapter\")","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p> <pre><code>from tutorial.data_generator import Protein\nproteins = [Protein() for _ in range(10)]\n</code></pre> <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> <pre><code>def node_generator():\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties()\n        )\n</code></pre> <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#schema-configuration","title":"Schema configuration","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p> <pre><code>protein: # mapping\n  represented_as: node # schema configuration\n  preferred_id: uniprot # uniqueness\n  input_label: uniprot_protein # connection to input stream\n</code></pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph. While this can be done \"online\", i.e., by connecting to a running DBMS instance, we will in this example use the offline mode of BioCypher, which does not require setting up a graph database instance. The following code will use the data stream and configuration set up above to write the files for knowledge graph creation:</p> <pre><code>import os\nos.chdir('../')\n</code></pre> <pre><code>from biocypher import BioCypher\n\nbc = BioCypher(\n    biocypher_config_path=\"tutorial/01_biocypher_config.yaml\",\n    schema_config_path=\"tutorial/01_schema_config.yaml\",\n)\n\nbc.write_nodes(node_generator())\n</code></pre> <p>We pass our configuration files at instantiation of the interface, and we pass the data stream to the <code>write_nodes</code> function. BioCypher will then create the graph and write it to the output directory, which is set to <code>biocypher-out/</code> by default, creating a subfolder with the current datetime for each driver instance.</p> <p>Note</p> <p>The <code>biocypher_config_path</code> parameter at instantiation of the <code>BioCypher</code> class should in most cases not be needed; we are using it here to increase convenience of the tutorial and to showcase its use. We are overriding the default value of only two settings: the offline mode (<code>offline</code> in <code>biocypher</code>) and the database name (<code>database_name</code> in <code>neo4j</code>).</p> <p>By default, BioCypher will look for a file named <code>biocypher_config.yaml</code> in the current working directory and in its subfolder <code>config</code>, as well as in various user directories. For more information, see the section on configuration.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#importing-data-into-neo4j","title":"Importing data into Neo4j","text":"<p>If you want to build an actual Neo4j graph from the tutorial output files, please follow the Neo4j import tutorial.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#quality-control-and-convenience-functions","title":"Quality control and convenience functions","text":"<p>BioCypher provides a number of convenience functions for quality control and data exploration. In addition to writing the import call for Neo4j, we can print a log of ontological classes that were present in the input data but are not accounted for in the schema configuration, as well as a log of duplicates in the input data (for the level of granularity that was used for the import). We can also print the ontological hierarchy derived from the underlying model(s) according to the classes that were given in the schema configuration:</p> <pre><code>bc.log_missing_input_labels()   # show input unaccounted for in the schema\nbc.log_duplicates()             # show duplicates in the input data\nbc.show_ontology_structure()    # show ontological hierarchy\n</code></pre>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#section-2-merging-data","title":"Section 2: Merging data","text":""},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#plain-merge","title":"Plain merge","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/02__merge.py</code>. Schema files are at <code>tutorial/02_schema_config.yaml</code>, configuration in <code>tutorial/02_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: uniprot\n  input_label: [uniprot_protein, entrez_protein]\n</code></pre> <p>This again creates a single output file, now for both protein types, including both input streams, and the graph can be created as before using the command line call created by BioCypher. However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#ad-hoc-subclassing","title":"Ad hoc subclassing","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/03__implicit_subclass.py</code>. Schema files are at <code>tutorial/03_schema_config.yaml</code>, configuration in <code>tutorial/03_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n</code></pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p> <p>Note</p> <p>The only change affected upon the code from the previous section is the referral to the updated schema configuration file.</p> <p>Hint</p> <p>In the output, we now generate two separate files for the <code>protein</code> class, one for each subclass (with names in PascalCase).</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#section-3-handling-properties","title":"Section 3: Handling properties","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Note</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#designated-properties","title":"Designated properties","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/04__properties.py</code>. Schema files are at <code>tutorial/04_schema_config.yaml</code>, configuration in <code>tutorial/04_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li> <p><code>str</code> (or <code>string</code>),</p> </li> <li> <p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li> <p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li> <p><code>bool</code> (or <code>boolean</code>),</p> </li> <li> <p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n  properties:\n    sequence: str\n    description: str\n    taxon: str\n    mass: dbl\n</code></pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p> <p>Note</p> <p>If we wanted to ignore the mass value for all properties, we could simply remove the <code>mass</code> key from the <code>properties</code> dictionary.</p> <p>Tip</p> <p>BioCypher provides feedback about property conflicts; try running the code for this example (<code>04__properties.py</code>) with the schema configuration of the previous section (<code>03_schema_config.yaml</code>) and see what happens.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#inheriting-properties","title":"Inheriting properties","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/05__property_inheritance.py</code>. Schema files are at <code>tutorial/05_schema_config.yaml</code>, configuration in <code>tutorial/05_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p> <pre><code>protein isoform:\n  is_a: protein\n  inherit_properties: true\n  represented_as: node\n  preferred_id: uniprot\n  input_label: uniprot_isoform\n</code></pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Note</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>Hint</p> <p>We now create three separate data files, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#section-4-handling-relationships","title":"Section 4: Handling relationships","text":"<p>Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/06__relationships.py</code>. Schema files are at <code>tutorial/06_schema_config.yaml</code>, configuration in <code>tutorial/06_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can add the following to the schema configuration above:</p> <pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  preferred_id: intact\n  input_label: interacts_with\n  properties:\n    method: str\n    source: str\n</code></pre> <p>Here, we use explicit subclassing to define the protein-protein interaction, which is not represented in the basic Biolink model, as a direct child of the Biolink \"pairwise molecular interaction\" class. We also reify this relationship by representing it as a node. This allows us to connect it to other nodes in the graph, for example to evidences for each interaction. If we do not want to reify the relationship, we can set <code>represented_as</code> to <code>edge</code> instead.</p>"},{"location":"BioCypher/learn/tutorials/tutorial001_basics/#relationship-identifiers","title":"Relationship identifiers","text":"<p>In biomedical data, relationships often do not have curated unique identifiers. Nevertheless, we may want to be able to refer to them in the graph. Thus, edges possess an ID field similar to nodes, which can be supplied in the input data as an optional first element in the edge tuple. Generating this ID from the properties of the edge (source and target identifiers, and additionally any properties that the edge possesses) can be done, for instance, by using the MD5 hash of the concatenation of these values. Edge IDs are active by default, but can be deactivated by setting the <code>use_id</code> field to <code>false</code> in the <code>schema_config.yaml</code> file.</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: edge\n  use_id: false\n  # ...\n</code></pre>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/","title":"Tutorial - Handling Ontologies","text":"<p>BioCypher relies on ontologies to ground the knowledge graph contents in biology. This has the advantages of providing machine readability and therefore automation capabilities as well as making working with BioCypher accessible to biologically oriented researchers. However, it also means that BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required. In the following we will cover the basics of ontologies and how to use them in BioCypher.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#what-is-an-ontology","title":"What is an ontology?","text":"<p>An ontology is a formal representation of a domain of knowledge. It is a hierarchical structure of concepts and relations. The concepts are organized into a hierarchy, where each concept is a subclass of a more general concept. For instance, a wardrobe is a subclass of a piece of furniture. Individual wardrobes, such as yours or mine, are instances of the concept wardrobe, and as such would be represented as Wardrobe nodes in a knowledge graph. In BioCypher, these nodes would additionally inherit the PieceOfFurniture label from the ontological hierarchy of things.</p> <p>Note</p> <p>Why is the class called piece of furniture but the label is PieceOfFurniture?</p> <p>The Biolink model uses two different case notations for its labels: the \"internal\" designation of classes is in lower sentence case (\"protein\", \"pairwise molecular interaction\"), while the \"external\" designation is in PascalCase (\"Protein\", \"PairwiseMolecularInteraction\"). BioCypher uses the same paradigm: in most cases (input, schema configuration, internally), the lower sentence case is used, while in the output (Neo4j labels, file system names) the PascalCase is more suitable; Neo4j labels and system file names don't deal well with spaces and special characters. Therefore, we check the output file names for their compliance with the Neo4j naming rules. All non compliant characters are removed from the file name (e.g. if the ontology class is called \"desk (piece of furniture)\", the brackets would be removed and the file name will be \"DeskPieceOfFurniture\"). We also remove the \"biolink:\" CURIE prefix for use in file names and Neo4j labels.</p> <p>The relations between concepts can also be organized into a hierarchy. In the specific case of a Neo4j graph, however, relationships cannot possess multiple labels; therefore, if concept inheritance is desired for relationships, they need to be \"reified\", i.e., turned into nodes. BioCypher provides a simple way of converting edges to nodes and vice versa (using the <code>represented_as</code> field). For a more in-depth explanation of ontologies, we recommend this introduction.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#how-biocypher-uses-ontologies","title":"How BioCypher uses ontologies","text":"<p>BioCypher is agnostic to the choice of ontology. Practically, we have built our initial projects around the Biolink model, because it provides a large but shallow collection of concepts that are relevant to the biomedical domain. Other examples of generalist ontologies are the Experimental Factor Ontology and the Basic Formal Ontology. To account for the specific requirements of expert systems, it is possible to use multiple ontologies in the same project. For instance, one might want to extend the rather basic classes relating to molecular interactions in Biolink (the most specific being <code>pairwise molecular interaction</code>) with more specific classes from a more domain-specific ontology, such as the EBI molecular interactions ontology (PSI-MI). A different project may need to define very specific genetics concepts, and thus extend the Biolink model at the terminal node <code>sequence variant</code> with the corresponding subtree of the Sequence Ontology. The OBO Foundry and the BioPortal collect many such specialised ontologies.</p> <p>The default format for ingesting ontology definitions into BioCypher is the Web Ontology Language (OWL); BioCypher can read <code>.owl</code>, <code>.rdf</code>, and <code>.ttl</code> files. The preferred way to specify the ontology or ontologies to be used in a project is to specify them in the biocypher configuration file (<code>biocypher_config.yaml</code>). This file is used to specify the location of the ontology files, as well as the root node of the main (\"head\") ontology and join nodes as fusion points for all \"tail\" ontologies. For more info, see the section on hybridising ontologies.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#visualising-ontologies","title":"Visualising ontologies","text":"<p>BioCypher provides a simple way of visualising the ontology hierarchy. This is useful for debugging and for getting a quick overview of the ontology and which parts are actually used in the knowledge graph to be created. Depending on your use case you can either visualise the parts of the ontology used in the knowledge graph (sufficient for most use cases) or the full ontology. If the used ontology is more complex and contains multiple inheritance please refer to the section on visualising complex ontologies.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#visualise-only-the-parts-of-the-ontology-used-in-the-knowledge-graph","title":"Visualise only the parts of the ontology used in the knowledge graph","text":"<p>To get an overview of the structure of our project, we can run the following command via the interface:</p> Visualising the ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure()\n</code></pre> <p>This will build the ontology scaffold and print a tree visualisation of its hierarchy to the console using the treelib library. You can see this in action in tutorial part 6 (<code>tutorial/06_relationships.py</code>). The output will look something like this:</p> <pre><code>Showing ontology structure, based on Biolink 3.0.3:\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u251c\u2500\u2500 mixin\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre> <p>Note</p> <p>BioCypher will only show the parts of the ontology that are actually used in the knowledge graph with the exception of intermediary nodes that are needed to build a complete tree. For instance, the <code>protein</code> class is linked to the root class <code>entity</code> via <code>polypeptide</code>, <code>biological entity</code>, and <code>named thing</code>, all of which are not part of the input data.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#visualise-the-full-ontology","title":"Visualise the full ontology","text":"<p>If you want to see the complete ontology tree, you can call <code>show_ontology_structure</code> with the parameter <code>full=True</code>.</p> Visualising the full ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure(full=True)\n</code></pre>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#visualise-complex-ontologies","title":"Visualise complex ontologies","text":"<p>Not all ontologies can be easily visualised as a tree, such as ontologies with multiple inheritance, where classes in the ontology can have multiple parent classes. This violates the definition of a tree, where each node can only have one parent node. Consequently, ontologies with multiple inheritance cannot be visualised as a tree.</p> <p>BioCypher can still handle these ontologies, and you can call <code>show_ontology_structure()</code> to get a visualisation of the ontology. However, each ontology class will only be added to the hierarchy tree once (a class with multiple parent classes is only placed under one parent in the hierarchy tree). Since this will occur the first time the class is seen, the ontology class might not be placed where you would expect it. This only applies to the visualisation; the underlying ontology is still correct and contains all ontology classes and their relationships.</p> <p>Note</p> <p>When calling <code>show_ontology_structure()</code>, BioCypher automatically checks if the ontology contains multiple inheritance and logs a warning message if so.</p> <p>If you need to get a visualisation of the ontology with multiple inheritance, you can call <code>show_ontology_structure()</code> with the parameter <code>to_disk=/some/path/where_to_store_the_file</code>. This creates a <code>GraphML</code> file and stores it at the specified location.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#using-ontologies-plain-biolink","title":"Using ontologies: plain Biolink","text":"<p>BioCypher maps any input data to the underlying ontology; in the basic case, the Biolink model. This mapping is defined in the schema configuration (<code>schema_config.yaml</code>, see also here). In the simplest case, the representation of a concept in the knowledge graph to be built and the Biolink model class representing this concept are synonymous. For instance, the concept protein is represented by the Biolink class protein. To introduce proteins into the knowledge graph, one would simply define a node constituent with the class label protein. This is the mechanism we implicitly used for proteins in the basic tutorial (part 1); to reiterate:</p> schema_config.yaml<pre><code>protein:\n  represented_as: node\n  # ...\n</code></pre>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#model-extensions","title":"Model extensions","text":"<p>There are multiple reasons why a user might want to modify the basic model of the ontology or ontologies used. A class that is relevant to the user's task might be missing (Explicit inheritance). A class might not be granular enough, and the user would like to split it into subclasses based on distinct inputs (Implicit inheritance). For some very common use cases, we recommend going one step further and, maybe after some testing using the above \"soft\" model extensions, proposing the introduction of a new class to the model itself. For instance, Biolink is an open source community project, and new classes can be requested by opening an issue or filing a pull request directly on the Biolink model GitHub repository. Similar mechanisms apply for OBO Foundry ontologies.</p> <p>BioCypher provides further methods for ontology manipulation. The name of a class of the model may be too unwieldy for the use inside the desired knowledge graph, and the user would like to introduce a synonym/alias (Synonyms). Finally, the user might want to extend the basic model with another, more specialised ontology (Hybridising ontologies).</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#explicit-inheritance","title":"Explicit inheritance","text":"<p>Explicit inheritance is the most straightforward way of extending the basic model. It is also the most common use case. For instance, the Biolink model does not contain a class for <code>protein isoform</code>, and neither does it contain a relationship class for <code>protein protein interaction</code>, both of which we have already used in the basic tutorial. Since protein isoforms are specific types of protein, it makes sense to extend the existing Biolink model class <code>protein</code> with the concept of protein isoforms. To do this, we simply add a new class <code>protein isoform</code> to the schema configuration, and specify that it is a subclass of <code>protein</code> using the (optional) <code>is_a</code> field:</p> schema_config.yaml<pre><code>protein isoform:\n  is_a: protein\n  represented_as: node\n  # ...\n</code></pre> <p>Explicit inheritance can also be used to introduce new relationship classes. However, if the output is a Neo4j graph, these relationships must be represented as nodes to provide full functionality, since edges do not allow multiple labels. This does not mean that explicit inheritance cannot be used in edges; it is even recommended to do so to situate all components of the knowledge graph in the ontological hierarchy. However, to have the ancestry represented in the resulting Neo4j graph DB, multiple labels are required. For instance, we have already used the <code>protein protein interaction</code> relationship in the basic tutorial (part 6), making it a child of the Biolink model class <code>pairwise molecular interaction</code>. To reiterate:</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  # ...\n</code></pre> <p>The <code>is_a</code> field can be used to specify multiple inheritance, i.e., multiple ancestor classes and their direct parent-child relationships can be created by specifying multiple classes (as a list) in the <code>is_a</code> field. For instance, if we wanted to further extend the protein-protein interaction with a more specific <code>enzymatic interaction</code> class, we could do so as follows:</p> schema_config.yaml<pre><code>enzymatic interaction:\n  is_a: [protein protein interaction, pairwise molecular interaction]\n  represented_as: node\n  # ...\n</code></pre> <p>Note</p> <p>To create this multiple inheritance chain, we do not require the creation of a <code>protein protein interaction</code> class as shown above; all intermediary classes are automatically created by BioCypher and inserted into the ontological hierarchy. To obtain a continuous ontology tree, the target class (i.e., the last in the list) must be a real Biolink model class.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#implicit-inheritance","title":"Implicit inheritance","text":"<p>The base model (in the standard case, Biolink) can also be extended without specifying an explicit <code>is_a</code> field. This \"implicit\" inheritance happens when a class has multiple input labels that each refer to a distinct preferred identifier. In other words, if both the <code>input_label</code> and the <code>preferred_id</code> fields of a schema configuration class are lists, BioCypher will automatically create a subclass for each of the preferred identifiers. This is demonstrated in part 3 of the basic tutorial.</p> <p>Caution</p> <p>If only the <code>input_label</code> field - but not the <code>preferred_id</code> field - is a list, BioCypher will merge the inputs instead. This is useful for cases where different input streams should be unified under the same class label. See part 2 of the basic tutorial for more information.</p> <p>To make this more concrete, let's consider the example of <code>pathway</code> annotations. There are multiple projects that provide pathway annotations, such as Reactome and Wikipathways, and, in contrast to proteins, pathways are not easily mapped one-to-one. For classes where mapping is difficult or even impossible, we can use implicit subclassing instead. The Biolink model contains a <code>pathway</code> class, which we can use as a parent class of the Reactome and Wikipathways classes; we simply need to provide the pathways as two separate inputs with their own labels (e.g., \"react\" and \"wiki\"), and specify a corresponding list of preferred identifiers in the <code>preferred_id</code> field:</p> schema_config.yaml<pre><code>pathway:\n  represented_as: node\n  preferred_id: [reactome, wikipathways]\n  input_label: [react, wiki]\n  # ...\n</code></pre> <p>This will prompt BioCypher to create two subclasses of <code>pathway</code>, one for each input, and to map the input data to these subclasses. In the resulting knowledge graph, the Reactome and Wikipathways pathways will be represented as distinct classes by prepending the preferred identifier to the class label: <code>Reactome.Pathway</code> and <code>Wikipathways.Pathway</code>. By virtue of BioCypher's multiple labelling paradigm, those nodes will also inherit the <code>Pathway</code> class label as well as all parent labels and mixins of <code>Pathway</code> (<code>BiologicalProcess</code>, etc.). This allows us to query the graph for all <code>Pathway</code> nodes as well as for specific datasets depending on the desired granularity.</p> <p>Note</p> <p>This also works for relationships, but in this case, not the preferred identifiers but the sources (defined in the <code>source</code> field) are used to create the subclasses.</p>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#synonyms","title":"Synonyms","text":"<p>Note: Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/07__synonyms.py</code>. Schema files are at <code>tutorial/07_schema_config.yaml</code>, configuration in <code>tutorial/07_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In some cases, an ontology may contain a biological concept, but the name of the concept does for some reason not agree with the users desired knowledge graph structure. For instance, the user may not want to represent protein complexes in the graph as <code>macromolecular complex</code> nodes due to ease of use and/or readability criteria and rather call these nodes <code>complex</code>. In such cases, the user can introduce a synonym for the ontology class. This is done by selecting another, more desirable name for the respective class(es) and specifying the <code>synonym_for</code> field in their schema configuration. In this case, as we would like to represent protein complexes as <code>complex</code> nodes, we can do so as follows:</p> schema_config.yaml<pre><code>complex:\n  synonym_for: macromolecular complex\n  represented_as: node\n  # ...\n</code></pre> <p>Importantly, BioCypher preserves these mappings to enable compatibility between different structural instantiations of the ontology (or combination of ontologies). All entities that are mapped to ontology classes in any way can be harmonised even between different types of concrete representations.</p> <p>Note</p> <p>It is essential that the desired class name is used as the main class key in the schema configuration, and the ontology class name is given in the <code>synonym_for</code> field. The name given in the <code>synonym_for</code> field must be an existing class name (in this example, a real Biolink class).</p> <p>We can visualise the structure of the ontology as we have before. Instead of using <code>bc.show_ontology_structure()</code> however, we can use the <code>bc.summary()</code> method to show the structure and simultaneously check for duplicates and missing labels. This is useful for debugging purposes, and we can see that the import was completed without encountering duplicates, and all labels in the input are accounted for in the schema configuration. We also observe in the tree that the <code>complex</code> class is now a synonym for the <code>macromolecular complex</code> class (their being synonyms indicated as an equals sign):</p> <pre><code>Showing ontology structure based on https://raw.githubusercontent.com/biolink/biolink-model/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u251c\u2500\u2500 complex = macromolecular complex\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre>"},{"location":"BioCypher/learn/tutorials/tutorial002_handling_ontologies/#hybridising-ontologies","title":"Hybridising ontologies","text":"<p>A broad, general ontology is a useful tool for knowledge representation, but often the task at hand requires more specific and granular concepts. In such cases, it is possible to hybridise the general ontology with a more specific one. For instance, there are many different types of sequence variants in biology, but Biolink only provides a generic \"sequence variant\" class (and it clearly exceeds the scope of Biolink to provide granular classes for all thinkable cases). However, there are many specialist ontologies, such as the Sequence Ontology (SO), which provides a more granular representation of sequence variants, and MONDO, which provides a more granular representation of diseases.</p> <p>To hybridise the Biolink model with the SO and MONDO, we can use the generic ontology adapter class of BioCypher by providing \"tail ontologies\" as dictionaries consisting of an OWL format ontology file and a set of nodes, one in the head ontology (which by default is Biolink), and one in the tail ontology. Each of the tail ontologies will then be joined to the head ontology to form the hybridised ontology at the specified nodes. It is up to the user to make sure that the concept at which the ontologies shall be joined makes sense as a point of contact between the ontologies; ideally, it is the exact same concept.</p> <p>Hint</p> <p>If the concept does not exist in the head ontology, but is a feasible child class of an existing concept, you can set the <code>merge_nodes</code> option to <code>False</code> to prevent the merging of head and tail join nodes, but instead adding the tail join node as a child of the head join node you have specified. For instance, in the example below, we merge <code>sequence variant</code> from Biolink and <code>sequence_variant</code> from Sequence Ontology into a single node, but we add the MONDO subtree of <code>human disease</code> as a child of <code>disease</code> in Biolink.</p> <p><code>merge_nodes</code> is set to <code>True</code> by default, so there is no need to specify it in the configuration file if you want to merge the nodes.</p> <p>The ontology adapter also accepts any arbitrary \"head ontology\" as a base ontology, but if none is provided, the Biolink model is used as the default head ontology. However, it is strongly recommended to explicitly specify your desired ontology version here. These options can be provided to the BioCypher interface as parameters, or as options in the BioCypher configuration file, which is the preferred method for transparency reasons:</p> Using biocypher_config.yaml<pre><code># ...\n\nbiocypher:  # biocypher settings\n\n  # Ontology configuration\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n\n  tail_ontologies:\n\n    so:\n      url: data/so.owl\n      head_join_node: sequence variant\n      tail_join_node: sequence_variant\n\n    mondo:\n      url: http://purl.obolibrary.org/obo/mondo.owl\n      head_join_node: disease\n      tail_join_node: human disease\n      merge_nodes: false\n\n# ...\n</code></pre> <p>Note</p> <p>The <code>url</code> parameter can be either a local path or a URL to a remote resource.</p> <p>If you need to pass the ontology configuration programmatically, you can do so as follows at BioCypher interface instantiation:</p> Programmatic usage<pre><code>bc = BioCypher(\n    # ...\n\n    head_ontology={\n      'url': 'https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl',\n      'root_node': 'entity',\n    },\n\n    tail_ontologies={\n        'so':\n            {\n                'url': 'test/ontologies/so.owl',\n                'head_join_node': 'sequence variant',\n                'tail_join_node': 'sequence_variant',\n            },\n        'mondo':\n            {\n                'url': 'test/ontologies/mondo.owl',\n                'head_join_node': 'disease',\n                'tail_join_node': 'human disease',\n                'merge_nodes': False,\n            }\n    },\n\n    # ...\n)\n</code></pre>"},{"location":"BioCypher/learn/tutorials/tutorial003_adapters/","title":"Tutorial - Adapters","text":"<p>Note</p> <p>For a list of existing and planned adapters, please see here. You can also get an overview of pipelines and the adapters they use in our meta graph.</p> <p></p> <p>Note</p> <p>To facilitate the creation of a BioCypher pipeline, we have created a template repository that can be used as a starting point for your own adapter. It contains a basic structure for an adapter, as well as a script that can be used as a blueprint for a build pipeline. The repository can be found here.</p> <p>A \"BioCypher adapter\" is a python program responsible for connecting to the BioCypher core and providing it with the data from its associated resource. In doing so, it should adhere to several design principles to ensure simple interoperability between the core and multiple adapters. In essence, an adapter should conform to an interface that is defined by the core to give information about the nodes and edges the adapter provides to enable automatic harmonisation of the contents. An adapter can be \"primary\", i.e., responsible for a single \"atomic\" resource (e.g. UniProt, Reactome, etc.), or \"secondary\", i.e., connecting to a resource that is itself a combination of multiple primary resources (e.g. OmniPath, Open Targets, etc.). Due to extensive prior harmonisation, the latter is often easier to implement and thus is a good starting point that can be subsequently extended to and replaced by primary adapters.</p> <p>Warning</p> <p>The adapter interface is still under development and may change rapidly.</p>"},{"location":"BioCypher/learn/tutorials/tutorial003_adapters/#adapter-philosophy","title":"Adapter philosophy","text":"<p>There are currently two 'flavours' of adapters. The first is simpler and used in workflows that are similar to harmonisation scripts, where the BioCypher interface is instantiated in the same script as the adapter(s). In the second, the BioCypher interface is contained in the adapter class, which makes for a more complex architecture, but allows for more involved workflows. In pseudo-code, the two approaches look like this:</p> Simple adapter<pre><code>from biocypher import BioCypher\nfrom adapter import Adapter\n\nbc = BioCypher()\nadapter = Adapter()\n\nbc.write_nodes(adapter.get_nodes())\n</code></pre> <p>Here, the script file is the central point of control, orchestrating the entire interaction between the BioCypher core and the adapter(s). Examples of this simpler format are the Open Targets KG and the CROssBAR v2.</p> <p>On the other hand, the more involved approach looks like this:</p> Adapter base class<pre><code>from biocypher import BioCypher\n\nclass Adapter:\n    __bc = BioCypher()\n\n    def __init__(self):\n        # setup\n\n    @classmethod\n    def get_biocypher(cls):\n        return Adapter.__bc\n\n    def get_nodes(self):\n        # ...\n        return nodes\n\n    def write_nodes(self):\n        Adapter.get_biocypher.write_nodes(self.get_nodes())\n</code></pre> <p>Here, the adapter class (and adapters inheriting from it) contains a singleton instance of the BioCypher interface. Thus, the adapter needs to provide BioCypher functionality to the outside via dedicated methods. This allows for more complex workflows, for instance, reducing clutter when executing multiple adapters in a single for-loop, or writing from a stream of data, e.g. in a Neo4j transaction (which happens inside the adapter).</p> Main script<pre><code>from adapters import AdapterChild1, AdapterChild2\n\nadapters = [AdapterChild1(), AdapterChild2()]\n\nfor adapter in adapters:\n    adapter.write_nodes()\n</code></pre> <p>Examples of this approach are the IGVF Knowledge Graph and the Clinical Knowledge Graph migration.</p> <p>Note</p> <p>While there are differences in implementation details, both approaches are largely functionally equivalent. At the current time, there is no clear preference for one over the other; both are used. As the ecosystem matures and more high-level functionality is added (e.g. the pipeline), advantages of one approach over the other may become more apparent.</p>"},{"location":"BioCypher/learn/tutorials/tutorial003_adapters/#adapter-functions","title":"Adapter functions","text":"<p>In general, a single adapter fulfils the following tasks:</p> <ol> <li> <p>Loading the data from the primary resource, for instance by using pypath download / caching functions (as in the UniProt example adapter), by using columnar distributed data formats such as Parquet (as in the Open Targets example adapter), by using a running database instance (as in the CKG example adapter), or by simply reading a file from disk (as in the Dependency Map example adapter). Generally, any method that allows the efficient transfer of the data from adapter to BioCypher core is acceptable.</p> </li> <li> <p>Passing the data to BioCypher as a stream or list to be written to the used DBMS (or application) via a Python driver (\"online\") or via batch import (e.g. from CSV files).  The latter has the advantage of high throughput and a low memory footprint, while the former allows for a more interactive workflow but is often much slower, thus making it better suited for small incremental updates.</p> </li> <li> <p>Providing or connecting to additional functionality that is useful for the creation of knowledge graphs, such as identifier translation (e.g. via pypath.mapping as in the UniProt example adapter), or identifier and prefix standardisation and validation (e.g. via Bioregistry as in the UniProt example adapter and others).</p> </li> </ol> <p>Note</p> <p>For developers: We follow a design philosophy of \"separation of concerns\" in BioCypher. This means that the core should not be concerned with the details of how data is loaded, but only with the data itself. This is why the core does not contain any code for loading data from a resource, but only for writing it to the database. The adapter is responsible for loading the data and passing it to the core, which allows for a more modular design and makes it easier to maintain, extend, and reuse the code.</p> <p>For introduction of new features, we recommend to first implement them in the adapter, and to move them to the core only if they have shown to be useful for multiple adapters.</p>"},{"location":"BioCypher/learn/tutorials/tutorial003_adapters/#1-loading-the-data","title":"1. Loading the Data","text":"<p>Depending on the data source, it is up to the developer of the adapter to find and define a suitable representation to be piped into BioCypher; for instance, in out <code>pypath</code> adapter, we load the entire <code>pypath</code> object into memory to be passed to BioCypher using a generator that evaluates each <code>pypath</code> object and transforms it to the tuple representation described below. This is made possible by the \"pre-harmonised\" form in which the data is represented within <code>pypath</code>. For more heterogeneous data representations, additional transformations may be necessary before piping into BioCypher.</p> <p>For larger datasets, it can be beneficial to adopt a streaming approach or batch processing, as demonstrated in the Open Targets adapter and the CKG adapter. BioCypher can handle input streams of arbitrary length via Python generators.</p>"},{"location":"BioCypher/learn/tutorials/tutorial003_adapters/#2-passing-the-data","title":"2. Passing the Data","text":"<p>We currently pass data into BioCypher as a collection of tuples. Nodes are represented as 3-tuples, containing: - the node ID (unique in the space of the knowledge graph, ideally a CURIE with   a prefix registered in the Bioregistry) - the node type, i.e., its label (this is the string that is mapped to an   ontological class via the <code>input_label</code> field in the schema configuration) - a dictionary of node attributes</p> <p>While edges are represented as 5-tuples, containing: - the (optional) relationship ID (unique in the space of the KG) - the source node ID (referring to a unique node ID in the KG) - the target node ID (referring to a unique node ID in the KG) - the relationship type, i.e., its label (this is the string that is mapped to   an ontological class via the <code>input_label</code> field in the schema configuration) - a dictionary of relationship attributes</p> <p>Note</p> <p>This representation will probably be subject to change soon and yield to a more standardised interface for nodes and edges, derived from a BioCypher core class. We refer to this development in an issue.</p>"},{"location":"BioCypher/learn/tutorials/tutorial003_adapters/#strict-mode","title":"Strict mode","text":"<p>We can activate BioCypher strict mode with the <code>strict_mode</code> option in the configuration. In strict mode, the driver will raise an error if it encounters a node or edge without data source, version, and licence. These currently need to be provided as part of the node and edge attribute dictionaries, with the reserved keywords <code>source</code>, <code>version</code>, and <code>licence</code> (or <code>license</code>). This may change to a more rigorous implementation in the future.</p>"},{"location":"BioCypher/reference/","title":"Reference Index","text":"<ul> <li> <p> API Reference</p> <p>The BioCypher API reference provides a comprehensive overview of the API, detailing its methods, available parameters, and their functionality. This guide is designed for users who have a foundational understanding of key BioCypher concepts and developers.</p> <p> To the reference guide</p> </li> </ul> <ul> <li> <p> BioCypher Configuration Reference</p> <p>The BioCypher configuration reference offers an in-depth guide to the settings, fields, and parameters that control BioCypher\u2019s behavior and execution. It serves as a resource for customizing BioCypher to fit specific use cases and workflows.</p> <p> To the configuration reference</p> </li> </ul> <ul> <li> <p> Output Configuration Reference</p> <p>The output configuration reference describes the options available for BioCypher's output formats.</p> <p> To the output configuration reference</p> </li> </ul> <ul> <li> <p> Schema Configuration Reference</p> <p>The schema configuration reference explains the parameters used in the YAML file that defines the structure of the data model. It provides guidance on mapping entities and relationships to ensure seamless data integration.</p> <p> To the schema configuration reference</p> </li> </ul>"},{"location":"BioCypher/reference/api-reference/","title":"Api reference","text":"<ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> <li> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> </li> <li> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/","title":"BioCypher Configuration Reference","text":"<p>BioCypher comes with a default set of configuration parameters. You can overwrite them by creating a <code>biocypher_config.yaml</code> file in the root directory or the <code>config</code> directory of your project. You only need to specify the ones you wish to override from default. If you want to create global user settings, you can create a <code>biocypher_config.yaml</code> in your default BioCypher user directory (as found using <code>appdirs.user_config_dir('biocypher')</code>). For instance, on Mac OS, this would be <code>~/Library/Caches/biocypher/biocypher_config.yaml</code>. Finally, you can also point an instance of the BioCypher class to any YAML file using the biocypher_config_path parameter.</p> <p>Note</p> <p>It is important to follow the rules of indentation in the YAML file. BioCypher module configuration is found under the top-level keyword biocypher, while the settings for DBMS systems (e.g., Neo4j) are found under their respective keywords (e.g., neo4j).</p> <p>Quote characters</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Configuration files are read in the order <code>default -&gt; user level -&gt; project level</code>, with the later ones overriding the preceding. The following parameters are available:</p>"},{"location":"BioCypher/reference/biocypher-config-guide/#purpose","title":"Purpose","text":"<p>The configuration in BioCypher customizes its behavior by overriding default settings through a <code>biocypher_config.yaml</code> file. It ensures flexibility for different use cases by allowing you to define data sources, database connections, and output formats.</p>"},{"location":"BioCypher/reference/biocypher-config-guide/#convention-for-naming","title":"Convention for naming","text":"<p>It is important to follow the rules of indentation in the YAML file. BioCypher module configuration is found under the top-level keyword <code>biocypher</code>, while the settings for DBMS systems (e.g., Neo4j) are found under their respective keywords (e.g., <code>neo4j</code>).</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Quote characters</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Configuration files are read in the order <code>default -&gt; user level -&gt; project level</code>, with the later ones overriding the preceding. The following parameters are available:</p>"},{"location":"BioCypher/reference/biocypher-config-guide/#yaml-file-skeleton","title":"YAML file Skeleton","text":"biocypher_config.yaml<pre><code>biocypher:\n  #---- REQUIRED PARAMETERS\n\n  dbms: neo4j\n  schema_config_path: config/schema_config.yaml\n  offline: true\n  strict_mode: false\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n    switch_label_and_id: true\n\n  #---- OPTIONAL PARAMETERS\n  log_to_disk: true\n\n  debug: true\n\n  log_directory: biocypher-log\n\n  output_directory: biocypher-out\n\n  cache_directory: .cache\n\n  #---- OPTIONAL TAIL ONTOLOGIES\n\n  # tail_ontologies:\n  #   so:\n  #     url: test/ontologies/so.owl\n  #     head_join_node: sequence variant\n  #     tail_join_node: sequence_variant\n  #     switch_label_and_id: true\n  #   mondo:\n  #     url: test/ontologies/mondo.owl\n  #     head_join_node: disease\n  #     tail_join_node: disease\n  #     switch_label_and_id: true\n\n#-------------------------------------------------------------------\n#-----------------       OUTPUT Configuration      -----------------\n#-------------------------------------------------------------------\n#---- NEO4J database management system\nneo4j:\n  database_name: neo4j\n  wipe: true\n\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n\n  delimiter: \";\"\n  array_delimiter: \"|\"\n  quote_character: \"'\"\n\n  multi_db: true\n\n  skip_duplicate_nodes: false\n  skip_bad_relationships: false\n\n  # import_call_bin_prefix: bin/\n  # import_call_file_prefix: path/to/files/\n\n#---- PostgreSQL database management system\npostgresql:\n  database_name: postgres\n\n  host: localhost # host\n  port: 5432 # port\n\n  user: postgres\n  password: postgres # password\n\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"psql\"\n  # import_call_file_prefix: '/path/to/files'\n\n#---- SQLite database management system\nsqlite:\n  ### SQLite configuration ###\n\n  # SQLite connection credentials\n  database_name: sqlite.db # DB name\n\n  # SQLite import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"sqlite3\"\n  # import_call_file_prefix: '/path/to/files'\n\n#---- RDF (Resource Description Framework) data model\nrdf:\n  ### RDF configuration ###\n  rdf_format: turtle\n\n#---- NetworkX graph data model\nnetworkx:\n  ### NetworkX configuration ###\n  some_config: some_value # placeholder for technical reasons TODO\n\n#---- CSV (Comma-Separated Values) text file format\ncsv:\n  ### CSV/Pandas configuration ###\n  delimiter: \",\"\n</code></pre>"},{"location":"BioCypher/reference/biocypher-config-guide/#fields-reference","title":"Fields reference:","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#biocypher-section-parameters","title":"Biocypher section parameters","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#required-parameters","title":"Required parameters","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#dbms","title":"<code>dbms</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#head_ontology","title":"<code>head_ontology</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#offline","title":"<code>offline</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#root_node","title":"<code>root_node</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#schema_config_path","title":"<code>schema_config_path</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#strict_mode","title":"<code>strict_mode</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#switch_label_and_id","title":"<code>switch_label_and_id</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#url","title":"<code>url</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#optional-parameters","title":"Optional parameters","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#cache_directory","title":"<code>cache_directory</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#debug","title":"<code>debug</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#log_directory","title":"<code>log_directory</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#log_to_disk","title":"<code>log_to_disk</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#output_directory","title":"<code>output_directory</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#switch_label_and_id_1","title":"<code>switch_label_and_id</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#tail_join_node","title":"<code>tail_join_node</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#tail_ontologies","title":"<code>tail_ontologies</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#url_1","title":"<code>url</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#output-configuration-parameters","title":"Output configuration parameters","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#neo4j-dbms","title":"NEO4j DBMS","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#array_delimiter","title":"<code>array_delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#database_name","title":"<code>database_name</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#delimiter","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#import_call_bin_prefix","title":"<code>import_call_bin_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#import_call_file_prefix","title":"<code>import_call_file_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#multi_db","title":"<code>multi_db</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#password","title":"<code>password</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#quote_character","title":"<code>quote_character</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#skip_duplicate_nodes","title":"<code>skip_duplicate_nodes</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#skip_bad_relationships","title":"<code>skip_bad_relationships</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#uri","title":"<code>uri</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#user","title":"<code>user</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#wipe","title":"<code>wipe</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#postgresql-dbms","title":"PostgreSQL DBMS","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#database_name_1","title":"<code>database_name</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#delimiter_1","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#host","title":"<code>host</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#import_call_bin_prefix_1","title":"<code>import_call_bin_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#import_call_file_prefix_1","title":"<code>import_call_file_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#password_1","title":"<code>password</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#port","title":"<code>port</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#quote_character_1","title":"<code>quote_character</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#user_1","title":"<code>user</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#sqlite-dbms","title":"SQLite DBMS","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#database_name_2","title":"<code>database_name</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#delimiter_2","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#import_call_bin_prefix_2","title":"<code>import_call_bin_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#import_call_file_prefix_2","title":"<code>import_call_file_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#quote_character_2","title":"<code>quote_character</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#rdf-data-model","title":"RDF data model","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#rdf_format","title":"<code>rdf_format</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#networkx-graph-data-model","title":"NetworkX graph data model","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#some_config","title":"<code>some_config</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#networkx-graph-data-model_1","title":"NetworkX graph data model","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#some_config_1","title":"<code>some_config</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/biocypher-config-guide/#csv-file-format","title":"CSV file format","text":""},{"location":"BioCypher/reference/biocypher-config-guide/#delimiter_3","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/","title":"Schema Configuration Reference","text":""},{"location":"BioCypher/reference/schema-config-guide/#purpose","title":"Purpose:","text":""},{"location":"BioCypher/reference/schema-config-guide/#convention-for-naming","title":"Convention for naming:","text":""},{"location":"BioCypher/reference/schema-config-guide/#skeleton","title":"Skeleton:","text":"<pre><code>#-------------------------------------------------------------------\n#---- Title: Schema Configuration file example\n#---- Authors: &lt;author 1&gt;, &lt;author 2&gt;\n#---- Description: Schema to load information related to proteins, and\n#                  and their interactions.\n#\n#-------------------------------------------------------------------\n\n#-------------------------------------------------------------------\n#-------------------------      NODES      -------------------------\n#-------------------------------------------------------------------\n#=========    PARENT NODES\nprotein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n  properties:\n    sequence: str\n    description: str\n    taxon: str\n    mass: int\n\n#=========    INHERITED NODES\nprotein isoform:\n  is_a: protein\n  inherit_properties: true\n  represented_as: node\n  preferred_id: uniprot\n  input_label: uniprot_isoform\n\n#-------------------------------------------------------------------\n#------------------      RELATIONSHIPS (EDGES)     -----------------\n#-------------------------------------------------------------------\n#=========    PARENT EDGES\nprotein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: edge\n  preferred_id: intact\n  input_label: interacts_with\n  properties:\n      method: str\n      source: str\n\n#=========    INHERITED EDGES\n\n#=========    EDGES AS NODES\n\n\n#--------------------------------------------------------------------\n#---- Dictionary of custom keywords: add additional keywords if you\n#     need it. Please document each new keyword as in the following\n#     template. DO NOT DELETE the hash symbol (#) in each line.\n\n# &lt;keyword's name&gt;\n#    Description:\n#    Possible values:\n#        - possible value 1 [*datatype*]\n#        - possible value 2 [*datatype*]\n#\n</code></pre>"},{"location":"BioCypher/reference/schema-config-guide/#fields-reference","title":"Fields reference:","text":""},{"location":"BioCypher/reference/schema-config-guide/#exclude_properties","title":"<code>exclude_properties</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#inherit_properties","title":"<code>inherit_properties</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#input_label","title":"<code>input_label</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#is_a","title":"<code>is_a</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#label_as_edge","title":"<code>label_as_edge</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#preferred_id","title":"<code>preferred_id</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#properties","title":"<code>properties</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#represented_as","title":"<code>represented_as</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#source","title":"<code>source</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#synonym_for","title":"<code>synonym_for</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#target","title":"<code>target</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"BioCypher/reference/schema-config-guide/#add-custom-fields","title":"Add custom fields","text":"<p>Tip</p> <p>Do not forget to document your custom keywords at the end of the schema config file, this is especially useful if you share your schema configuration file with others. They will understand what is the purpose of those new keywords.</p> <p>You can use other keywords for local functionalities without interfering with the default ones. For instance, a particular user has added the <code>db_collection_name</code> field for its own purposes.</p> Example: schema configuration with a custom keyword<pre><code>#...\nprotein:\n  represented_as: node\n  preferred_id: uniprot\n  input_label: protein\n  db_collection_name: proteins\n  properties:\n    name: str\n    score: float\n    taxon: int\n    genes: str[]\n#...\n</code></pre>"},{"location":"BioCypher/reference/outputs/","title":"Index","text":""},{"location":"BioCypher/reference/outputs/#introduction","title":"Introduction","text":"<p>BioCypher development was initially centred around a Neo4j graph database output due to the migration of OmniPath to a Neo4j backend. Importantly, we understand BioCypher as an abstraction of the build process of a biomedical knowledge graph, and thus are open towards any output format for the knowledge representation.</p> <p>The used output format is specified via the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> (see the Configuration for an example). Currently supported are:</p> <ul> <li><code>neo4j</code></li> <li><code>arangodb</code></li> <li><code>rdf</code></li> <li><code>postgres</code></li> <li><code>sqlite</code></li> <li><code>tabular</code></li> <li><code>csv</code></li> <li><code>pandas</code></li> <li><code>networkx</code></li> </ul> <p>Furthermore, you can specify whether to use the <code>offline</code> or <code>online</code> mode.</p> <ul> <li> <p>For the online mode set <code>offline: false</code>. The behavior of the online mode   depends on the specified <code>dbms</code>. If the specified <code>dbms</code> is an in-memory   database (e.g. <code>csv</code>, <code>networkx</code>), the in-memory Knowledge Graph can   directly be accessed from the BioCypher instance. If the specified <code>dbms</code> is   a database (e.g. <code>neo4j</code>), the online mode requires a running database   instance and BioCypher will connect to this instance and directly writes the   output to the database.</p> </li> <li> <p>For the offline mode set <code>offline: true</code>. BioCypher will <code>write</code> the   knowledge graph to files in a designated output folder (standard being   <code>biocypher-out/</code> and the current datetime). Furthermore you can generate a   bash script to insert the knowledge graph files into the specified <code>dbms</code> by   running <code>bc.write_import_call()</code>.</p> </li> </ul> <p>Warning</p> <p>The <code>online</code> mode is currently only supported for <code>neo4j</code>, <code>tabular</code>, <code>csv</code>, <code>pandas</code>, and <code>networkx</code>.</p> <p>Details about the usage of the <code>online</code> and <code>offline</code> mode and the different supported output formats are described on the following pages:</p>"},{"location":"BioCypher/reference/outputs/#available-output-formats","title":"Available Output formats","text":"Method Offline mode Online mode (in-memory) <code>Neo4j</code> <code>ArangoDB</code>  (pending) <code>RDF</code>  (pending) <code>PostgreSQL</code>  (pending) <code>SQLite</code>  (pending) <code>Tabular</code> <code>CSV</code> <code>Pandas</code> <code>NetworkX</code>"},{"location":"BioCypher/reference/outputs/arangodb-output/","title":"ArangoDB","text":"<p>The documentation related to ArangoDB and BioCypher is under construction</p>"},{"location":"BioCypher/reference/outputs/arangodb-output/#install-arangodb","title":"Install ArangoDB","text":""},{"location":"BioCypher/reference/outputs/arangodb-output/#arangodb-settings","title":"ArangoDB settings","text":""},{"location":"BioCypher/reference/outputs/arangodb-output/#offline-mode","title":"Offline mode","text":""},{"location":"BioCypher/reference/outputs/arangodb-output/#online-mode","title":"Online mode","text":""},{"location":"BioCypher/reference/outputs/neo4j-output/","title":"Neo4j","text":"<p>In the following section, we give an overview of interacting with Neo4j from the perspective of BioCypher, but we refer the reader to the Neo4j documentation for more details.</p>"},{"location":"BioCypher/reference/outputs/neo4j-output/#install-neo4j","title":"Install Neo4j","text":"<p>Neo4j provide a Neo4j Desktop application that can be used to create a local instance of Neo4j. The desktop application provides information about the DBMS folder and can open a terminal at the DBMS root location.</p> <p>Neo4j is also available as a command line interface (CLI) tool. To use the CLI with the BioCypher admin import call, directory structures and permissions need to be set up correctly. The Neo4j CLI tool can be downloaded from the Neo4j download center. Please follow the Neo4j documentation for correct setup and usage on your system.</p> <p>Be mindful that different versions of Neo4j may differ in features and thus are also documented differently.</p> <p>Note</p> <p>We use the APOC library for Neo4j, which is not included automatically, but needs to be installed as a plugin to the DMBS. For more information, please refer to the APOC documentation.</p>"},{"location":"BioCypher/reference/outputs/neo4j-output/#neo4j-settings","title":"Neo4j settings","text":"<p>To overwrite the standard settings of Neo4j, add a <code>neo4j</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>neo4j:  ### Neo4j configuration ###\n\n  # Database name\n  database_name: neo4j\n\n  # Wipe DB before import (offline mode: --force)\n  wipe: true\n\n  # Neo4j authentication\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n\n  # Neo4j admin import batch writer settings\n  delimiter: ';'\n  array_delimiter: '|'\n  quote_character: \"'\"\n\n  # How to write the labels in the export files.\n  labels_order: \"Ascending\" # Default: From more specific to more generic.\n  # Or:\n  # labels_order: \"Descending\" # From more generic to more specific.\n  # labels_order: \"Alphabetical\" # Alphabetically. Legacy option.\n  # labels_order: \"Leaves\" # Only the more specific label.\n\n\n  # MultiDB functionality\n  # Set to false for using community edition or older versions of Neo4j\n  multi_db: true\n\n  # Import options\n  skip_duplicate_nodes: false\n  skip_bad_relationships: false\n\n  # Import call prefixes to adjust the autogenerated shell script\n  import_call_bin_prefix: bin/  # path to \"neo4j-admin\"\n  import_call_file_prefix: path/to/files/\n</code></pre>"},{"location":"BioCypher/reference/outputs/neo4j-output/#offline-mode","title":"Offline mode","text":"<p>Particularly if the data are very extensive (or performance is of the utmost priority), BioCypher can be used to facilitate a speedy and safe import of the data using the <code>neo4j-admin import</code> console command. Admin Import is a particularly fast method of writing data to a newly created graph database (the database needs to be completely empty) that gains most of its performance advantage from turning off safety features regarding input data consistency. Therefore, a sound and consistent representation of the nodes and edges in the graph is of high importance in this process, which is why the BioCypher export functionality has been specifically designed to perform type and content checking for all data to be written to the graph.</p> <p>Data input from the source database is exactly as in the case of interacting with a running database, with the data representation being converted to a series of CSV files in a designated output folder (standard being <code>biocypher-out/</code> and the current datetime).  BioCypher creates separate header and data files for all node and edge types to be represented in the graph. Additionally, it creates a file called <code>neo4j-admin-import-call.sh</code> containing the console command for creating a new database, which only has to be executed from the directory of the currently running Neo4j database.</p> <p>The name of the database to be created is given by the <code>db_name</code> setting, and can be arbitrary. In case the <code>db_name</code> is not the default Neo4j database name, <code>neo4j</code>, the database needs to be created in Neo4j before or after using the <code>neo4j-admin import</code> statement. This can be done by executing, in the running database (<code>&lt;db_name&gt;</code> being the name assigned in the method):</p> <ol> <li><code>:use system</code></li> <li><code>create database &lt;db_name&gt;</code></li> <li><code>:use &lt;db_name&gt;</code></li> </ol> <p>After writing knowledge graph files with BioCypher in offline mode for the Neo4j DBMS (database management system), the graph can now be imported into Neo4j using the <code>neo4j-admin</code> command line tool. This is not necessary if the graph is created in online mode. For convenience, BioCypher provides the command line call required to import the data into Neo4j:</p> <pre><code>bc.write_import_call()\n</code></pre> <p>This creates an executable shell script in the output directory that can be executed from the location of the database folder (or copied into the Neo4j terminal) to import the graph into Neo4j. Since BioCypher creates separate header and data files for each entity type, the import call conveniently aggregates this information into one command, detailing the location of all files on disk, so no data need to be copied around.</p> <p>Note</p> <p>The generated import call differs between Neo4j version 4 and 5. Starting from major version 5, Neo4j <code>import</code> command needs the <code>database</code> scope. BioCypher takes care of this. The generated import script <code>neo4j-admin-import-call.sh</code> first checks the Neo4j database version and uses the correct import statement for the detected version. Therefore make sure to run the script from the targeted DBMS root location.</p> <p>Neo4j can manage multiple projects, each with multiple DBMS (database management system) instances, each of which can house multiple databases. The screenshot below shows a project managed by Neo4j Desktop (project name \"BioCypher\") containing a DBMS instance (called \"Test DBMS\") with multiple named databases in it: the non-removable \"system\" DB, the default \"neo4j\" DB, and several user-created databases.</p> <p></p> <p>Crucially, the import call generated by BioCypher needs to be executed by the <code>neo4j-admin</code> binary in the DBMS folder (in the <code>bin/</code> directory of the root of the DBMS folder). The DBMS folder is the folder that contains the <code>data/</code> directory, which in turn contains the <code>databases/</code> folder, which is where the graph data is stored in individual folders corresponding to the DB names in the DBMS. On Neo4j Desktop, you can open a terminal at the DBMS root location by clicking on the three dots next to the DBMS name and selecting \"Terminal\" (see screenshot below).</p> <p></p> <p>Since the import call should be executed in the root of the DMBS folder, BioCypher generates the import call starting with <code>bin/neo4j-admin</code>, which can be copied into the terminal opened at the DBMS root location. For other operating systems and Neo4j installations (e.g. in Docker containers), please refer to the Neo4j documentation to find the correct location of your DMBS. We are working on extensions of the BioCypher process to improve interoperability with Neo4j as well as other storage systems.</p>"},{"location":"BioCypher/reference/outputs/neo4j-output/#online-mode","title":"Online mode","text":"<p>BioCypher provides a Python driver for interacting with Neo4j, which is accessed through the <code>BioCypher</code> class when setting <code>offline</code> to <code>False</code>. More details can be found in the API docs.</p> <p>If there exists no BioCypher graph in the currently active database, or if the user explicitly specifies so using the <code>wipe</code> attribute of the driver, a new BioCypher database is created using the schema configuration specified in the schema-config.yaml.</p>"},{"location":"BioCypher/reference/outputs/neo4j-output/#note-on-labels-order","title":"Note on labels order","text":"<p>Neo4j do not support managing the hierarchy of types of the vocabulary given by the input ontology. What it does is to attach to nodes and edges each type label of all the ancestors in the types hierarchy.</p> <p>By default, the Neo4j driver exports those type labels as an alphabetically-sorted list, which may be useful for comparing output files. But this may be less easy to understand within a graph browser that would show the labels in the same order.</p> <p>To get a more readable labels list, you can set <code>labels_order</code> as either <code>Ascending</code> or <code>Descending</code>; both of which will display labels in the order given by the type hierarchy of the ontology's vocabulary.</p> <p>To get even simpler label, you can set <code>labels_order: Leaves</code>, which will write down only the more specific type label (the \"leaf\" of the types tree). Be warn that the resulting export will completely lose the ontological information, hence making it impossible to query the graph on high-level types.</p>"},{"location":"BioCypher/reference/outputs/networkx-output/","title":"NetworkX","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>networkx</code>, the BioCypher Knowledge Graph is transformed into a NetworkX DiGraph.</p>"},{"location":"BioCypher/reference/outputs/networkx-output/#networkx-settings","title":"NetworkX settings","text":"<p>To overwrite the standard settings of NetworkX, add a <code>networkx</code> section to the <code>biocypher_config.yaml</code> file.  At the moment there are no configuration options supported/implemented.  Feel free to reach out and create issues or pull requests if you need specific configuration options.</p> biocypher_config.yaml<pre><code>networkx:\n  ### NetworkX configuration ###\n</code></pre>"},{"location":"BioCypher/reference/outputs/networkx-output/#offline-mode","title":"Offline mode","text":""},{"location":"BioCypher/reference/outputs/networkx-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>networkx</code>, the output folder contains:</p> <ul> <li> <p><code>networkx_graph.pkl</code>: The pickle file containing with the BioCypher Knowledge Graph as NetworkX DiGraph.</p> </li> <li> <p><code>import_networkx.py</code>: A Python script to load the created pickle file.</p> </li> </ul> <p>Note</p> <p>If any of the files is missing make sure to run <code>bc.write_import_call()</code>.</p>"},{"location":"BioCypher/reference/outputs/networkx-output/#online-mode","title":"Online mode","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>false</code> and the <code>dbms</code> set to <code>networkx</code>, you can get the in-memory networkx representation of the Knowledge Graph directly from BioCypher by calling the <code>get_kg()</code> function.</p> <pre><code># Initialize BioCypher\nbc = BioCypher(\n    biocypher_config_path=\"biocypher_config.yaml\",\n    schema_config_path=\"schema_config.yaml\",\n)\n\n# Add nodes and edges\nbc.add_nodes(nodes)\nbc.add_edges(edges)\n\n# Get the in-memory representation of the Knowledge Graph\nin_memory_kg = bc.get_kg()\n</code></pre>"},{"location":"BioCypher/reference/outputs/postgresql-output/","title":"PostgreSQL","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>postgres</code>, the BioCypher Knowledge Graph is written to a PostgreSQL database. PostgreSQL is a relational database management system.</p>"},{"location":"BioCypher/reference/outputs/postgresql-output/#install-postgresql","title":"Install PostgreSQL","text":"<p>To get a PostgreSQL instance running quickly, you can use Docker. The following command will start a PostgreSQL instance with the password <code>postgres</code> and the port <code>5432</code> exposed to the host system.</p> <pre><code>docker run --restart always \\\n           --publish=5432:5432 \\\n           --env POSTGRES_PASSWORD=postgres \\\n           -d postgres\n</code></pre> <p>The <code>postgresql-client</code> (also known as <code>psql</code> command line tool) can be used to connect and interact with the running PostgreSQL database instance. Installation instructions can be found here.</p>"},{"location":"BioCypher/reference/outputs/postgresql-output/#postgresql-settings","title":"PostgreSQL settings","text":"<p>To overwrite the standard settings of PostgreSQL, add a <code>postgresql</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>postgresql:  ### PostgreSQL configuration ###\n\n  # PostgreSQL connection credentials\n  database_name: postgres\n  user: postgres\n  password: postgres\n  port: 5432\n\n  # PostgreSQL import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n\n  # Import call prefixes to adjust the autogenerated shell script\n  import_call_bin_prefix: ''  # path to \"psql\"\n  import_call_file_prefix: /path/to/files/\n</code></pre>"},{"location":"BioCypher/reference/outputs/postgresql-output/#offline-mode","title":"Offline mode","text":""},{"location":"BioCypher/reference/outputs/postgresql-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>postgres</code>, the output folder contains:</p> <ul> <li> <p><code>entity-create_table.sql</code>: The SQL scripts to create the tables for the nodes/edges. Entity is replaced by your nodes and edges and for each node and edge type an own SQL script is generated.</p> </li> <li> <p><code>entity-part000.csv</code>: The CSV file containing the data for the entity.</p> </li> <li> <p><code>postgres-import-call.sh</code>: The import script to create a database with the SQL scripts and insert the data from the CSV files.</p> </li> </ul> <p>Note</p> <p>If the <code>postgres-import-call.sh</code> is missing, you can create it by running <code>bc.write_import_call()</code>.</p>"},{"location":"BioCypher/reference/outputs/postgresql-output/#create-the-postgresql-database","title":"Create the PostgreSQL database","text":"<p>A running PostgreSQL instance (e.g. in a Docker container created with the command from above) is required to create the database and import the data. By running the import script <code>postgres-import-call.sh</code>, the content should be written to the PostgreSQL database.</p>"},{"location":"BioCypher/reference/outputs/postgresql-output/#access-the-postgresql-database","title":"Access the PostgreSQL database","text":"<p>To check the content of the database, you can use the <code>psql</code> command line tool.</p> <p>First connect to the running PostgreSQL database instance: <pre><code>psql -h localhost -p 5432 -U postgres\n</code></pre></p> <p>Then you can execute SQL queries. For example, you can list all tables in the database by running the following command in the terminal: <pre><code>SELECT table_name\nFROM information_schema.tables\nWHERE table_schema = 'public';\n</code></pre></p>"},{"location":"BioCypher/reference/outputs/rdf-output/","title":"Resource Description Framework (RDF)","text":"<p>In this section, we will learn how to use and implement the output to RDF using the <code>_RDFWriter</code> module.</p>"},{"location":"BioCypher/reference/outputs/rdf-output/#rdf-settings","title":"RDF settings","text":"<p>To write your output to RDF, you have to specify some RDF settings in the <code>biocypher_config.yaml</code>. Using <code>rdf_format</code>, you can choose to export to <code>XML</code>, <code>Turtle</code> or any other format <code>rdflib</code> supports. The second configuration is the <code>rdf_namespaces</code>, where you can specify which namespaces exist in your data. If, for instance, your data contain SO (Sequence ontology) terms such as <code>SO:0000001</code>, IDs will be converted into valid URIs to allow referencing. Thus, <code>SO:0000001</code> will be converted into <code>http://purl.obolibrary.org/obo/SO_0000001</code>. When a node cannot be converted, a default URI will be used (<code>https://biocypher.org/biocypher#&lt;node_id&gt;</code>). Running the pipeline, the <code>_RDFWriter</code> will create a file for every node and relationship type you have specified.</p> biocypher_config.yaml<pre><code>biocypher:\n  strict_mode: true\n  schema_config_path: config/schema_config.yaml\n  dbms: rdf\n\n### RDF configuration ###\nrdf:\n  rdf_format: turtle\n  # options: xml, n3, turtle, nt, pretty-xml, trix, trig, nquads, json-ld\n  rdf_namespaces:\n    so: http://purl.obolibrary.org/obo/SO_\n    efo: http://www.ebi.ac.uk/efo/EFO_\n</code></pre>"},{"location":"BioCypher/reference/outputs/sqlite-output/","title":"SQLite","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>sqlite</code>, the BioCypher Knowledge Graph is written to a SQLite database. SQLite is a lightweight relational database management system. It is suitable for fast prototyping and development. For more mature applications have a look at PostgreSQL.</p>"},{"location":"BioCypher/reference/outputs/sqlite-output/#sqlite-settings","title":"SQLite settings","text":"<p>To overwrite the standard settings of SQLite, add a <code>sqlite</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>sqlite:\n  ### SQLite configuration ###\n\n  database_name: sqlite.db # DB name\n\n  # SQLite import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"sqlite3\"\n  # import_call_file_prefix: '/path/to/files'\n</code></pre>"},{"location":"BioCypher/reference/outputs/sqlite-output/#offline-mode","title":"Offline mode","text":""},{"location":"BioCypher/reference/outputs/sqlite-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>sqlite</code>, the output folder contains:</p> <ul> <li> <p><code>entity-create_table.sql</code>: The SQL scripts to create the tables for the nodes/edges. Entity is replaced by your nodes and edges and for each node and edge type an own SQL script is generated.</p> </li> <li> <p><code>entity-part000.csv</code>: The CSV file containing the data for the entity.</p> </li> <li> <p><code>sqlite-import-call.sh</code>: The import script to create a database with the SQL scripts and insert the data from the CSV files.</p> </li> </ul> <p>Note</p> <p>If the <code>sqlite-import-call.sh</code> is missing, you can create it by running <code>bc.write_import_call()</code>.</p>"},{"location":"BioCypher/reference/outputs/sqlite-output/#create-the-sqlite-database","title":"Create the SQLite database","text":"<p>To create the SQLite database, run the import script <code>sqlite-import-call.sh</code>. In the default case (without any changes to the <code>database_name</code>in the configuration), the file containing the database is created with the name <code>sqlite.db</code>.</p> <p>Note</p> <p>The import script expects, that the sqlite3 command line tool is installed on your system.</p>"},{"location":"BioCypher/reference/outputs/sqlite-output/#access-the-sqlite-database","title":"Access the SQLite database","text":"<p>Now you can access the created SQLite database. This can be done with the sqlite3 command line tool. For example, you can list all tables in the database by running the following command in the terminal:</p> <pre><code>sqlite3 sqlite.db \"SELECT name FROM sqlite_master WHERE type='table';\"\n</code></pre>"},{"location":"BioCypher/reference/outputs/tabular-output/","title":"Tabular output","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, the BioCypher Knowledge Graph is created in one of several possible tabular formats.</p>"},{"location":"BioCypher/reference/outputs/tabular-output/#tabular-output-settings","title":"Tabular output settings","text":"<p>To overwrite the standard settings of the CSV writer, add a <code>csv</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>csv:\n  ### CSV/Pandas configuration ###\n  delimiter: ','  # The delimiter to be used in the CSV files. Default is ','.\n</code></pre>"},{"location":"BioCypher/reference/outputs/tabular-output/#offline-mode","title":"Offline mode","text":""},{"location":"BioCypher/reference/outputs/tabular-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, the output folder contains:</p> <ul> <li> <p><code>*.csv</code>: The CSV files containing the node/edge data.</p> </li> <li> <p><code>import_pandas_csv.csv</code>: A Python script to load the created CSV files into Pandas DataFrames.</p> </li> </ul>"},{"location":"BioCypher/reference/outputs/tabular-output/#online-mode","title":"Online mode","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>false</code> and the <code>dbms</code> set to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, you can get the in-memory representation of the Knowledge Graph directly from BioCypher by calling the <code>get_kg()</code> function. This returns a dictionary with the corresponding data type (e.g., <code>Pandas</code> dataframes) for every node and edge type.</p> <pre><code># Initialize BioCypher\nbc = BioCypher(\n    biocypher_config_path=\"biocypher_config.yaml\",\n    schema_config_path=\"schema_config.yaml\",\n)\n\n# Add nodes and edges\nbc.add_nodes(nodes)\nbc.add_edges(edges)\n\n# Get the in-memory representation of the Knowledge Graph\nin_memory_kg = bc.get_kg()\n</code></pre>"},{"location":"BioCypher/reference/source/","title":"BioCypher","text":""},{"location":"BioCypher/reference/source/#modules","title":"Modules","text":""},{"location":"BioCypher/reference/source/#_corepy","title":"_core.py","text":"<p>BioCypher core module.</p> <p>Interfaces with the user and distributes tasks to submodules.</p>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher","title":"<code>BioCypher</code>","text":"<p>Orchestration of BioCypher operations.</p> <p>Instantiate this class to interact with BioCypher.</p> <pre><code>dbms (str): The database management system to use. For supported\n    systems see SUPPORTED_DBMS.\n\noffline (bool): Whether to run in offline mode. In offline mode\n    the Knowledge Graph is written to files. In online mode, it\n    is written to a database or hold in memory.\n\nstrict_mode (bool): Whether to run in strict mode. If True, the\n    translator will raise an error if a node or edge does not\n    provide source, version, and licence information.\n\nbiocypher_config_path (str): Path to the BioCypher config file.\n\nschema_config_path (str): Path to the user schema config\n    file.\n\nhead_ontology (dict): The head ontology defined by URL ('url') and root\n    node ('root_node').\n\ntail_ontologies (dict): The tail ontologies defined by URL and\n    join nodes for both head and tail ontology.\n\noutput_directory (str): Path to the output directory. If not\n    provided, the default value 'biocypher-out' will be used.\n\ncache_directory (str): Path to the cache directory.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>class BioCypher:\n    \"\"\"Orchestration of BioCypher operations.\n\n    Instantiate this class to interact with BioCypher.\n\n    Args:\n    ----\n        dbms (str): The database management system to use. For supported\n            systems see SUPPORTED_DBMS.\n\n        offline (bool): Whether to run in offline mode. In offline mode\n            the Knowledge Graph is written to files. In online mode, it\n            is written to a database or hold in memory.\n\n        strict_mode (bool): Whether to run in strict mode. If True, the\n            translator will raise an error if a node or edge does not\n            provide source, version, and licence information.\n\n        biocypher_config_path (str): Path to the BioCypher config file.\n\n        schema_config_path (str): Path to the user schema config\n            file.\n\n        head_ontology (dict): The head ontology defined by URL ('url') and root\n            node ('root_node').\n\n        tail_ontologies (dict): The tail ontologies defined by URL and\n            join nodes for both head and tail ontology.\n\n        output_directory (str): Path to the output directory. If not\n            provided, the default value 'biocypher-out' will be used.\n\n        cache_directory (str): Path to the cache directory.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dbms: str = None,\n        offline: bool = None,\n        strict_mode: bool = None,\n        biocypher_config_path: str = None,\n        schema_config_path: str = None,\n        head_ontology: dict = None,\n        tail_ontologies: dict = None,\n        output_directory: str = None,\n        cache_directory: str = None,\n        # legacy params\n        db_name: str = None,\n    ):\n        # Update configuration if custom path is provided\n        if biocypher_config_path:\n            _file_update(biocypher_config_path)\n\n        if db_name:\n            logger.warning(\n                \"The parameter `db_name` is deprecated. Please set the \"\n                \"`database_name` setting in the `biocypher_config.yaml` file \"\n                \"instead.\",\n            )\n            _config(**{db_name: {\"database_name\": db_name}})\n\n        # Load configuration\n        self.base_config = _config(\"biocypher\")\n\n        # Check for required configuration\n        for key in REQUIRED_CONFIG:\n            if key not in self.base_config:\n                msg = f\"Configuration key {key} is required.\"\n                raise ValueError(msg)\n\n        # Set configuration - mandatory\n        self._dbms = dbms or self.base_config[\"dbms\"]\n\n        if offline is None:\n            self._offline = self.base_config[\"offline\"]\n        else:\n            self._offline = offline\n\n        if strict_mode is None:\n            self._strict_mode = self.base_config[\"strict_mode\"]\n        else:\n            self._strict_mode = strict_mode\n\n        self._schema_config_path = schema_config_path or self.base_config.get(\n            \"schema_config_path\",\n        )\n\n        if not self._schema_config_path:\n            logger.warning(\"Running BioCypher without schema configuration.\")\n        else:\n            logger.info(\n                f\"Running BioCypher with schema configuration from {self._schema_config_path}.\",\n            )\n\n        self._head_ontology = head_ontology or self.base_config[\"head_ontology\"]\n\n        # Set configuration - optional\n        self._output_directory = output_directory or self.base_config.get(\n            \"output_directory\",\n        )\n        self._cache_directory = cache_directory or self.base_config.get(\n            \"cache_directory\",\n        )\n        self._tail_ontologies = tail_ontologies or self.base_config.get(\n            \"tail_ontologies\",\n        )\n\n        if self._dbms not in SUPPORTED_DBMS:\n            msg = f\"DBMS {self._dbms} not supported. Please select from {SUPPORTED_DBMS}.\"\n            raise ValueError(msg)\n\n        # Initialize\n        self._ontology_mapping = None\n        self._deduplicator = None\n        self._translator = None\n        self._downloader = None\n        self._ontology = None\n        self._writer = None\n        self._driver = None\n        self._in_memory_kg = None\n\n        self._in_memory_kg = None\n        self._nodes = None\n        self._edges = None\n\n    def _initialize_in_memory_kg(self) -&gt; None:\n        \"\"\"Create in-memory KG instance.\n\n        Set as instance variable `self._in_memory_kg`.\n        \"\"\"\n        if not self._in_memory_kg:\n            self._in_memory_kg = get_in_memory_kg(\n                dbms=self._dbms,\n                deduplicator=self._get_deduplicator(),\n            )\n\n    def add_nodes(self, nodes) -&gt; None:\n        \"\"\"Add new nodes to the internal representation.\n\n        Initially, receive nodes data from adaptor and create internal\n        representation for nodes.\n\n        Args:\n        ----\n            nodes(iterable): An iterable of nodes\n\n        \"\"\"\n        if isinstance(nodes, list):\n            self._nodes = list(itertools.chain(self._nodes, nodes))\n        else:\n            self._nodes = itertools.chain(self._nodes, nodes)\n\n    def add_edges(self, edges) -&gt; None:\n        \"\"\"Add new edges to the internal representation.\n\n        Initially, receive edges data from adaptor and create internal\n        representation for edges.\n\n        Args:\n        ----\n             edges(iterable): An iterable of edges.\n\n        \"\"\"\n        if isinstance(edges, list):\n            self._edges = list(itertools.chain(self._edges, edges))\n        else:\n            self._edges = itertools.chain(self._edges, edges)\n\n    def to_df(self):\n        \"\"\"Create DataFrame using internal representation.\n\n        TODO: to_df implies data frame, should be specifically that use case\n        \"\"\"\n        return self._to_KG()\n\n    def to_networkx(self):\n        \"\"\"Create networkx using internal representation.\"\"\"\n        return self._to_KG()\n\n    def _to_KG(self):\n        \"\"\"Convert the internal representation to knowledge graph.\n\n        The knowledge graph is returned based on the `dbms` parameter in\n        the biocypher configuration file.\n\n        Returns\n        -------\n             Any: knowledge graph.\n\n        \"\"\"\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        if not self._translator:\n            self._get_translator()\n        tnodes = self._translator.translate_entities(self._nodes)\n        tedges = self._translator.translate_entities(self._edges)\n        self._in_memory_kg.add_nodes(tnodes)\n        self._in_memory_kg.add_edges(tedges)\n        return self._in_memory_kg.get_kg()\n\n    def _get_deduplicator(self) -&gt; Deduplicator:\n        \"\"\"Create deduplicator if not exists and return.\"\"\"\n        if not self._deduplicator:\n            self._deduplicator = Deduplicator()\n\n        return self._deduplicator\n\n    def _get_ontology_mapping(self) -&gt; OntologyMapping:\n        \"\"\"Create ontology mapping if not exists and return.\"\"\"\n        if not self._schema_config_path:\n            self._ontology_mapping = OntologyMapping()\n\n        if not self._ontology_mapping:\n            self._ontology_mapping = OntologyMapping(\n                config_file=self._schema_config_path,\n            )\n\n        return self._ontology_mapping\n\n    def _get_ontology(self) -&gt; Ontology:\n        \"\"\"Create ontology if not exists and return.\"\"\"\n        if not self._ontology:\n            self._ontology = Ontology(\n                ontology_mapping=self._get_ontology_mapping(),\n                head_ontology=self._head_ontology,\n                tail_ontologies=self._tail_ontologies,\n            )\n\n        return self._ontology\n\n    def _get_translator(self) -&gt; Translator:\n        \"\"\"Create translator if not exists and return.\"\"\"\n        if not self._translator:\n            self._translator = Translator(\n                ontology=self._get_ontology(),\n                strict_mode=self._strict_mode,\n            )\n\n        return self._translator\n\n    def _get_writer(self):\n        \"\"\"Create writer if not online.\n\n        Set as instance variable `self._writer`.\n        \"\"\"\n        if self._offline:\n\n            def timestamp() -&gt; str:\n                return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n            outdir = self._output_directory or os.path.join(\n                \"biocypher-out\",\n                timestamp(),\n            )\n            self._output_directory = os.path.abspath(outdir)\n\n            self._writer = get_writer(\n                dbms=self._dbms,\n                translator=self._get_translator(),\n                deduplicator=self._get_deduplicator(),\n                output_directory=self._output_directory,\n                strict_mode=self._strict_mode,\n            )\n        else:\n            msg = \"Cannot get writer in online mode.\"\n            raise NotImplementedError(msg)\n\n        return self._writer\n\n    def _get_driver(self):\n        \"\"\"Create driver if not exists.\n\n        Set as instance variable `self._driver`.\n        \"\"\"\n        if not self._offline:\n            self._driver = get_connector(\n                dbms=self._dbms,\n                translator=self._get_translator(),\n            )\n        else:\n            msg = \"Cannot get driver in offline mode.\"\n            raise NotImplementedError(msg)\n\n        return self._driver\n\n    def _get_in_memory_kg(self):\n        \"\"\"Create in-memory KG instance.\n\n        Set as instance variable `self._in_memory_kg`.\n        \"\"\"\n        if not self._in_memory_kg:\n            self._in_memory_kg = get_in_memory_kg(\n                dbms=self._dbms,\n                deduplicator=self._get_deduplicator(),\n            )\n\n        return self._in_memory_kg\n\n    def _add_nodes(\n        self,\n        nodes,\n        batch_size: int = int(1e6),\n        force: bool = False,\n    ):\n        \"\"\"Add nodes to the BioCypher KG.\n\n        First uses the `_translator` to translate the nodes to `BioCypherNode`\n        objects. Depending on the configuration the translated nodes are then\n        passed to the\n\n        - `_writer`: if `_offline` is set to `False`\n\n        - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n            `IN_MEMORY_DBMS`\n\n        - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n            `IN_MEMORY_DBMS`\n\n        \"\"\"\n        if not self._translator:\n            self._get_translator()\n        translated_nodes = self._translator.translate_entities(nodes)\n\n        if self._offline:\n            passed = self._get_writer().write_nodes(\n                translated_nodes,\n                batch_size=batch_size,\n                force=force,\n            )\n        elif self._is_online_and_in_memory():\n            passed = self._get_in_memory_kg().add_nodes(translated_nodes)\n        else:\n            passed = self._get_driver().add_biocypher_nodes(translated_nodes)\n\n        return passed\n\n    def _add_edges(self, edges, batch_size: int = int(1e6)):\n        \"\"\"Add edges to the BioCypher KG.\n\n        First uses the `_translator` to translate the edges to `BioCypherEdge`\n        objects. Depending on the configuration the translated edges are then\n        passed to the\n\n        - `_writer`: if `_offline` is set to `False`\n\n        - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n            `IN_MEMORY_DBMS`\n\n        - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n            `IN_MEMORY_DBMS`\n\n        \"\"\"\n        if not self._translator:\n            self._get_translator()\n        translated_edges = self._translator.translate_entities(edges)\n\n        if self._offline:\n            if not self._writer:\n                self._initialize_writer()\n            passed = self._writer.write_edges(\n                translated_edges,\n                batch_size=batch_size,\n            )\n        elif self._is_online_and_in_memory():\n            if not self._in_memory_kg:\n                self._initialize_in_memory_kg()\n            passed = self._in_memory_kg.add_edges(translated_edges)\n        else:\n            if not self._driver:\n                self._initialize_driver()\n            passed = self._driver.add_biocypher_nodes(translated_edges)\n\n        return passed\n\n    def _is_online_and_in_memory(self) -&gt; bool:\n        \"\"\"Return True if in online mode and in-memory dbms is used.\"\"\"\n        return (not self._offline) &amp; (self._dbms in IN_MEMORY_DBMS)\n\n    def write_nodes(\n        self,\n        nodes,\n        batch_size: int = int(1e6),\n        force: bool = False,\n    ) -&gt; bool:\n        \"\"\"Write nodes to database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n        Args:\n        ----\n            nodes (iterable): An iterable of nodes to write to the database.\n            batch_size (int): The batch size to use when writing to disk.\n            force (bool): Whether to force writing to the output directory even\n                if the node type is not present in the schema config file.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_nodes(nodes, batch_size=batch_size, force=force)\n\n    def write_edges(self, edges, batch_size: int = int(1e6)) -&gt; bool:\n        \"\"\"Write edges to database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            edges (iterable): An iterable of edges to write to the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_edges(edges, batch_size=batch_size)\n\n    def add(self, entities) -&gt; None:\n        \"\"\"Add entities to the in-memory database.\n\n        Accepts an iterable of tuples (if given, translates to\n        ``BioCypherNode`` or ``BioCypherEdge`` objects) or an iterable of\n        ``BioCypherNode`` or ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            entities (iterable): An iterable of entities to add to the database.\n                Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n                4-tuples for edges (deprecated).\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        return self._add_nodes(entities)\n\n    def merge_nodes(self, nodes) -&gt; bool:\n        \"\"\"Merge nodes into database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n        Args:\n        ----\n            nodes (iterable): An iterable of nodes to merge into the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_nodes(nodes)\n\n    def merge_edges(self, edges) -&gt; bool:\n        \"\"\"Merge edges into database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            edges (iterable): An iterable of edges to merge into the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_edges(edges)\n\n    def get_kg(self):\n        \"\"\"Get the in-memory KG instance.\n\n        Depending on the specified `dbms` this could either be a list of Pandas\n        dataframes or a NetworkX DiGraph.\n        \"\"\"\n        if not self._is_online_and_in_memory():\n            msg = (f\"Getting the in-memory KG is only available in online mode for {IN_MEMORY_DBMS}.\",)\n            raise ValueError(msg)\n        if not self._in_memory_kg:\n            msg = \"No in-memory KG instance found. Please call `add()` first.\"\n            raise ValueError(msg)\n\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        return self._in_memory_kg.get_kg()\n\n    # DOWNLOAD AND CACHE MANAGEMENT METHODS ###\n\n    def _get_downloader(self, cache_dir: str | None = None):\n        \"\"\"Create downloader if not exists.\"\"\"\n        if not self._downloader:\n            self._downloader = Downloader(self._cache_directory)\n\n    def download(self, *resources) -&gt; None:\n        \"\"\"Download or load from cache the resources given by the adapter.\n\n        Args:\n        ----\n            resources (iterable): An iterable of resources to download or load\n                from cache.\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        self._get_downloader()\n        return self._downloader.download(*resources)\n\n    # OVERVIEW AND CONVENIENCE METHODS ###\n\n    def log_missing_input_labels(self) -&gt; dict[str, list[str]] | None:\n        \"\"\"Log missing input labels.\n\n        Get the set of input labels encountered without an entry in the\n        `schema_config.yaml` and print them to the logger.\n\n        Returns\n        -------\n            Optional[Dict[str, List[str]]]: A dictionary of Biolink types\n            encountered without an entry in the `schema_config.yaml` file.\n\n        \"\"\"\n        mt = self._translator.get_missing_biolink_types()\n\n        if mt:\n            msg = (\n                \"Input entities not accounted for due to them not being \"\n                f\"present in the schema configuration file {self._schema_config_path} \"\n                \"(this is not necessarily a problem, if you did not intend \"\n                \"to include them in the database; see the log for details): \\n\"\n            )\n            for k, v in mt.items():\n                msg += f\"    {k}: {v} \\n\"\n\n            logger.info(msg)\n            return mt\n\n        else:\n            logger.info(\"No missing labels in input.\")\n            return None\n\n    def log_duplicates(self) -&gt; None:\n        \"\"\"Log duplicate nodes and edges.\n\n        Get the set of duplicate nodes and edges encountered and print them to\n        the logger.\n        \"\"\"\n        dn = self._deduplicator.get_duplicate_nodes()\n\n        if dn:\n            ntypes = dn[0]\n            nids = dn[1]\n\n            msg = \"Duplicate node types encountered (IDs in log): \\n\"\n            for typ in ntypes:\n                msg += f\"    {typ}\\n\"\n\n            logger.info(msg)\n\n            idmsg = \"Duplicate node IDs encountered: \\n\"\n            for _id in nids:\n                idmsg += f\"    {_id}\\n\"\n\n            logger.debug(idmsg)\n\n        else:\n            logger.info(\"No duplicate nodes in input.\")\n\n        de = self._deduplicator.get_duplicate_edges()\n\n        if de:\n            etypes = de[0]\n            eids = de[1]\n\n            msg = \"Duplicate edge types encountered (IDs in log): \\n\"\n            for typ in etypes:\n                msg += f\"    {typ}\\n\"\n\n            logger.info(msg)\n\n            idmsg = \"Duplicate edge IDs encountered: \\n\"\n            for _id in eids:\n                idmsg += f\"    {_id}\\n\"\n\n            logger.debug(idmsg)\n\n        else:\n            logger.info(\"No duplicate edges in input.\")\n\n    def show_ontology_structure(self, **kwargs) -&gt; None:\n        \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n        Args:\n        ----\n            to_disk (str): If specified, the ontology structure will be saved\n                to disk as a GRAPHML file, to be opened in your favourite\n                graph visualisation tool.\n\n            full (bool): If True, the full ontology structure will be shown,\n                including all nodes and edges. If False, only the nodes and\n                edges that are relevant to the extended schema will be shown.\n\n        \"\"\"\n        if not self._ontology:\n            self._get_ontology()\n\n        return self._ontology.show_ontology_structure(**kwargs)\n\n    def write_import_call(self) -&gt; str:\n        \"\"\"Write a shell script to import the database.\n\n        Shell script is written depending on the chosen DBMS.\n\n        Returns\n        -------\n            str: path toward the file holding the import call.\n\n        \"\"\"\n        if not self._offline:\n            msg = \"Cannot write import call in online mode.\"\n            raise NotImplementedError(msg)\n\n        return self._writer.write_import_call()\n\n    def write_schema_info(self, as_node: bool = False) -&gt; None:\n        \"\"\"Write an extended schema info to file or node.\n\n        Creates a YAML file or KG node that extends the `schema_config.yaml`\n        with run-time information of the built KG. For instance, include\n        information on whether something present in the actual knowledge graph,\n        whether it is a relationship (which is important in the case of\n        representing relationships as nodes) and the actual sources and\n        targets of edges. Since this file can be used in place of the original\n        `schema_config.yaml` file, it indicates that it is the extended schema\n        by setting `is_schema_info` to `true`.\n\n        We start by using the `extended_schema` dictionary from the ontology\n        class instance, which contains all expanded entities and relationships.\n        The information of whether something is a relationship can be gathered\n        from the deduplicator instance, which keeps track of all entities that\n        have been seen.\n\n        Args:\n        ----\n            as_node (bool): If True, the schema info is written as a KG node.\n                If False, the schema info is written to a YAML file.\n\n        \"\"\"\n        if (not self._offline) and self._dbms not in IN_MEMORY_DBMS:\n            msg = \"Cannot write schema info in online mode.\"\n            raise NotImplementedError(msg)\n\n        ontology = self._get_ontology()\n        schema = ontology.mapping.extended_schema.copy()\n        schema[\"is_schema_info\"] = True\n\n        deduplicator = self._get_deduplicator()\n        for node in deduplicator.entity_types:\n            if node in schema:\n                schema[node][\"present_in_knowledge_graph\"] = True\n                schema[node][\"is_relationship\"] = False\n            else:\n                logger.info(\n                    f\"Node {node} not present in extended schema. Skipping schema info.\",\n                )\n\n        # find 'label_as_edge' cases in schema entries\n        changed_labels = {}\n        for k, v in schema.items():\n            if not isinstance(v, dict):\n                continue\n            if \"label_as_edge\" in v:\n                if v[\"label_as_edge\"] in deduplicator.seen_relationships:\n                    changed_labels[v[\"label_as_edge\"]] = k\n\n        for edge in deduplicator.seen_relationships:\n            if edge in changed_labels:\n                edge = changed_labels[edge]\n            if edge in schema:\n                schema[edge][\"present_in_knowledge_graph\"] = True\n                schema[edge][\"is_relationship\"] = True\n                # TODO information about source and target nodes\n            else:\n                logger.info(\n                    f\"Edge {edge} not present in extended schema. Skipping schema info.\",\n                )\n\n        # write to output directory as YAML file\n        path = os.path.join(self._output_directory, \"schema_info.yaml\")\n        with open(path, \"w\") as f:\n            f.write(yaml.dump(schema))\n\n        if as_node:\n            # write as node\n            node = BioCypherNode(\n                node_id=\"schema_info\",\n                node_label=\"schema_info\",\n                properties={\"schema_info\": json.dumps(schema)},\n            )\n            self.write_nodes([node], force=True)\n\n            # override import call with added schema info node\n            self.write_import_call()\n\n        return schema\n\n    # TRANSLATION METHODS ###\n\n    def translate_term(self, term: str) -&gt; str:\n        \"\"\"Translate a term to its BioCypher equivalent.\n\n        Args:\n        ----\n            term (str): The term to translate.\n\n        Returns:\n        -------\n            str: The BioCypher equivalent of the term.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.translate_term(term)\n\n    def summary(self) -&gt; None:\n        \"\"\"Call convenience and reporting methods.\n\n        Shows ontology structure and logs duplicates and missing input types.\n        \"\"\"\n        self.show_ontology_structure()\n        self.log_duplicates()\n        self.log_missing_input_labels()\n\n    def reverse_translate_term(self, term: str) -&gt; str:\n        \"\"\"Reverse translate a term from its BioCypher equivalent.\n\n        Args:\n        ----\n            term (str): The BioCypher term to reverse translate.\n\n        Returns:\n        -------\n            str: The original term.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.reverse_translate_term(term)\n\n    def translate_query(self, query: str) -&gt; str:\n        \"\"\"Translate a query to its BioCypher equivalent.\n\n        Args:\n        ----\n            query (str): The query to translate.\n\n        Returns:\n        -------\n            str: The BioCypher equivalent of the query.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.translate(query)\n\n    def reverse_translate_query(self, query: str) -&gt; str:\n        \"\"\"Reverse translate a query from its BioCypher equivalent.\n\n        Args:\n        ----\n            query (str): The BioCypher query to reverse translate.\n\n        Returns:\n        -------\n            str: The original query.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.reverse_translate(query)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._add_edges","title":"<code>_add_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Add edges to the BioCypher KG.</p> <p>First uses the <code>_translator</code> to translate the edges to <code>BioCypherEdge</code> objects. Depending on the configuration the translated edges are then passed to the</p> <ul> <li> <p><code>_writer</code>: if <code>_offline</code> is set to <code>False</code></p> </li> <li> <p><code>_in_memory_kg</code>: if <code>_offline</code> is set to <code>False</code> and the <code>_dbms</code> is an     <code>IN_MEMORY_DBMS</code></p> </li> <li> <p><code>_driver</code>: if <code>_offline</code> is set to <code>True</code> and the <code>_dbms</code> is not an     <code>IN_MEMORY_DBMS</code></p> </li> </ul> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _add_edges(self, edges, batch_size: int = int(1e6)):\n    \"\"\"Add edges to the BioCypher KG.\n\n    First uses the `_translator` to translate the edges to `BioCypherEdge`\n    objects. Depending on the configuration the translated edges are then\n    passed to the\n\n    - `_writer`: if `_offline` is set to `False`\n\n    - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n        `IN_MEMORY_DBMS`\n\n    - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n        `IN_MEMORY_DBMS`\n\n    \"\"\"\n    if not self._translator:\n        self._get_translator()\n    translated_edges = self._translator.translate_entities(edges)\n\n    if self._offline:\n        if not self._writer:\n            self._initialize_writer()\n        passed = self._writer.write_edges(\n            translated_edges,\n            batch_size=batch_size,\n        )\n    elif self._is_online_and_in_memory():\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        passed = self._in_memory_kg.add_edges(translated_edges)\n    else:\n        if not self._driver:\n            self._initialize_driver()\n        passed = self._driver.add_biocypher_nodes(translated_edges)\n\n    return passed\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._add_nodes","title":"<code>_add_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Add nodes to the BioCypher KG.</p> <p>First uses the <code>_translator</code> to translate the nodes to <code>BioCypherNode</code> objects. Depending on the configuration the translated nodes are then passed to the</p> <ul> <li> <p><code>_writer</code>: if <code>_offline</code> is set to <code>False</code></p> </li> <li> <p><code>_in_memory_kg</code>: if <code>_offline</code> is set to <code>False</code> and the <code>_dbms</code> is an     <code>IN_MEMORY_DBMS</code></p> </li> <li> <p><code>_driver</code>: if <code>_offline</code> is set to <code>True</code> and the <code>_dbms</code> is not an     <code>IN_MEMORY_DBMS</code></p> </li> </ul> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _add_nodes(\n    self,\n    nodes,\n    batch_size: int = int(1e6),\n    force: bool = False,\n):\n    \"\"\"Add nodes to the BioCypher KG.\n\n    First uses the `_translator` to translate the nodes to `BioCypherNode`\n    objects. Depending on the configuration the translated nodes are then\n    passed to the\n\n    - `_writer`: if `_offline` is set to `False`\n\n    - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n        `IN_MEMORY_DBMS`\n\n    - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n        `IN_MEMORY_DBMS`\n\n    \"\"\"\n    if not self._translator:\n        self._get_translator()\n    translated_nodes = self._translator.translate_entities(nodes)\n\n    if self._offline:\n        passed = self._get_writer().write_nodes(\n            translated_nodes,\n            batch_size=batch_size,\n            force=force,\n        )\n    elif self._is_online_and_in_memory():\n        passed = self._get_in_memory_kg().add_nodes(translated_nodes)\n    else:\n        passed = self._get_driver().add_biocypher_nodes(translated_nodes)\n\n    return passed\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_deduplicator","title":"<code>_get_deduplicator()</code>","text":"<p>Create deduplicator if not exists and return.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_deduplicator(self) -&gt; Deduplicator:\n    \"\"\"Create deduplicator if not exists and return.\"\"\"\n    if not self._deduplicator:\n        self._deduplicator = Deduplicator()\n\n    return self._deduplicator\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_downloader","title":"<code>_get_downloader(cache_dir=None)</code>","text":"<p>Create downloader if not exists.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_downloader(self, cache_dir: str | None = None):\n    \"\"\"Create downloader if not exists.\"\"\"\n    if not self._downloader:\n        self._downloader = Downloader(self._cache_directory)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_driver","title":"<code>_get_driver()</code>","text":"<p>Create driver if not exists.</p> <p>Set as instance variable <code>self._driver</code>.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_driver(self):\n    \"\"\"Create driver if not exists.\n\n    Set as instance variable `self._driver`.\n    \"\"\"\n    if not self._offline:\n        self._driver = get_connector(\n            dbms=self._dbms,\n            translator=self._get_translator(),\n        )\n    else:\n        msg = \"Cannot get driver in offline mode.\"\n        raise NotImplementedError(msg)\n\n    return self._driver\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_in_memory_kg","title":"<code>_get_in_memory_kg()</code>","text":"<p>Create in-memory KG instance.</p> <p>Set as instance variable <code>self._in_memory_kg</code>.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_in_memory_kg(self):\n    \"\"\"Create in-memory KG instance.\n\n    Set as instance variable `self._in_memory_kg`.\n    \"\"\"\n    if not self._in_memory_kg:\n        self._in_memory_kg = get_in_memory_kg(\n            dbms=self._dbms,\n            deduplicator=self._get_deduplicator(),\n        )\n\n    return self._in_memory_kg\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_ontology","title":"<code>_get_ontology()</code>","text":"<p>Create ontology if not exists and return.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_ontology(self) -&gt; Ontology:\n    \"\"\"Create ontology if not exists and return.\"\"\"\n    if not self._ontology:\n        self._ontology = Ontology(\n            ontology_mapping=self._get_ontology_mapping(),\n            head_ontology=self._head_ontology,\n            tail_ontologies=self._tail_ontologies,\n        )\n\n    return self._ontology\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_ontology_mapping","title":"<code>_get_ontology_mapping()</code>","text":"<p>Create ontology mapping if not exists and return.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_ontology_mapping(self) -&gt; OntologyMapping:\n    \"\"\"Create ontology mapping if not exists and return.\"\"\"\n    if not self._schema_config_path:\n        self._ontology_mapping = OntologyMapping()\n\n    if not self._ontology_mapping:\n        self._ontology_mapping = OntologyMapping(\n            config_file=self._schema_config_path,\n        )\n\n    return self._ontology_mapping\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_translator","title":"<code>_get_translator()</code>","text":"<p>Create translator if not exists and return.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_translator(self) -&gt; Translator:\n    \"\"\"Create translator if not exists and return.\"\"\"\n    if not self._translator:\n        self._translator = Translator(\n            ontology=self._get_ontology(),\n            strict_mode=self._strict_mode,\n        )\n\n    return self._translator\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._get_writer","title":"<code>_get_writer()</code>","text":"<p>Create writer if not online.</p> <p>Set as instance variable <code>self._writer</code>.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _get_writer(self):\n    \"\"\"Create writer if not online.\n\n    Set as instance variable `self._writer`.\n    \"\"\"\n    if self._offline:\n\n        def timestamp() -&gt; str:\n            return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        outdir = self._output_directory or os.path.join(\n            \"biocypher-out\",\n            timestamp(),\n        )\n        self._output_directory = os.path.abspath(outdir)\n\n        self._writer = get_writer(\n            dbms=self._dbms,\n            translator=self._get_translator(),\n            deduplicator=self._get_deduplicator(),\n            output_directory=self._output_directory,\n            strict_mode=self._strict_mode,\n        )\n    else:\n        msg = \"Cannot get writer in online mode.\"\n        raise NotImplementedError(msg)\n\n    return self._writer\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._initialize_in_memory_kg","title":"<code>_initialize_in_memory_kg()</code>","text":"<p>Create in-memory KG instance.</p> <p>Set as instance variable <code>self._in_memory_kg</code>.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _initialize_in_memory_kg(self) -&gt; None:\n    \"\"\"Create in-memory KG instance.\n\n    Set as instance variable `self._in_memory_kg`.\n    \"\"\"\n    if not self._in_memory_kg:\n        self._in_memory_kg = get_in_memory_kg(\n            dbms=self._dbms,\n            deduplicator=self._get_deduplicator(),\n        )\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._is_online_and_in_memory","title":"<code>_is_online_and_in_memory()</code>","text":"<p>Return True if in online mode and in-memory dbms is used.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _is_online_and_in_memory(self) -&gt; bool:\n    \"\"\"Return True if in online mode and in-memory dbms is used.\"\"\"\n    return (not self._offline) &amp; (self._dbms in IN_MEMORY_DBMS)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._to_KG","title":"<code>_to_KG()</code>","text":"<p>Convert the internal representation to knowledge graph.</p> <p>The knowledge graph is returned based on the <code>dbms</code> parameter in the biocypher configuration file.</p>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher._to_KG--returns","title":"Returns","text":"<pre><code> Any: knowledge graph.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def _to_KG(self):\n    \"\"\"Convert the internal representation to knowledge graph.\n\n    The knowledge graph is returned based on the `dbms` parameter in\n    the biocypher configuration file.\n\n    Returns\n    -------\n         Any: knowledge graph.\n\n    \"\"\"\n    if not self._in_memory_kg:\n        self._initialize_in_memory_kg()\n    if not self._translator:\n        self._get_translator()\n    tnodes = self._translator.translate_entities(self._nodes)\n    tedges = self._translator.translate_entities(self._edges)\n    self._in_memory_kg.add_nodes(tnodes)\n    self._in_memory_kg.add_edges(tedges)\n    return self._in_memory_kg.get_kg()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.add","title":"<code>add(entities)</code>","text":"<p>Add entities to the in-memory database.</p> <p>Accepts an iterable of tuples (if given, translates to <code>BioCypherNode</code> or <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherNode</code> or <code>BioCypherEdge</code> objects.</p> <pre><code>entities (iterable): An iterable of entities to add to the database.\n    Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n    4-tuples for edges (deprecated).\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def add(self, entities) -&gt; None:\n    \"\"\"Add entities to the in-memory database.\n\n    Accepts an iterable of tuples (if given, translates to\n    ``BioCypherNode`` or ``BioCypherEdge`` objects) or an iterable of\n    ``BioCypherNode`` or ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        entities (iterable): An iterable of entities to add to the database.\n            Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n            4-tuples for edges (deprecated).\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    return self._add_nodes(entities)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.add_edges","title":"<code>add_edges(edges)</code>","text":"<p>Add new edges to the internal representation.</p> <p>Initially, receive edges data from adaptor and create internal representation for edges.</p> <pre><code> edges(iterable): An iterable of edges.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def add_edges(self, edges) -&gt; None:\n    \"\"\"Add new edges to the internal representation.\n\n    Initially, receive edges data from adaptor and create internal\n    representation for edges.\n\n    Args:\n    ----\n         edges(iterable): An iterable of edges.\n\n    \"\"\"\n    if isinstance(edges, list):\n        self._edges = list(itertools.chain(self._edges, edges))\n    else:\n        self._edges = itertools.chain(self._edges, edges)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Add new nodes to the internal representation.</p> <p>Initially, receive nodes data from adaptor and create internal representation for nodes.</p> <pre><code>nodes(iterable): An iterable of nodes\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def add_nodes(self, nodes) -&gt; None:\n    \"\"\"Add new nodes to the internal representation.\n\n    Initially, receive nodes data from adaptor and create internal\n    representation for nodes.\n\n    Args:\n    ----\n        nodes(iterable): An iterable of nodes\n\n    \"\"\"\n    if isinstance(nodes, list):\n        self._nodes = list(itertools.chain(self._nodes, nodes))\n    else:\n        self._nodes = itertools.chain(self._nodes, nodes)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.download","title":"<code>download(*resources)</code>","text":"<p>Download or load from cache the resources given by the adapter.</p> <pre><code>resources (iterable): An iterable of resources to download or load\n    from cache.\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def download(self, *resources) -&gt; None:\n    \"\"\"Download or load from cache the resources given by the adapter.\n\n    Args:\n    ----\n        resources (iterable): An iterable of resources to download or load\n            from cache.\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    self._get_downloader()\n    return self._downloader.download(*resources)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.get_kg","title":"<code>get_kg()</code>","text":"<p>Get the in-memory KG instance.</p> <p>Depending on the specified <code>dbms</code> this could either be a list of Pandas dataframes or a NetworkX DiGraph.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def get_kg(self):\n    \"\"\"Get the in-memory KG instance.\n\n    Depending on the specified `dbms` this could either be a list of Pandas\n    dataframes or a NetworkX DiGraph.\n    \"\"\"\n    if not self._is_online_and_in_memory():\n        msg = (f\"Getting the in-memory KG is only available in online mode for {IN_MEMORY_DBMS}.\",)\n        raise ValueError(msg)\n    if not self._in_memory_kg:\n        msg = \"No in-memory KG instance found. Please call `add()` first.\"\n        raise ValueError(msg)\n\n    if not self._in_memory_kg:\n        self._initialize_in_memory_kg()\n    return self._in_memory_kg.get_kg()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.log_duplicates","title":"<code>log_duplicates()</code>","text":"<p>Log duplicate nodes and edges.</p> <p>Get the set of duplicate nodes and edges encountered and print them to the logger.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def log_duplicates(self) -&gt; None:\n    \"\"\"Log duplicate nodes and edges.\n\n    Get the set of duplicate nodes and edges encountered and print them to\n    the logger.\n    \"\"\"\n    dn = self._deduplicator.get_duplicate_nodes()\n\n    if dn:\n        ntypes = dn[0]\n        nids = dn[1]\n\n        msg = \"Duplicate node types encountered (IDs in log): \\n\"\n        for typ in ntypes:\n            msg += f\"    {typ}\\n\"\n\n        logger.info(msg)\n\n        idmsg = \"Duplicate node IDs encountered: \\n\"\n        for _id in nids:\n            idmsg += f\"    {_id}\\n\"\n\n        logger.debug(idmsg)\n\n    else:\n        logger.info(\"No duplicate nodes in input.\")\n\n    de = self._deduplicator.get_duplicate_edges()\n\n    if de:\n        etypes = de[0]\n        eids = de[1]\n\n        msg = \"Duplicate edge types encountered (IDs in log): \\n\"\n        for typ in etypes:\n            msg += f\"    {typ}\\n\"\n\n        logger.info(msg)\n\n        idmsg = \"Duplicate edge IDs encountered: \\n\"\n        for _id in eids:\n            idmsg += f\"    {_id}\\n\"\n\n        logger.debug(idmsg)\n\n    else:\n        logger.info(\"No duplicate edges in input.\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.log_missing_input_labels","title":"<code>log_missing_input_labels()</code>","text":"<p>Log missing input labels.</p> <p>Get the set of input labels encountered without an entry in the <code>schema_config.yaml</code> and print them to the logger.</p>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.log_missing_input_labels--returns","title":"Returns","text":"<pre><code>Optional[Dict[str, List[str]]]: A dictionary of Biolink types\nencountered without an entry in the `schema_config.yaml` file.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def log_missing_input_labels(self) -&gt; dict[str, list[str]] | None:\n    \"\"\"Log missing input labels.\n\n    Get the set of input labels encountered without an entry in the\n    `schema_config.yaml` and print them to the logger.\n\n    Returns\n    -------\n        Optional[Dict[str, List[str]]]: A dictionary of Biolink types\n        encountered without an entry in the `schema_config.yaml` file.\n\n    \"\"\"\n    mt = self._translator.get_missing_biolink_types()\n\n    if mt:\n        msg = (\n            \"Input entities not accounted for due to them not being \"\n            f\"present in the schema configuration file {self._schema_config_path} \"\n            \"(this is not necessarily a problem, if you did not intend \"\n            \"to include them in the database; see the log for details): \\n\"\n        )\n        for k, v in mt.items():\n            msg += f\"    {k}: {v} \\n\"\n\n        logger.info(msg)\n        return mt\n\n    else:\n        logger.info(\"No missing labels in input.\")\n        return None\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.merge_edges","title":"<code>merge_edges(edges)</code>","text":"<p>Merge edges into database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherEdge</code> objects.</p> <pre><code>edges (iterable): An iterable of edges to merge into the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def merge_edges(self, edges) -&gt; bool:\n    \"\"\"Merge edges into database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        edges (iterable): An iterable of edges to merge into the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_edges(edges)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.merge_nodes","title":"<code>merge_nodes(nodes)</code>","text":"<p>Merge nodes into database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherNode</code> objects) or an iterable of <code>BioCypherNode</code> objects.</p> <pre><code>nodes (iterable): An iterable of nodes to merge into the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def merge_nodes(self, nodes) -&gt; bool:\n    \"\"\"Merge nodes into database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n    Args:\n    ----\n        nodes (iterable): An iterable of nodes to merge into the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_nodes(nodes)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.reverse_translate_query","title":"<code>reverse_translate_query(query)</code>","text":"<p>Reverse translate a query from its BioCypher equivalent.</p> <pre><code>query (str): The BioCypher query to reverse translate.\n</code></pre> <pre><code>str: The original query.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def reverse_translate_query(self, query: str) -&gt; str:\n    \"\"\"Reverse translate a query from its BioCypher equivalent.\n\n    Args:\n    ----\n        query (str): The BioCypher query to reverse translate.\n\n    Returns:\n    -------\n        str: The original query.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.reverse_translate(query)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.reverse_translate_term","title":"<code>reverse_translate_term(term)</code>","text":"<p>Reverse translate a term from its BioCypher equivalent.</p> <pre><code>term (str): The BioCypher term to reverse translate.\n</code></pre> <pre><code>str: The original term.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def reverse_translate_term(self, term: str) -&gt; str:\n    \"\"\"Reverse translate a term from its BioCypher equivalent.\n\n    Args:\n    ----\n        term (str): The BioCypher term to reverse translate.\n\n    Returns:\n    -------\n        str: The original term.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.reverse_translate_term(term)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.show_ontology_structure","title":"<code>show_ontology_structure(**kwargs)</code>","text":"<p>Show the ontology structure using treelib or write to GRAPHML file.</p> <pre><code>to_disk (str): If specified, the ontology structure will be saved\n    to disk as a GRAPHML file, to be opened in your favourite\n    graph visualisation tool.\n\nfull (bool): If True, the full ontology structure will be shown,\n    including all nodes and edges. If False, only the nodes and\n    edges that are relevant to the extended schema will be shown.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def show_ontology_structure(self, **kwargs) -&gt; None:\n    \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n    Args:\n    ----\n        to_disk (str): If specified, the ontology structure will be saved\n            to disk as a GRAPHML file, to be opened in your favourite\n            graph visualisation tool.\n\n        full (bool): If True, the full ontology structure will be shown,\n            including all nodes and edges. If False, only the nodes and\n            edges that are relevant to the extended schema will be shown.\n\n    \"\"\"\n    if not self._ontology:\n        self._get_ontology()\n\n    return self._ontology.show_ontology_structure(**kwargs)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.summary","title":"<code>summary()</code>","text":"<p>Call convenience and reporting methods.</p> <p>Shows ontology structure and logs duplicates and missing input types.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Call convenience and reporting methods.\n\n    Shows ontology structure and logs duplicates and missing input types.\n    \"\"\"\n    self.show_ontology_structure()\n    self.log_duplicates()\n    self.log_missing_input_labels()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.to_df","title":"<code>to_df()</code>","text":"<p>Create DataFrame using internal representation.</p> <p>TODO: to_df implies data frame, should be specifically that use case</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def to_df(self):\n    \"\"\"Create DataFrame using internal representation.\n\n    TODO: to_df implies data frame, should be specifically that use case\n    \"\"\"\n    return self._to_KG()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.to_networkx","title":"<code>to_networkx()</code>","text":"<p>Create networkx using internal representation.</p> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def to_networkx(self):\n    \"\"\"Create networkx using internal representation.\"\"\"\n    return self._to_KG()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.translate_query","title":"<code>translate_query(query)</code>","text":"<p>Translate a query to its BioCypher equivalent.</p> <pre><code>query (str): The query to translate.\n</code></pre> <pre><code>str: The BioCypher equivalent of the query.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def translate_query(self, query: str) -&gt; str:\n    \"\"\"Translate a query to its BioCypher equivalent.\n\n    Args:\n    ----\n        query (str): The query to translate.\n\n    Returns:\n    -------\n        str: The BioCypher equivalent of the query.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.translate(query)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.translate_term","title":"<code>translate_term(term)</code>","text":"<p>Translate a term to its BioCypher equivalent.</p> <pre><code>term (str): The term to translate.\n</code></pre> <pre><code>str: The BioCypher equivalent of the term.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def translate_term(self, term: str) -&gt; str:\n    \"\"\"Translate a term to its BioCypher equivalent.\n\n    Args:\n    ----\n        term (str): The term to translate.\n\n    Returns:\n    -------\n        str: The BioCypher equivalent of the term.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.translate_term(term)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.write_edges","title":"<code>write_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Write edges to database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherEdge</code> objects.</p> <pre><code>edges (iterable): An iterable of edges to write to the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def write_edges(self, edges, batch_size: int = int(1e6)) -&gt; bool:\n    \"\"\"Write edges to database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        edges (iterable): An iterable of edges to write to the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_edges(edges, batch_size=batch_size)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.write_import_call","title":"<code>write_import_call()</code>","text":"<p>Write a shell script to import the database.</p> <p>Shell script is written depending on the chosen DBMS.</p>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.write_import_call--returns","title":"Returns","text":"<pre><code>str: path toward the file holding the import call.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def write_import_call(self) -&gt; str:\n    \"\"\"Write a shell script to import the database.\n\n    Shell script is written depending on the chosen DBMS.\n\n    Returns\n    -------\n        str: path toward the file holding the import call.\n\n    \"\"\"\n    if not self._offline:\n        msg = \"Cannot write import call in online mode.\"\n        raise NotImplementedError(msg)\n\n    return self._writer.write_import_call()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.write_nodes","title":"<code>write_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Write nodes to database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherNode</code> objects) or an iterable of <code>BioCypherNode</code> objects.</p> <pre><code>nodes (iterable): An iterable of nodes to write to the database.\nbatch_size (int): The batch size to use when writing to disk.\nforce (bool): Whether to force writing to the output directory even\n    if the node type is not present in the schema config file.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def write_nodes(\n    self,\n    nodes,\n    batch_size: int = int(1e6),\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"Write nodes to database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n    Args:\n    ----\n        nodes (iterable): An iterable of nodes to write to the database.\n        batch_size (int): The batch size to use when writing to disk.\n        force (bool): Whether to force writing to the output directory even\n            if the node type is not present in the schema config file.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_nodes(nodes, batch_size=batch_size, force=force)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._core.BioCypher.write_schema_info","title":"<code>write_schema_info(as_node=False)</code>","text":"<p>Write an extended schema info to file or node.</p> <p>Creates a YAML file or KG node that extends the <code>schema_config.yaml</code> with run-time information of the built KG. For instance, include information on whether something present in the actual knowledge graph, whether it is a relationship (which is important in the case of representing relationships as nodes) and the actual sources and targets of edges. Since this file can be used in place of the original <code>schema_config.yaml</code> file, it indicates that it is the extended schema by setting <code>is_schema_info</code> to <code>true</code>.</p> <p>We start by using the <code>extended_schema</code> dictionary from the ontology class instance, which contains all expanded entities and relationships. The information of whether something is a relationship can be gathered from the deduplicator instance, which keeps track of all entities that have been seen.</p> <pre><code>as_node (bool): If True, the schema info is written as a KG node.\n    If False, the schema info is written to a YAML file.\n</code></pre> Source code in <code>biocypher/biocypher/_core.py</code> <pre><code>def write_schema_info(self, as_node: bool = False) -&gt; None:\n    \"\"\"Write an extended schema info to file or node.\n\n    Creates a YAML file or KG node that extends the `schema_config.yaml`\n    with run-time information of the built KG. For instance, include\n    information on whether something present in the actual knowledge graph,\n    whether it is a relationship (which is important in the case of\n    representing relationships as nodes) and the actual sources and\n    targets of edges. Since this file can be used in place of the original\n    `schema_config.yaml` file, it indicates that it is the extended schema\n    by setting `is_schema_info` to `true`.\n\n    We start by using the `extended_schema` dictionary from the ontology\n    class instance, which contains all expanded entities and relationships.\n    The information of whether something is a relationship can be gathered\n    from the deduplicator instance, which keeps track of all entities that\n    have been seen.\n\n    Args:\n    ----\n        as_node (bool): If True, the schema info is written as a KG node.\n            If False, the schema info is written to a YAML file.\n\n    \"\"\"\n    if (not self._offline) and self._dbms not in IN_MEMORY_DBMS:\n        msg = \"Cannot write schema info in online mode.\"\n        raise NotImplementedError(msg)\n\n    ontology = self._get_ontology()\n    schema = ontology.mapping.extended_schema.copy()\n    schema[\"is_schema_info\"] = True\n\n    deduplicator = self._get_deduplicator()\n    for node in deduplicator.entity_types:\n        if node in schema:\n            schema[node][\"present_in_knowledge_graph\"] = True\n            schema[node][\"is_relationship\"] = False\n        else:\n            logger.info(\n                f\"Node {node} not present in extended schema. Skipping schema info.\",\n            )\n\n    # find 'label_as_edge' cases in schema entries\n    changed_labels = {}\n    for k, v in schema.items():\n        if not isinstance(v, dict):\n            continue\n        if \"label_as_edge\" in v:\n            if v[\"label_as_edge\"] in deduplicator.seen_relationships:\n                changed_labels[v[\"label_as_edge\"]] = k\n\n    for edge in deduplicator.seen_relationships:\n        if edge in changed_labels:\n            edge = changed_labels[edge]\n        if edge in schema:\n            schema[edge][\"present_in_knowledge_graph\"] = True\n            schema[edge][\"is_relationship\"] = True\n            # TODO information about source and target nodes\n        else:\n            logger.info(\n                f\"Edge {edge} not present in extended schema. Skipping schema info.\",\n            )\n\n    # write to output directory as YAML file\n    path = os.path.join(self._output_directory, \"schema_info.yaml\")\n    with open(path, \"w\") as f:\n        f.write(yaml.dump(schema))\n\n    if as_node:\n        # write as node\n        node = BioCypherNode(\n            node_id=\"schema_info\",\n            node_label=\"schema_info\",\n            properties={\"schema_info\": json.dumps(schema)},\n        )\n        self.write_nodes([node], force=True)\n\n        # override import call with added schema info node\n        self.write_import_call()\n\n    return schema\n</code></pre>"},{"location":"BioCypher/reference/source/#_createpy","title":"_create.py","text":"<p>BioCypher 'create' module. Handles the creation of BioCypher node and edge dataclasses.</p>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge","title":"<code>BioCypherEdge</code>  <code>dataclass</code>","text":"<p>Handoff class to represent biomedical relationships in Neo4j.</p> <p>Has source and target ids, label, property dict; ids and label (in the Neo4j sense of a label, ie, the entity descriptor after the colon, such as \":TARGETS\") are non-optional and called source_id, target_id, and relationship_label to avoid confusion with properties called \"label\", which usually denotes the human-readable form. Relationship labels are written in UPPERCASE and as verbs, as per Neo4j consensus.</p> <p>Args:</p> <pre><code>source_id (string): consensus \"best\" id for biological entity\n\ntarget_id (string): consensus \"best\" id for biological entity\n\nrelationship_label (string): type of interaction, UPPERCASE\n\nproperties (dict): collection of all other properties of the\nrespective edge\n</code></pre> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherEdge:\n    \"\"\"\n    Handoff class to represent biomedical relationships in Neo4j.\n\n    Has source and target ids, label, property dict; ids and label (in\n    the Neo4j sense of a label, ie, the entity descriptor after the\n    colon, such as \":TARGETS\") are non-optional and called source_id,\n    target_id, and relationship_label to avoid confusion with properties\n    called \"label\", which usually denotes the human-readable form.\n    Relationship labels are written in UPPERCASE and as verbs, as per\n    Neo4j consensus.\n\n    Args:\n\n        source_id (string): consensus \"best\" id for biological entity\n\n        target_id (string): consensus \"best\" id for biological entity\n\n        relationship_label (string): type of interaction, UPPERCASE\n\n        properties (dict): collection of all other properties of the\n        respective edge\n\n    \"\"\"\n\n    source_id: str\n    target_id: str\n    relationship_label: str\n    relationship_id: str = None\n    properties: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"\n        Check for reserved keywords.\n        \"\"\"\n\n        if \":TYPE\" in self.properties.keys():\n            logger.debug(\n                \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\":TYPE\"]\n        elif \"id\" in self.properties.keys():\n            logger.debug(\n                \"Keyword 'id' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\"id\"]\n        elif \"_ID\" in self.properties.keys():\n            logger.debug(\n                \"Keyword '_ID' is reserved for Postgres. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\"_ID\"]\n\n    def get_id(self) -&gt; Union[str, None]:\n        \"\"\"\n        Returns primary node identifier or None.\n\n        Returns:\n            str: node_id\n        \"\"\"\n\n        return self.relationship_id\n\n    def get_source_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier of relationship source.\n\n        Returns:\n            str: source_id\n        \"\"\"\n        return self.source_id\n\n    def get_target_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier of relationship target.\n\n        Returns:\n            str: target_id\n        \"\"\"\n        return self.target_id\n\n    def get_label(self) -&gt; str:\n        \"\"\"\n        Returns relationship label.\n\n        Returns:\n            str: relationship_label\n        \"\"\"\n        return self.relationship_label\n\n    def get_type(self) -&gt; str:\n        \"\"\"\n        Returns relationship label.\n\n        Returns:\n            str: relationship_label\n        \"\"\"\n        return self.relationship_label\n\n    def get_properties(self) -&gt; dict:\n        \"\"\"\n        Returns all other relationship properties apart from primary ids\n        and label as key-value pairs.\n\n        Returns:\n            dict: properties\n        \"\"\"\n        return self.properties\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"\n        Return dict of ids, label, and properties.\n\n        Returns:\n            dict: source_id, target_id and relationship_label as\n                top-level key-value pairs, properties as second-level\n                dict.\n        \"\"\"\n        return {\n            \"relationship_id\": self.relationship_id or None,\n            \"source_id\": self.source_id,\n            \"target_id\": self.target_id,\n            \"relationship_label\": self.relationship_label,\n            \"properties\": self.properties,\n        }\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check for reserved keywords.</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Check for reserved keywords.\n    \"\"\"\n\n    if \":TYPE\" in self.properties.keys():\n        logger.debug(\n            \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\":TYPE\"]\n    elif \"id\" in self.properties.keys():\n        logger.debug(\n            \"Keyword 'id' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\"id\"]\n    elif \"_ID\" in self.properties.keys():\n        logger.debug(\n            \"Keyword '_ID' is reserved for Postgres. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\"_ID\"]\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_dict","title":"<code>get_dict()</code>","text":"<p>Return dict of ids, label, and properties.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>source_id, target_id and relationship_label as top-level key-value pairs, properties as second-level dict.</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"\n    Return dict of ids, label, and properties.\n\n    Returns:\n        dict: source_id, target_id and relationship_label as\n            top-level key-value pairs, properties as second-level\n            dict.\n    \"\"\"\n    return {\n        \"relationship_id\": self.relationship_id or None,\n        \"source_id\": self.source_id,\n        \"target_id\": self.target_id,\n        \"relationship_label\": self.relationship_label,\n        \"properties\": self.properties,\n    }\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_id","title":"<code>get_id()</code>","text":"<p>Returns primary node identifier or None.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>node_id</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_id(self) -&gt; Union[str, None]:\n    \"\"\"\n    Returns primary node identifier or None.\n\n    Returns:\n        str: node_id\n    \"\"\"\n\n    return self.relationship_id\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_label","title":"<code>get_label()</code>","text":"<p>Returns relationship label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>relationship_label</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_label(self) -&gt; str:\n    \"\"\"\n    Returns relationship label.\n\n    Returns:\n        str: relationship_label\n    \"\"\"\n    return self.relationship_label\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_properties","title":"<code>get_properties()</code>","text":"<p>Returns all other relationship properties apart from primary ids and label as key-value pairs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>properties</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_properties(self) -&gt; dict:\n    \"\"\"\n    Returns all other relationship properties apart from primary ids\n    and label as key-value pairs.\n\n    Returns:\n        dict: properties\n    \"\"\"\n    return self.properties\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_source_id","title":"<code>get_source_id()</code>","text":"<p>Returns primary node identifier of relationship source.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>source_id</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_source_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier of relationship source.\n\n    Returns:\n        str: source_id\n    \"\"\"\n    return self.source_id\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_target_id","title":"<code>get_target_id()</code>","text":"<p>Returns primary node identifier of relationship target.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>target_id</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_target_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier of relationship target.\n\n    Returns:\n        str: target_id\n    \"\"\"\n    return self.target_id\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherEdge.get_type","title":"<code>get_type()</code>","text":"<p>Returns relationship label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>relationship_label</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"\n    Returns relationship label.\n\n    Returns:\n        str: relationship_label\n    \"\"\"\n    return self.relationship_label\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode","title":"<code>BioCypherNode</code>  <code>dataclass</code>","text":"<p>Handoff class to represent biomedical entities as Neo4j nodes.</p> <p>Has id, label, property dict; id and label (in the Neo4j sense of a label, ie, the entity descriptor after the colon, such as \":Protein\") are non-optional and called node_id and node_label to avoid confusion with \"label\" properties. Node labels are written in PascalCase and as nouns, as per Neo4j consensus.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>string</code> <p>consensus \"best\" id for biological entity</p> required <code>node_label</code> <code>string</code> <p>primary type of entity, capitalised</p> required <code>**properties</code> <code>kwargs</code> <p>collection of all other properties to be passed to neo4j for the respective node (dict)</p> <code>dict()</code> Todo <ul> <li>check and correct small inconsistencies such as capitalisation     of ID names (\"uniprot\" vs \"UniProt\")</li> <li>check for correct ID patterns (eg \"ENSG\" + string of numbers,     uniprot length)</li> <li>ID conversion using pypath translation facilities for now</li> </ul> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherNode:\n    \"\"\"\n    Handoff class to represent biomedical entities as Neo4j nodes.\n\n    Has id, label, property dict; id and label (in the Neo4j sense of a\n    label, ie, the entity descriptor after the colon, such as\n    \":Protein\") are non-optional and called node_id and node_label to\n    avoid confusion with \"label\" properties. Node labels are written in\n    PascalCase and as nouns, as per Neo4j consensus.\n\n    Args:\n        node_id (string): consensus \"best\" id for biological entity\n        node_label (string): primary type of entity, capitalised\n        **properties (kwargs): collection of all other properties to be\n            passed to neo4j for the respective node (dict)\n\n    Todo:\n        - check and correct small inconsistencies such as capitalisation\n            of ID names (\"uniprot\" vs \"UniProt\")\n        - check for correct ID patterns (eg \"ENSG\" + string of numbers,\n            uniprot length)\n        - ID conversion using pypath translation facilities for now\n    \"\"\"\n\n    node_id: str\n    node_label: str\n    preferred_id: str = \"id\"\n    properties: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"\n        Add id field to properties.\n\n        Check for reserved keywords.\n\n        Replace unwanted characters in properties.\n        \"\"\"\n        self.properties[\"id\"] = self.node_id\n        self.properties[\"preferred_id\"] = self.preferred_id or None\n        # TODO actually make None possible here; as is, \"id\" is the default in\n        # the dataclass as well as in the configuration file\n\n        if \":TYPE\" in self.properties.keys():\n            logger.warning(\n                \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\":TYPE\"]\n\n        for k, v in self.properties.items():\n            if isinstance(v, str):\n                self.properties[k] = (\n                    v.replace(\n                        os.linesep,\n                        \" \",\n                    )\n                    .replace(\n                        \"\\n\",\n                        \" \",\n                    )\n                    .replace(\n                        \"\\r\",\n                        \" \",\n                    )\n                )\n\n            elif isinstance(v, list):\n                self.properties[k] = [\n                    val.replace(\n                        os.linesep,\n                        \" \",\n                    )\n                    .replace(\n                        \"\\n\",\n                        \" \",\n                    )\n                    .replace(\"\\r\", \" \")\n                    for val in v\n                ]\n\n    def get_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier.\n\n        Returns:\n            str: node_id\n        \"\"\"\n        return self.node_id\n\n    def get_label(self) -&gt; str:\n        \"\"\"\n        Returns primary node label.\n\n        Returns:\n            str: node_label\n        \"\"\"\n        return self.node_label\n\n    def get_type(self) -&gt; str:\n        \"\"\"\n        Returns primary node label.\n\n        Returns:\n            str: node_label\n        \"\"\"\n        return self.node_label\n\n    def get_preferred_id(self) -&gt; str:\n        \"\"\"\n        Returns preferred id.\n\n        Returns:\n            str: preferred_id\n        \"\"\"\n        return self.preferred_id\n\n    def get_properties(self) -&gt; dict:\n        \"\"\"\n        Returns all other node properties apart from primary id and\n        label as key-value pairs.\n\n        Returns:\n            dict: properties\n        \"\"\"\n        return self.properties\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"\n        Return dict of id, labels, and properties.\n\n        Returns:\n            dict: node_id and node_label as top-level key-value pairs,\n            properties as second-level dict.\n        \"\"\"\n        return {\n            \"node_id\": self.node_id,\n            \"node_label\": self.node_label,\n            \"properties\": self.properties,\n        }\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Add id field to properties.</p> <p>Check for reserved keywords.</p> <p>Replace unwanted characters in properties.</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Add id field to properties.\n\n    Check for reserved keywords.\n\n    Replace unwanted characters in properties.\n    \"\"\"\n    self.properties[\"id\"] = self.node_id\n    self.properties[\"preferred_id\"] = self.preferred_id or None\n    # TODO actually make None possible here; as is, \"id\" is the default in\n    # the dataclass as well as in the configuration file\n\n    if \":TYPE\" in self.properties.keys():\n        logger.warning(\n            \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\":TYPE\"]\n\n    for k, v in self.properties.items():\n        if isinstance(v, str):\n            self.properties[k] = (\n                v.replace(\n                    os.linesep,\n                    \" \",\n                )\n                .replace(\n                    \"\\n\",\n                    \" \",\n                )\n                .replace(\n                    \"\\r\",\n                    \" \",\n                )\n            )\n\n        elif isinstance(v, list):\n            self.properties[k] = [\n                val.replace(\n                    os.linesep,\n                    \" \",\n                )\n                .replace(\n                    \"\\n\",\n                    \" \",\n                )\n                .replace(\"\\r\", \" \")\n                for val in v\n            ]\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.get_dict","title":"<code>get_dict()</code>","text":"<p>Return dict of id, labels, and properties.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>node_id and node_label as top-level key-value pairs,</p> <code>dict</code> <p>properties as second-level dict.</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"\n    Return dict of id, labels, and properties.\n\n    Returns:\n        dict: node_id and node_label as top-level key-value pairs,\n        properties as second-level dict.\n    \"\"\"\n    return {\n        \"node_id\": self.node_id,\n        \"node_label\": self.node_label,\n        \"properties\": self.properties,\n    }\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.get_id","title":"<code>get_id()</code>","text":"<p>Returns primary node identifier.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_id</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier.\n\n    Returns:\n        str: node_id\n    \"\"\"\n    return self.node_id\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.get_label","title":"<code>get_label()</code>","text":"<p>Returns primary node label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_label</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_label(self) -&gt; str:\n    \"\"\"\n    Returns primary node label.\n\n    Returns:\n        str: node_label\n    \"\"\"\n    return self.node_label\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.get_preferred_id","title":"<code>get_preferred_id()</code>","text":"<p>Returns preferred id.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>preferred_id</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_preferred_id(self) -&gt; str:\n    \"\"\"\n    Returns preferred id.\n\n    Returns:\n        str: preferred_id\n    \"\"\"\n    return self.preferred_id\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.get_properties","title":"<code>get_properties()</code>","text":"<p>Returns all other node properties apart from primary id and label as key-value pairs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>properties</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_properties(self) -&gt; dict:\n    \"\"\"\n    Returns all other node properties apart from primary id and\n    label as key-value pairs.\n\n    Returns:\n        dict: properties\n    \"\"\"\n    return self.properties\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherNode.get_type","title":"<code>get_type()</code>","text":"<p>Returns primary node label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_label</p> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"\n    Returns primary node label.\n\n    Returns:\n        str: node_label\n    \"\"\"\n    return self.node_label\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._create.BioCypherRelAsNode","title":"<code>BioCypherRelAsNode</code>  <code>dataclass</code>","text":"<p>Class to represent relationships as nodes (with in- and outgoing edges) as a triplet of a BioCypherNode and two BioCypherEdges. Main usage in type checking (instances where the receiving function needs to check whether it receives a relationship as a single edge or as a triplet).</p> <p>Args:</p> <pre><code>node (BioCypherNode): node representing the relationship\n\nsource_edge (BioCypherEdge): edge representing the source of the\n    relationship\n\ntarget_edge (BioCypherEdge): edge representing the target of the\n    relationship\n</code></pre> Source code in <code>biocypher/biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherRelAsNode:\n    \"\"\"\n    Class to represent relationships as nodes (with in- and outgoing\n    edges) as a triplet of a BioCypherNode and two BioCypherEdges. Main\n    usage in type checking (instances where the receiving function needs\n    to check whether it receives a relationship as a single edge or as\n    a triplet).\n\n    Args:\n\n        node (BioCypherNode): node representing the relationship\n\n        source_edge (BioCypherEdge): edge representing the source of the\n            relationship\n\n        target_edge (BioCypherEdge): edge representing the target of the\n            relationship\n\n    \"\"\"\n\n    node: BioCypherNode\n    source_edge: BioCypherEdge\n    target_edge: BioCypherEdge\n\n    def __post_init__(self):\n        if not isinstance(self.node, BioCypherNode):\n            raise TypeError(\n                f\"BioCypherRelAsNode.node must be a BioCypherNode, \" f\"not {type(self.node)}.\",\n            )\n\n        if not isinstance(self.source_edge, BioCypherEdge):\n            raise TypeError(\n                f\"BioCypherRelAsNode.source_edge must be a BioCypherEdge, \" f\"not {type(self.source_edge)}.\",\n            )\n\n        if not isinstance(self.target_edge, BioCypherEdge):\n            raise TypeError(\n                f\"BioCypherRelAsNode.target_edge must be a BioCypherEdge, \" f\"not {type(self.target_edge)}.\",\n            )\n\n    def get_node(self) -&gt; BioCypherNode:\n        return self.node\n\n    def get_source_edge(self) -&gt; BioCypherEdge:\n        return self.source_edge\n\n    def get_target_edge(self) -&gt; BioCypherEdge:\n        return self.target_edge\n</code></pre>"},{"location":"BioCypher/reference/source/#_deduplicatepy","title":"_deduplicate.py","text":""},{"location":"BioCypher/reference/source/#biocypher._deduplicate.Deduplicator","title":"<code>Deduplicator</code>","text":"<p>Singleton class responsible of deduplicating BioCypher inputs. Maintains sets/dictionaries of node and edge types and their unique identifiers.</p> <p>Nodes identifiers should be globally unique (represented as a set), while edge identifiers are only unique per edge type (represented as a dict of sets, keyed by edge type).</p> <p>Stores collection of duplicate node and edge identifiers and types for troubleshooting and to avoid overloading the log.</p> Source code in <code>biocypher/biocypher/_deduplicate.py</code> <pre><code>class Deduplicator:\n    \"\"\"\n    Singleton class responsible of deduplicating BioCypher inputs. Maintains\n    sets/dictionaries of node and edge types and their unique identifiers.\n\n    Nodes identifiers should be globally unique (represented as a set), while\n    edge identifiers are only unique per edge type (represented as a dict of\n    sets, keyed by edge type).\n\n    Stores collection of duplicate node and edge identifiers and types for\n    troubleshooting and to avoid overloading the log.\n    \"\"\"\n\n    def __init__(self):\n        self.seen_entity_ids = set()\n        self.duplicate_entity_ids = set()\n\n        self.entity_types = set()\n        self.duplicate_entity_types = set()\n\n        self.seen_relationships = {}\n        self.duplicate_relationship_ids = set()\n        self.duplicate_relationship_types = set()\n\n    def node_seen(self, entity: BioCypherNode) -&gt; bool:\n        \"\"\"\n        Adds a node to the instance and checks if it has been seen before.\n\n        Args:\n            node: BioCypherNode to be added.\n\n        Returns:\n            True if the node has been seen before, False otherwise.\n        \"\"\"\n        if entity.get_label() not in self.entity_types:\n            self.entity_types.add(entity.get_label())\n\n        if entity.get_id() in self.seen_entity_ids:\n            self.duplicate_entity_ids.add(entity.get_id())\n            if entity.get_label() not in self.duplicate_entity_types:\n                logger.warning(f\"Duplicate node type {entity.get_label()} found. \")\n                self.duplicate_entity_types.add(entity.get_label())\n            return True\n\n        self.seen_entity_ids.add(entity.get_id())\n        return False\n\n    def edge_seen(self, relationship: BioCypherEdge) -&gt; bool:\n        \"\"\"\n        Adds an edge to the instance and checks if it has been seen before.\n\n        Args:\n            edge: BioCypherEdge to be added.\n\n        Returns:\n            True if the edge has been seen before, False otherwise.\n        \"\"\"\n        if relationship.get_type() not in self.seen_relationships:\n            self.seen_relationships[relationship.get_type()] = set()\n\n        # concatenate source and target if no id is present\n        if not relationship.get_id():\n            _id = f\"{relationship.get_source_id()}_{relationship.get_target_id()}\"\n        else:\n            _id = relationship.get_id()\n\n        if _id in self.seen_relationships[relationship.get_type()]:\n            self.duplicate_relationship_ids.add(_id)\n            if relationship.get_type() not in self.duplicate_relationship_types:\n                logger.warning(f\"Duplicate edge type {relationship.get_type()} found. \")\n                self.duplicate_relationship_types.add(relationship.get_type())\n            return True\n\n        self.seen_relationships[relationship.get_type()].add(_id)\n        return False\n\n    def rel_as_node_seen(self, rel_as_node: BioCypherRelAsNode) -&gt; bool:\n        \"\"\"\n        Adds a rel_as_node to the instance (one entity and two relationships)\n        and checks if it has been seen before. Only the node is relevant for\n        identifying the rel_as_node as a duplicate.\n\n        Args:\n            rel_as_node: BioCypherRelAsNode to be added.\n\n        Returns:\n            True if the rel_as_node has been seen before, False otherwise.\n        \"\"\"\n        node = rel_as_node.get_node()\n\n        if node.get_label() not in self.seen_relationships:\n            self.seen_relationships[node.get_label()] = set()\n\n        # rel as node always has an id\n        _id = node.get_id()\n\n        if _id in self.seen_relationships[node.get_type()]:\n            self.duplicate_relationship_ids.add(_id)\n            if node.get_type() not in self.duplicate_relationship_types:\n                logger.warning(f\"Duplicate edge type {node.get_type()} found. \")\n                self.duplicate_relationship_types.add(node.get_type())\n            return True\n\n        self.seen_relationships[node.get_type()].add(_id)\n        return False\n\n    def get_duplicate_nodes(self):\n        \"\"\"\n        Function to return a list of duplicate nodes.\n\n        Returns:\n            list: list of duplicate nodes\n        \"\"\"\n\n        if self.duplicate_entity_types:\n            return (self.duplicate_entity_types, self.duplicate_entity_ids)\n        else:\n            return None\n\n    def get_duplicate_edges(self):\n        \"\"\"\n        Function to return a list of duplicate edges.\n\n        Returns:\n            list: list of duplicate edges\n        \"\"\"\n\n        if self.duplicate_relationship_types:\n            return (\n                self.duplicate_relationship_types,\n                self.duplicate_relationship_ids,\n            )\n        else:\n            return None\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._deduplicate.Deduplicator.edge_seen","title":"<code>edge_seen(relationship)</code>","text":"<p>Adds an edge to the instance and checks if it has been seen before.</p> <p>Parameters:</p> Name Type Description Default <code>edge</code> <p>BioCypherEdge to be added.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the edge has been seen before, False otherwise.</p> Source code in <code>biocypher/biocypher/_deduplicate.py</code> <pre><code>def edge_seen(self, relationship: BioCypherEdge) -&gt; bool:\n    \"\"\"\n    Adds an edge to the instance and checks if it has been seen before.\n\n    Args:\n        edge: BioCypherEdge to be added.\n\n    Returns:\n        True if the edge has been seen before, False otherwise.\n    \"\"\"\n    if relationship.get_type() not in self.seen_relationships:\n        self.seen_relationships[relationship.get_type()] = set()\n\n    # concatenate source and target if no id is present\n    if not relationship.get_id():\n        _id = f\"{relationship.get_source_id()}_{relationship.get_target_id()}\"\n    else:\n        _id = relationship.get_id()\n\n    if _id in self.seen_relationships[relationship.get_type()]:\n        self.duplicate_relationship_ids.add(_id)\n        if relationship.get_type() not in self.duplicate_relationship_types:\n            logger.warning(f\"Duplicate edge type {relationship.get_type()} found. \")\n            self.duplicate_relationship_types.add(relationship.get_type())\n        return True\n\n    self.seen_relationships[relationship.get_type()].add(_id)\n    return False\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._deduplicate.Deduplicator.get_duplicate_edges","title":"<code>get_duplicate_edges()</code>","text":"<p>Function to return a list of duplicate edges.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>list of duplicate edges</p> Source code in <code>biocypher/biocypher/_deduplicate.py</code> <pre><code>def get_duplicate_edges(self):\n    \"\"\"\n    Function to return a list of duplicate edges.\n\n    Returns:\n        list: list of duplicate edges\n    \"\"\"\n\n    if self.duplicate_relationship_types:\n        return (\n            self.duplicate_relationship_types,\n            self.duplicate_relationship_ids,\n        )\n    else:\n        return None\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._deduplicate.Deduplicator.get_duplicate_nodes","title":"<code>get_duplicate_nodes()</code>","text":"<p>Function to return a list of duplicate nodes.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>list of duplicate nodes</p> Source code in <code>biocypher/biocypher/_deduplicate.py</code> <pre><code>def get_duplicate_nodes(self):\n    \"\"\"\n    Function to return a list of duplicate nodes.\n\n    Returns:\n        list: list of duplicate nodes\n    \"\"\"\n\n    if self.duplicate_entity_types:\n        return (self.duplicate_entity_types, self.duplicate_entity_ids)\n    else:\n        return None\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._deduplicate.Deduplicator.node_seen","title":"<code>node_seen(entity)</code>","text":"<p>Adds a node to the instance and checks if it has been seen before.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>BioCypherNode to be added.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node has been seen before, False otherwise.</p> Source code in <code>biocypher/biocypher/_deduplicate.py</code> <pre><code>def node_seen(self, entity: BioCypherNode) -&gt; bool:\n    \"\"\"\n    Adds a node to the instance and checks if it has been seen before.\n\n    Args:\n        node: BioCypherNode to be added.\n\n    Returns:\n        True if the node has been seen before, False otherwise.\n    \"\"\"\n    if entity.get_label() not in self.entity_types:\n        self.entity_types.add(entity.get_label())\n\n    if entity.get_id() in self.seen_entity_ids:\n        self.duplicate_entity_ids.add(entity.get_id())\n        if entity.get_label() not in self.duplicate_entity_types:\n            logger.warning(f\"Duplicate node type {entity.get_label()} found. \")\n            self.duplicate_entity_types.add(entity.get_label())\n        return True\n\n    self.seen_entity_ids.add(entity.get_id())\n    return False\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._deduplicate.Deduplicator.rel_as_node_seen","title":"<code>rel_as_node_seen(rel_as_node)</code>","text":"<p>Adds a rel_as_node to the instance (one entity and two relationships) and checks if it has been seen before. Only the node is relevant for identifying the rel_as_node as a duplicate.</p> <p>Parameters:</p> Name Type Description Default <code>rel_as_node</code> <code>BioCypherRelAsNode</code> <p>BioCypherRelAsNode to be added.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the rel_as_node has been seen before, False otherwise.</p> Source code in <code>biocypher/biocypher/_deduplicate.py</code> <pre><code>def rel_as_node_seen(self, rel_as_node: BioCypherRelAsNode) -&gt; bool:\n    \"\"\"\n    Adds a rel_as_node to the instance (one entity and two relationships)\n    and checks if it has been seen before. Only the node is relevant for\n    identifying the rel_as_node as a duplicate.\n\n    Args:\n        rel_as_node: BioCypherRelAsNode to be added.\n\n    Returns:\n        True if the rel_as_node has been seen before, False otherwise.\n    \"\"\"\n    node = rel_as_node.get_node()\n\n    if node.get_label() not in self.seen_relationships:\n        self.seen_relationships[node.get_label()] = set()\n\n    # rel as node always has an id\n    _id = node.get_id()\n\n    if _id in self.seen_relationships[node.get_type()]:\n        self.duplicate_relationship_ids.add(_id)\n        if node.get_type() not in self.duplicate_relationship_types:\n            logger.warning(f\"Duplicate edge type {node.get_type()} found. \")\n            self.duplicate_relationship_types.add(node.get_type())\n        return True\n\n    self.seen_relationships[node.get_type()].add(_id)\n    return False\n</code></pre>"},{"location":"BioCypher/reference/source/#_getpy","title":"_get.py","text":"<p>BioCypher get module. Used to download and cache data from external sources.</p>"},{"location":"BioCypher/reference/source/#biocypher._get.APIRequest","title":"<code>APIRequest</code>","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>class APIRequest(Resource):\n    def __init__(self, name: str, url_s: str | list[str], lifetime: int = 0):\n        \"\"\"\n        Represents basic information for an API Request.\n\n        Args:\n            name(str): The name of the API Request.\n\n            url_s(str|list): The URL of the API endpoint.\n\n            lifetime(int): The lifetime of the API Request in days. If 0, the\n                API Request is cached indefinitely.\n\n        \"\"\"\n        super().__init__(name, url_s, lifetime)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.APIRequest.__init__","title":"<code>__init__(name, url_s, lifetime=0)</code>","text":"<p>Represents basic information for an API Request.</p> <p>Parameters:</p> Name Type Description Default <code>name(str)</code> <p>The name of the API Request.</p> required <code>url_s(str|list)</code> <p>The URL of the API endpoint.</p> required <code>lifetime(int)</code> <p>The lifetime of the API Request in days. If 0, the API Request is cached indefinitely.</p> required Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def __init__(self, name: str, url_s: str | list[str], lifetime: int = 0):\n    \"\"\"\n    Represents basic information for an API Request.\n\n    Args:\n        name(str): The name of the API Request.\n\n        url_s(str|list): The URL of the API endpoint.\n\n        lifetime(int): The lifetime of the API Request in days. If 0, the\n            API Request is cached indefinitely.\n\n    \"\"\"\n    super().__init__(name, url_s, lifetime)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader","title":"<code>Downloader</code>","text":"Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>class Downloader:\n    def __init__(self, cache_dir: Optional[str] = None) -&gt; None:\n        \"\"\"\n        The Downloader is a class that manages resources that can be downloaded\n        and cached locally. It manages the lifetime of downloaded resources by\n        keeping a JSON record of the download date of each resource.\n\n        Args:\n            cache_dir (str): The directory where the resources are cached. If\n                not given, a temporary directory is created.\n        \"\"\"\n        self.cache_dir = cache_dir or TemporaryDirectory().name\n        self.cache_file = os.path.join(self.cache_dir, \"cache.json\")\n        self.cache_dict = self._load_cache_dict()\n\n    def download(self, *resources: Resource):\n        \"\"\"\n        Download one or multiple resources. Load from cache if the resource is\n        already downloaded and the cache is not expired.\n\n        Args:\n            resources (Resource): The resource(s) to download or load from\n                cache.\n\n        Returns:\n            list[str]: The path or paths to the resource(s) that were downloaded\n                or loaded from cache.\n\n        \"\"\"\n        paths = []\n        for resource in resources:\n            paths.append(self._download_or_cache(resource))\n\n        # flatten list if it is nested\n        if is_nested(paths):\n            paths = [path for sublist in paths for path in sublist]\n\n        return paths\n\n    def _download_or_cache(self, resource: Resource, cache: bool = True):\n        \"\"\"\n        Download a resource if it is not cached or exceeded its lifetime.\n\n        Args:\n            resource (Resource): The resource to download.\n        Returns:\n            list[str]: The path or paths to the downloaded resource(s).\n\n        \"\"\"\n        expired = self._is_cache_expired(resource)\n\n        if expired or not cache:\n            self._delete_expired_cache(resource)\n            if isinstance(resource, FileDownload):\n                logger.info(f\"Asking for download of resource {resource.name}.\")\n                paths = self._download_files(cache, resource)\n            elif isinstance(resource, APIRequest):\n                logger.info(f\"Asking for download of api request {resource.name}.\")\n                paths = self._download_api_request(resource)\n            else:\n                raise TypeError(f\"Unknown resource type: {type(resource)}\")\n        else:\n            paths = self.get_cached_version(resource)\n        self._update_cache_record(resource)\n        return paths\n\n    def _is_cache_expired(self, resource: Resource) -&gt; bool:\n        \"\"\"\n        Check if resource or API request cache is expired.\n\n        Args:\n            resource (Resource): The resource to download.\n\n        Returns:\n            bool: cache is expired or not.\n        \"\"\"\n        cache_record = self._get_cache_record(resource)\n        if cache_record:\n            download_time = datetime.strptime(cache_record.get(\"date_downloaded\"), \"%Y-%m-%d %H:%M:%S.%f\")\n            lifetime = timedelta(days=resource.lifetime)\n            expired = download_time + lifetime &lt; datetime.now()\n        else:\n            expired = True\n        return expired\n\n    def _delete_expired_cache(self, resource: Resource):\n        cache_resource_path = self.cache_dir + \"/\" + resource.name\n        if os.path.exists(cache_resource_path) and os.path.isdir(cache_resource_path):\n            shutil.rmtree(cache_resource_path)\n\n    def _download_files(self, cache, file_download: FileDownload):\n        \"\"\"\n        Download a resource given it is a file or a directory and return the\n        path.\n\n        Args:\n            cache (bool): Whether to cache the resource or not.\n            file_download (FileDownload): The resource to download.\n\n        Returns:\n            list[str]: The path or paths to the downloaded resource(s).\n        \"\"\"\n        if file_download.is_dir:\n            files = self._get_files(file_download)\n            file_download.url_s = [file_download.url_s + \"/\" + file for file in files]\n            file_download.is_dir = False\n            paths = self._download_or_cache(file_download, cache)\n        elif isinstance(file_download.url_s, list):\n            paths = []\n            for url in file_download.url_s:\n                fname = url[url.rfind(\"/\") + 1 :].split(\"?\")[0]\n                path = self._retrieve(\n                    url=url,\n                    fname=fname,\n                    path=os.path.join(self.cache_dir, file_download.name),\n                )\n                paths.append(path)\n        else:\n            paths = []\n            fname = file_download.url_s[file_download.url_s.rfind(\"/\") + 1 :].split(\"?\")[0]\n            results = self._retrieve(\n                url=file_download.url_s,\n                fname=fname,\n                path=os.path.join(self.cache_dir, file_download.name),\n            )\n            if isinstance(results, list):\n                paths.extend(results)\n            else:\n                paths.append(results)\n\n        # sometimes a compressed file contains multiple files\n        # TODO ask for a list of files in the archive to be used from the\n        # adapter\n        return paths\n\n    def _download_api_request(self, api_request: APIRequest):\n        \"\"\"\n        Download an API request and return the path.\n\n        Args:\n            api_request(APIRequest): The API request result that is being cached.\n        Returns:\n            list[str]: The path to the cached API request.\n\n        \"\"\"\n        urls = api_request.url_s if isinstance(api_request.url_s, list) else [api_request.url_s]\n        paths = []\n        for url in urls:\n            fname = url[url.rfind(\"/\") + 1 :].rsplit(\".\", 1)[0]\n            logger.info(f\"Asking for caching API of {api_request.name} {fname}.\")\n            response = requests.get(url=url)\n\n            if response.status_code != 200:\n                response.raise_for_status()\n            response_data = response.json()\n            api_path = os.path.join(self.cache_dir, api_request.name, f\"{fname}.json\")\n\n            os.makedirs(os.path.dirname(api_path), exist_ok=True)\n            with open(api_path, \"w\") as f:\n                json.dump(response_data, f)\n                logger.info(f\"Caching API request to {api_path}.\")\n            paths.append(api_path)\n        return paths\n\n    def get_cached_version(self, resource: Resource) -&gt; list[str]:\n        \"\"\"Get the cached version of a resource.\n\n        Args:\n            resource(Resource): The resource to get the cached version of.\n\n        Returns:\n            list[str]: The paths to the cached resource(s).\n        \"\"\"\n        cached_location = os.path.join(self.cache_dir, resource.name)\n        logger.info(f\"Use cached version from {cached_location}.\")\n        paths = []\n        for file in os.listdir(cached_location):\n            paths.append(os.path.join(cached_location, file))\n        return paths\n\n    def _retrieve(\n        self,\n        url: str,\n        fname: str,\n        path: str,\n        known_hash: str = None,\n    ):\n        \"\"\"\n        Retrieve a file from a URL using Pooch. Infer type of file from\n        extension and use appropriate processor.\n\n        Args:\n            url (str): The URL to retrieve the file from.\n\n            fname (str): The name of the file.\n\n            path (str): The path to the file.\n        \"\"\"\n        if fname.endswith(\".zip\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Unzip(),\n                progressbar=True,\n            )\n\n        elif fname.endswith(\".tar.gz\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Untar(),\n                progressbar=True,\n            )\n\n        elif fname.endswith(\".gz\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Decompress(),\n                progressbar=True,\n            )\n\n        else:\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                progressbar=True,\n            )\n\n    def _get_files(self, file_download: FileDownload):\n        \"\"\"\n        Get the files contained in a directory file.\n\n        Args:\n            file_download (FileDownload): The directory file.\n\n        Returns:\n            list: The files contained in the directory.\n        \"\"\"\n        if file_download.url_s.startswith(\"ftp://\"):\n            # remove protocol\n            url = file_download.url_s[6:]\n            # get base url\n            url = url[: url.find(\"/\")]\n            # get directory (remove initial slash as well)\n            dir = file_download.url_s[7 + len(url) :]\n            # get files\n            ftp = ftplib.FTP(url)\n            ftp.login()\n            ftp.cwd(dir)\n            files = ftp.nlst()\n            ftp.quit()\n        else:\n            raise NotImplementedError(\"Only FTP directories are supported at the moment.\")\n\n        return files\n\n    def _load_cache_dict(self):\n        \"\"\"\n        Load the cache dictionary from the cache file. Create an empty cache\n        file if it does not exist.\n        \"\"\"\n        if not os.path.exists(self.cache_dir):\n            logger.info(f\"Creating cache directory {self.cache_dir}.\")\n            os.makedirs(self.cache_dir)\n\n        if not os.path.exists(self.cache_file):\n            logger.info(f\"Creating cache file {self.cache_file}.\")\n            with open(self.cache_file, \"w\") as f:\n                json.dump({}, f)\n\n        with open(self.cache_file, \"r\") as f:\n            logger.info(f\"Loading cache file {self.cache_file}.\")\n            return json.load(f)\n\n    def _get_cache_record(self, resource: Resource):\n        \"\"\"\n        Get the cache record of a resource.\n\n        Args:\n            resource (Resource): The resource to get the cache record of.\n\n        Returns:\n            The cache record of the resource.\n        \"\"\"\n        return self.cache_dict.get(resource.name, {})\n\n    def _update_cache_record(self, resource: Resource):\n        \"\"\"\n        Update the cache record of a resource.\n\n        Args:\n            resource (Resource): The resource to update the cache record of.\n        \"\"\"\n        cache_record = {}\n        cache_record[\"url\"] = to_list(resource.url_s)\n        cache_record[\"date_downloaded\"] = str(datetime.now())\n        cache_record[\"lifetime\"] = resource.lifetime\n        self.cache_dict[resource.name] = cache_record\n        with open(self.cache_file, \"w\") as f:\n            json.dump(self.cache_dict, f, default=str)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader.__init__","title":"<code>__init__(cache_dir=None)</code>","text":"<p>The Downloader is a class that manages resources that can be downloaded and cached locally. It manages the lifetime of downloaded resources by keeping a JSON record of the download date of each resource.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>str</code> <p>The directory where the resources are cached. If not given, a temporary directory is created.</p> <code>None</code> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def __init__(self, cache_dir: Optional[str] = None) -&gt; None:\n    \"\"\"\n    The Downloader is a class that manages resources that can be downloaded\n    and cached locally. It manages the lifetime of downloaded resources by\n    keeping a JSON record of the download date of each resource.\n\n    Args:\n        cache_dir (str): The directory where the resources are cached. If\n            not given, a temporary directory is created.\n    \"\"\"\n    self.cache_dir = cache_dir or TemporaryDirectory().name\n    self.cache_file = os.path.join(self.cache_dir, \"cache.json\")\n    self.cache_dict = self._load_cache_dict()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._download_api_request","title":"<code>_download_api_request(api_request)</code>","text":"<p>Download an API request and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>api_request(APIRequest)</code> <p>The API request result that is being cached.</p> required <p>Returns:     list[str]: The path to the cached API request.</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _download_api_request(self, api_request: APIRequest):\n    \"\"\"\n    Download an API request and return the path.\n\n    Args:\n        api_request(APIRequest): The API request result that is being cached.\n    Returns:\n        list[str]: The path to the cached API request.\n\n    \"\"\"\n    urls = api_request.url_s if isinstance(api_request.url_s, list) else [api_request.url_s]\n    paths = []\n    for url in urls:\n        fname = url[url.rfind(\"/\") + 1 :].rsplit(\".\", 1)[0]\n        logger.info(f\"Asking for caching API of {api_request.name} {fname}.\")\n        response = requests.get(url=url)\n\n        if response.status_code != 200:\n            response.raise_for_status()\n        response_data = response.json()\n        api_path = os.path.join(self.cache_dir, api_request.name, f\"{fname}.json\")\n\n        os.makedirs(os.path.dirname(api_path), exist_ok=True)\n        with open(api_path, \"w\") as f:\n            json.dump(response_data, f)\n            logger.info(f\"Caching API request to {api_path}.\")\n        paths.append(api_path)\n    return paths\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._download_files","title":"<code>_download_files(cache, file_download)</code>","text":"<p>Download a resource given it is a file or a directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>bool</code> <p>Whether to cache the resource or not.</p> required <code>file_download</code> <code>FileDownload</code> <p>The resource to download.</p> required <p>Returns:</p> Type Description <p>list[str]: The path or paths to the downloaded resource(s).</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _download_files(self, cache, file_download: FileDownload):\n    \"\"\"\n    Download a resource given it is a file or a directory and return the\n    path.\n\n    Args:\n        cache (bool): Whether to cache the resource or not.\n        file_download (FileDownload): The resource to download.\n\n    Returns:\n        list[str]: The path or paths to the downloaded resource(s).\n    \"\"\"\n    if file_download.is_dir:\n        files = self._get_files(file_download)\n        file_download.url_s = [file_download.url_s + \"/\" + file for file in files]\n        file_download.is_dir = False\n        paths = self._download_or_cache(file_download, cache)\n    elif isinstance(file_download.url_s, list):\n        paths = []\n        for url in file_download.url_s:\n            fname = url[url.rfind(\"/\") + 1 :].split(\"?\")[0]\n            path = self._retrieve(\n                url=url,\n                fname=fname,\n                path=os.path.join(self.cache_dir, file_download.name),\n            )\n            paths.append(path)\n    else:\n        paths = []\n        fname = file_download.url_s[file_download.url_s.rfind(\"/\") + 1 :].split(\"?\")[0]\n        results = self._retrieve(\n            url=file_download.url_s,\n            fname=fname,\n            path=os.path.join(self.cache_dir, file_download.name),\n        )\n        if isinstance(results, list):\n            paths.extend(results)\n        else:\n            paths.append(results)\n\n    # sometimes a compressed file contains multiple files\n    # TODO ask for a list of files in the archive to be used from the\n    # adapter\n    return paths\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._download_or_cache","title":"<code>_download_or_cache(resource, cache=True)</code>","text":"<p>Download a resource if it is not cached or exceeded its lifetime.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to download.</p> required <p>Returns:     list[str]: The path or paths to the downloaded resource(s).</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _download_or_cache(self, resource: Resource, cache: bool = True):\n    \"\"\"\n    Download a resource if it is not cached or exceeded its lifetime.\n\n    Args:\n        resource (Resource): The resource to download.\n    Returns:\n        list[str]: The path or paths to the downloaded resource(s).\n\n    \"\"\"\n    expired = self._is_cache_expired(resource)\n\n    if expired or not cache:\n        self._delete_expired_cache(resource)\n        if isinstance(resource, FileDownload):\n            logger.info(f\"Asking for download of resource {resource.name}.\")\n            paths = self._download_files(cache, resource)\n        elif isinstance(resource, APIRequest):\n            logger.info(f\"Asking for download of api request {resource.name}.\")\n            paths = self._download_api_request(resource)\n        else:\n            raise TypeError(f\"Unknown resource type: {type(resource)}\")\n    else:\n        paths = self.get_cached_version(resource)\n    self._update_cache_record(resource)\n    return paths\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._get_cache_record","title":"<code>_get_cache_record(resource)</code>","text":"<p>Get the cache record of a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to get the cache record of.</p> required <p>Returns:</p> Type Description <p>The cache record of the resource.</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _get_cache_record(self, resource: Resource):\n    \"\"\"\n    Get the cache record of a resource.\n\n    Args:\n        resource (Resource): The resource to get the cache record of.\n\n    Returns:\n        The cache record of the resource.\n    \"\"\"\n    return self.cache_dict.get(resource.name, {})\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._get_files","title":"<code>_get_files(file_download)</code>","text":"<p>Get the files contained in a directory file.</p> <p>Parameters:</p> Name Type Description Default <code>file_download</code> <code>FileDownload</code> <p>The directory file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The files contained in the directory.</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _get_files(self, file_download: FileDownload):\n    \"\"\"\n    Get the files contained in a directory file.\n\n    Args:\n        file_download (FileDownload): The directory file.\n\n    Returns:\n        list: The files contained in the directory.\n    \"\"\"\n    if file_download.url_s.startswith(\"ftp://\"):\n        # remove protocol\n        url = file_download.url_s[6:]\n        # get base url\n        url = url[: url.find(\"/\")]\n        # get directory (remove initial slash as well)\n        dir = file_download.url_s[7 + len(url) :]\n        # get files\n        ftp = ftplib.FTP(url)\n        ftp.login()\n        ftp.cwd(dir)\n        files = ftp.nlst()\n        ftp.quit()\n    else:\n        raise NotImplementedError(\"Only FTP directories are supported at the moment.\")\n\n    return files\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._is_cache_expired","title":"<code>_is_cache_expired(resource)</code>","text":"<p>Check if resource or API request cache is expired.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to download.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>cache is expired or not.</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _is_cache_expired(self, resource: Resource) -&gt; bool:\n    \"\"\"\n    Check if resource or API request cache is expired.\n\n    Args:\n        resource (Resource): The resource to download.\n\n    Returns:\n        bool: cache is expired or not.\n    \"\"\"\n    cache_record = self._get_cache_record(resource)\n    if cache_record:\n        download_time = datetime.strptime(cache_record.get(\"date_downloaded\"), \"%Y-%m-%d %H:%M:%S.%f\")\n        lifetime = timedelta(days=resource.lifetime)\n        expired = download_time + lifetime &lt; datetime.now()\n    else:\n        expired = True\n    return expired\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._load_cache_dict","title":"<code>_load_cache_dict()</code>","text":"<p>Load the cache dictionary from the cache file. Create an empty cache file if it does not exist.</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _load_cache_dict(self):\n    \"\"\"\n    Load the cache dictionary from the cache file. Create an empty cache\n    file if it does not exist.\n    \"\"\"\n    if not os.path.exists(self.cache_dir):\n        logger.info(f\"Creating cache directory {self.cache_dir}.\")\n        os.makedirs(self.cache_dir)\n\n    if not os.path.exists(self.cache_file):\n        logger.info(f\"Creating cache file {self.cache_file}.\")\n        with open(self.cache_file, \"w\") as f:\n            json.dump({}, f)\n\n    with open(self.cache_file, \"r\") as f:\n        logger.info(f\"Loading cache file {self.cache_file}.\")\n        return json.load(f)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._retrieve","title":"<code>_retrieve(url, fname, path, known_hash=None)</code>","text":"<p>Retrieve a file from a URL using Pooch. Infer type of file from extension and use appropriate processor.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to retrieve the file from.</p> required <code>fname</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to the file.</p> required Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _retrieve(\n    self,\n    url: str,\n    fname: str,\n    path: str,\n    known_hash: str = None,\n):\n    \"\"\"\n    Retrieve a file from a URL using Pooch. Infer type of file from\n    extension and use appropriate processor.\n\n    Args:\n        url (str): The URL to retrieve the file from.\n\n        fname (str): The name of the file.\n\n        path (str): The path to the file.\n    \"\"\"\n    if fname.endswith(\".zip\"):\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            processor=pooch.Unzip(),\n            progressbar=True,\n        )\n\n    elif fname.endswith(\".tar.gz\"):\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            processor=pooch.Untar(),\n            progressbar=True,\n        )\n\n    elif fname.endswith(\".gz\"):\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            processor=pooch.Decompress(),\n            progressbar=True,\n        )\n\n    else:\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            progressbar=True,\n        )\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader._update_cache_record","title":"<code>_update_cache_record(resource)</code>","text":"<p>Update the cache record of a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to update the cache record of.</p> required Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def _update_cache_record(self, resource: Resource):\n    \"\"\"\n    Update the cache record of a resource.\n\n    Args:\n        resource (Resource): The resource to update the cache record of.\n    \"\"\"\n    cache_record = {}\n    cache_record[\"url\"] = to_list(resource.url_s)\n    cache_record[\"date_downloaded\"] = str(datetime.now())\n    cache_record[\"lifetime\"] = resource.lifetime\n    self.cache_dict[resource.name] = cache_record\n    with open(self.cache_file, \"w\") as f:\n        json.dump(self.cache_dict, f, default=str)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader.download","title":"<code>download(*resources)</code>","text":"<p>Download one or multiple resources. Load from cache if the resource is already downloaded and the cache is not expired.</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>Resource</code> <p>The resource(s) to download or load from cache.</p> <code>()</code> <p>Returns:</p> Type Description <p>list[str]: The path or paths to the resource(s) that were downloaded or loaded from cache.</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def download(self, *resources: Resource):\n    \"\"\"\n    Download one or multiple resources. Load from cache if the resource is\n    already downloaded and the cache is not expired.\n\n    Args:\n        resources (Resource): The resource(s) to download or load from\n            cache.\n\n    Returns:\n        list[str]: The path or paths to the resource(s) that were downloaded\n            or loaded from cache.\n\n    \"\"\"\n    paths = []\n    for resource in resources:\n        paths.append(self._download_or_cache(resource))\n\n    # flatten list if it is nested\n    if is_nested(paths):\n        paths = [path for sublist in paths for path in sublist]\n\n    return paths\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Downloader.get_cached_version","title":"<code>get_cached_version(resource)</code>","text":"<p>Get the cached version of a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource(Resource)</code> <p>The resource to get the cached version of.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The paths to the cached resource(s).</p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def get_cached_version(self, resource: Resource) -&gt; list[str]:\n    \"\"\"Get the cached version of a resource.\n\n    Args:\n        resource(Resource): The resource to get the cached version of.\n\n    Returns:\n        list[str]: The paths to the cached resource(s).\n    \"\"\"\n    cached_location = os.path.join(self.cache_dir, resource.name)\n    logger.info(f\"Use cached version from {cached_location}.\")\n    paths = []\n    for file in os.listdir(cached_location):\n        paths.append(os.path.join(cached_location, file))\n    return paths\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.FileDownload","title":"<code>FileDownload</code>","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>class FileDownload(Resource):\n    def __init__(\n        self,\n        name: str,\n        url_s: str | list[str],\n        lifetime: int = 0,\n        is_dir: bool = False,\n    ):\n        \"\"\"\n        Represents basic information for a File Download.\n\n        Args:\n            name(str): The name of the File Download.\n\n            url_s(str|list[str]): The URL(s) of the File Download.\n\n            lifetime(int): The lifetime of the File Download in days. If 0, the\n                File Download is cached indefinitely.\n\n            is_dir (bool): Whether the URL points to a directory or not.\n        \"\"\"\n\n        super().__init__(name, url_s, lifetime)\n        self.is_dir = is_dir\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.FileDownload.__init__","title":"<code>__init__(name, url_s, lifetime=0, is_dir=False)</code>","text":"<p>Represents basic information for a File Download.</p> <p>Parameters:</p> Name Type Description Default <code>name(str)</code> <p>The name of the File Download.</p> required <code>url_s(str|list[str])</code> <p>The URL(s) of the File Download.</p> required <code>lifetime(int)</code> <p>The lifetime of the File Download in days. If 0, the File Download is cached indefinitely.</p> required <code>is_dir</code> <code>bool</code> <p>Whether the URL points to a directory or not.</p> <code>False</code> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    url_s: str | list[str],\n    lifetime: int = 0,\n    is_dir: bool = False,\n):\n    \"\"\"\n    Represents basic information for a File Download.\n\n    Args:\n        name(str): The name of the File Download.\n\n        url_s(str|list[str]): The URL(s) of the File Download.\n\n        lifetime(int): The lifetime of the File Download in days. If 0, the\n            File Download is cached indefinitely.\n\n        is_dir (bool): Whether the URL points to a directory or not.\n    \"\"\"\n\n    super().__init__(name, url_s, lifetime)\n    self.is_dir = is_dir\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>class Resource(ABC):\n    def __init__(\n        self,\n        name: str,\n        url_s: str | list[str],\n        lifetime: int = 0,\n    ):\n        \"\"\"\n        A Resource is a file, a list of files, an API request, or a list of API\n        requests, any of which can be downloaded from the given URL(s) and\n        cached locally. This class implements checks of the minimum requirements\n        for a resource, to be implemented by a biocypher adapter.\n\n        Args:\n            name (str): The name of the resource.\n\n            url_s (str | list[str]): The URL or URLs of the resource.\n\n            lifetime (int): The lifetime of the resource in days. If 0, the\n                resource is considered to be permanent.\n        \"\"\"\n        self.name = name\n        self.url_s = url_s\n        self.lifetime = lifetime\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._get.Resource.__init__","title":"<code>__init__(name, url_s, lifetime=0)</code>","text":"<p>A Resource is a file, a list of files, an API request, or a list of API requests, any of which can be downloaded from the given URL(s) and cached locally. This class implements checks of the minimum requirements for a resource, to be implemented by a biocypher adapter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the resource.</p> required <code>url_s</code> <code>str | list[str]</code> <p>The URL or URLs of the resource.</p> required <code>lifetime</code> <code>int</code> <p>The lifetime of the resource in days. If 0, the resource is considered to be permanent.</p> <code>0</code> Source code in <code>biocypher/biocypher/_get.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    url_s: str | list[str],\n    lifetime: int = 0,\n):\n    \"\"\"\n    A Resource is a file, a list of files, an API request, or a list of API\n    requests, any of which can be downloaded from the given URL(s) and\n    cached locally. This class implements checks of the minimum requirements\n    for a resource, to be implemented by a biocypher adapter.\n\n    Args:\n        name (str): The name of the resource.\n\n        url_s (str | list[str]): The URL or URLs of the resource.\n\n        lifetime (int): The lifetime of the resource in days. If 0, the\n            resource is considered to be permanent.\n    \"\"\"\n    self.name = name\n    self.url_s = url_s\n    self.lifetime = lifetime\n</code></pre>"},{"location":"BioCypher/reference/source/#_loggerpy","title":"_.logger.py","text":"<p>Configuration of the module logger.</p>"},{"location":"BioCypher/reference/source/#biocypher._logger.get_logger","title":"<code>get_logger(name='biocypher')</code>","text":"<p>Access the module logger, create a new one if does not exist yet.</p> <p>Method providing central logger instance to main module. Is called only from main submodule, :mod:<code>biocypher.driver</code>. In child modules, the standard Python logging facility is called (using <code>logging.getLogger(__name__)</code>), automatically inheriting the handlers from the central logger.</p> <p>The file handler creates a log file named after the current date and time. Levels to output to file and console can be set here.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger instance.</p> <code>'biocypher'</code> <p>Returns:</p> Type Description <code>Logger</code> <p>An instance of the Python mod:<code>logging.Logger</code>.</p> Source code in <code>biocypher/biocypher/_logger.py</code> <pre><code>def get_logger(name: str = \"biocypher\") -&gt; logging.Logger:\n    \"\"\"\n    Access the module logger, create a new one if does not exist yet.\n\n    Method providing central logger instance to main module. Is called\n    only from main submodule, :mod:`biocypher.driver`. In child modules,\n    the standard Python logging facility is called\n    (using ``logging.getLogger(__name__)``), automatically inheriting\n    the handlers from the central logger.\n\n    The file handler creates a log file named after the current date and\n    time. Levels to output to file and console can be set here.\n\n    Args:\n        name:\n            Name of the logger instance.\n\n    Returns:\n        An instance of the Python :py:mod:`logging.Logger`.\n    \"\"\"\n\n    if not logging.getLogger(name).hasHandlers():\n        # create logger\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.DEBUG)\n        logger.propagate = True\n\n        # formatting\n        file_formatter = logging.Formatter(\n            \"%(asctime)s\\t%(levelname)s\\tmodule:%(module)s\\n%(message)s\",\n        )\n        stdout_formatter = logging.Formatter(\"%(levelname)s -- %(message)s\")\n\n        # file name and creation\n        now = datetime.now()\n        date_time = now.strftime(\"%Y%m%d-%H%M%S\")\n\n        log_to_disk = _config.config(\"biocypher\").get(\"log_to_disk\")\n\n        if log_to_disk:\n            logdir = _config.config(\"biocypher\").get(\"log_directory\") or \"biocypher-log\"\n            os.makedirs(logdir, exist_ok=True)\n            logfile = os.path.join(logdir, f\"biocypher-{date_time}.log\")\n\n            # file handler\n            file_handler = logging.FileHandler(logfile)\n\n            if _config.config(\"biocypher\").get(\"debug\"):\n                file_handler.setLevel(logging.DEBUG)\n            else:\n                file_handler.setLevel(logging.INFO)\n\n            file_handler.setFormatter(file_formatter)\n\n            logger.addHandler(file_handler)\n\n        # handlers\n        # stream handler\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setLevel(logging.INFO)\n        stdout_handler.setFormatter(stdout_formatter)\n\n        # add handlers\n        logger.addHandler(stdout_handler)\n\n        # startup message\n        logger.info(f\"This is BioCypher v{__version__}.\")\n        if log_to_disk:\n            logger.info(f\"Logging into `{logfile}`.\")\n        else:\n            logger.info(\"Logging into stdout.\")\n\n    return logging.getLogger(name)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._logger.log","title":"<code>log()</code>","text":"<p>Browse the log file.</p> Source code in <code>biocypher/biocypher/_logger.py</code> <pre><code>def log():\n    \"\"\"\n    Browse the log file.\n    \"\"\"\n\n    with open(logfile()) as fp:\n        pydoc.pager(fp.read())\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._logger.logfile","title":"<code>logfile()</code>","text":"<p>Path to the log file.</p> Source code in <code>biocypher/biocypher/_logger.py</code> <pre><code>def logfile() -&gt; str:\n    \"\"\"\n    Path to the log file.\n    \"\"\"\n\n    return get_logger().handlers[0].baseFilename\n</code></pre>"},{"location":"BioCypher/reference/source/#_mappingpy","title":"_mapping.py","text":"<p>BioCypher 'mapping' module. Handles the mapping of user-defined schema to the underlying ontology.</p>"},{"location":"BioCypher/reference/source/#biocypher._mapping.OntologyMapping","title":"<code>OntologyMapping</code>","text":"<p>Class to store the ontology mapping and extensions.</p> Source code in <code>biocypher/biocypher/_mapping.py</code> <pre><code>class OntologyMapping:\n    \"\"\"\n    Class to store the ontology mapping and extensions.\n    \"\"\"\n\n    def __init__(self, config_file: str = None):\n        self.schema = self._read_config(config_file)\n\n        self.extended_schema = self._extend_schema()\n\n    def _read_config(self, config_file: str = None):\n        \"\"\"\n        Read the configuration file and store the ontology mapping and extensions.\n        \"\"\"\n        if config_file is None:\n            schema_config = {}\n\n        # load yaml file from web\n        elif config_file.startswith(\"http\"):\n            with urlopen(config_file) as f:\n                schema_config = yaml.safe_load(f)\n\n        # get graph state from config (assume file is local)\n        else:\n            with open(config_file, \"r\") as f:\n                schema_config = yaml.safe_load(f)\n\n        return schema_config\n\n    def _extend_schema(self, d: Optional[dict] = None) -&gt; dict:\n        \"\"\"\n        Get leaves of the tree hierarchy from the data structure dict\n        contained in the `schema_config.yaml`. Creates virtual leaves\n        (as children) from entries that provide more than one preferred\n        id type (and corresponding inputs).\n\n        Args:\n            d:\n                Data structure dict from yaml file.\n\n        \"\"\"\n\n        d = d or self.schema\n\n        extended_schema = dict()\n\n        # first pass: get parent leaves with direct representation in ontology\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            # preferred_id optional: if not provided, use `id`\n            if not v.get(\"preferred_id\"):\n                v[\"preferred_id\"] = \"id\"\n\n            # k is an entity that is present in the ontology\n            if \"is_a\" not in v:\n                extended_schema[k] = v\n\n        # second pass: \"vertical\" inheritance\n        d = self._vertical_property_inheritance(d)\n        for k, v in d.items():\n            if \"is_a\" in v:\n                # prevent loops\n                if k == v[\"is_a\"]:\n                    logger.warning(\n                        f\"Loop detected in ontology mapping: {k} -&gt; {v}. \"\n                        \"Removing item. Please fix the inheritance if you want \"\n                        \"to use this item.\"\n                    )\n                    continue\n\n                extended_schema[k] = v\n\n        # \"horizontal\" inheritance: create siblings for multiple identifiers or\n        # sources -&gt; virtual leaves or implicit children\n        mi_leaves = {}\n        ms_leaves = {}\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            if isinstance(v.get(\"preferred_id\"), list):\n                mi_leaves = self._horizontal_inheritance_pid(k, v)\n                extended_schema.update(mi_leaves)\n\n            elif isinstance(v.get(\"source\"), list):\n                ms_leaves = self._horizontal_inheritance_source(k, v)\n                extended_schema.update(ms_leaves)\n\n        return extended_schema\n\n    def _vertical_property_inheritance(self, d):\n        \"\"\"\n        Inherit properties from parents to children and update `d` accordingly.\n        \"\"\"\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            # k is an entity that is present in the ontology\n            if \"is_a\" not in v:\n                continue\n\n            # \"vertical\" inheritance: inherit properties from parent\n            if v.get(\"inherit_properties\", False):\n                # get direct ancestor\n                if isinstance(v[\"is_a\"], list):\n                    parent = v[\"is_a\"][0]\n                else:\n                    parent = v[\"is_a\"]\n\n                # ensure child has properties and exclude_properties\n                if \"properties\" not in v:\n                    v[\"properties\"] = {}\n                if \"exclude_properties\" not in v:\n                    v[\"exclude_properties\"] = {}\n\n                # update properties of child\n                parent_props = self.schema[parent].get(\"properties\", {})\n                if parent_props:\n                    v[\"properties\"].update(parent_props)\n\n                parent_excl_props = self.schema[parent].get(\"exclude_properties\", {})\n                if parent_excl_props:\n                    v[\"exclude_properties\"].update(parent_excl_props)\n\n                # update schema (d)\n                d[k] = v\n\n        return d\n\n    def _horizontal_inheritance_pid(self, key, value):\n        \"\"\"\n        Create virtual leaves for multiple preferred id types or sources.\n\n        If we create virtual leaves, input_label/label_in_input always has to be\n        a list.\n        \"\"\"\n\n        leaves = {}\n\n        preferred_id = value[\"preferred_id\"]\n        input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n        represented_as = value[\"represented_as\"]\n\n        # adjust lengths\n        max_l = max(\n            [\n                len(_misc.to_list(preferred_id)),\n                len(_misc.to_list(input_label)),\n                len(_misc.to_list(represented_as)),\n            ],\n        )\n\n        # adjust pid length if necessary\n        if isinstance(preferred_id, str):\n            pids = [preferred_id] * max_l\n        else:\n            pids = preferred_id\n\n        # adjust rep length if necessary\n        if isinstance(represented_as, str):\n            reps = [represented_as] * max_l\n        else:\n            reps = represented_as\n\n        for pid, lab, rep in zip(pids, input_label, reps):\n            skey = pid + \".\" + key\n            svalue = {\n                \"preferred_id\": pid,\n                \"input_label\": lab,\n                \"represented_as\": rep,\n                # mark as virtual\n                \"virtual\": True,\n            }\n\n            # inherit is_a if exists\n            if \"is_a\" in value.keys():\n                # treat as multiple inheritance\n                if isinstance(value[\"is_a\"], list):\n                    v = list(value[\"is_a\"])\n                    v.insert(0, key)\n                    svalue[\"is_a\"] = v\n\n                else:\n                    svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n            else:\n                # set parent as is_a\n                svalue[\"is_a\"] = key\n\n            # inherit everything except core attributes\n            for k, v in value.items():\n                if k not in [\n                    \"is_a\",\n                    \"preferred_id\",\n                    \"input_label\",\n                    \"label_in_input\",\n                    \"represented_as\",\n                ]:\n                    svalue[k] = v\n\n            leaves[skey] = svalue\n\n        return leaves\n\n    def _horizontal_inheritance_source(self, key, value):\n        \"\"\"\n        Create virtual leaves for multiple sources.\n\n        If we create virtual leaves, input_label/label_in_input always has to be\n        a list.\n        \"\"\"\n\n        leaves = {}\n\n        source = value[\"source\"]\n        input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n        represented_as = value[\"represented_as\"]\n\n        # adjust lengths\n        src_l = len(source)\n\n        # adjust label length if necessary\n        if isinstance(input_label, str):\n            labels = [input_label] * src_l\n        else:\n            labels = input_label\n\n        # adjust rep length if necessary\n        if isinstance(represented_as, str):\n            reps = [represented_as] * src_l\n        else:\n            reps = represented_as\n\n        for src, lab, rep in zip(source, labels, reps):\n            skey = src + \".\" + key\n            svalue = {\n                \"source\": src,\n                \"input_label\": lab,\n                \"represented_as\": rep,\n                # mark as virtual\n                \"virtual\": True,\n            }\n\n            # inherit is_a if exists\n            if \"is_a\" in value.keys():\n                # treat as multiple inheritance\n                if isinstance(value[\"is_a\"], list):\n                    v = list(value[\"is_a\"])\n                    v.insert(0, key)\n                    svalue[\"is_a\"] = v\n\n                else:\n                    svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n            else:\n                # set parent as is_a\n                svalue[\"is_a\"] = key\n\n            # inherit everything except core attributes\n            for k, v in value.items():\n                if k not in [\n                    \"is_a\",\n                    \"source\",\n                    \"input_label\",\n                    \"label_in_input\",\n                    \"represented_as\",\n                ]:\n                    svalue[k] = v\n\n            leaves[skey] = svalue\n\n        return leaves\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._mapping.OntologyMapping._extend_schema","title":"<code>_extend_schema(d=None)</code>","text":"<p>Get leaves of the tree hierarchy from the data structure dict contained in the <code>schema_config.yaml</code>. Creates virtual leaves (as children) from entries that provide more than one preferred id type (and corresponding inputs).</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Optional[dict]</code> <p>Data structure dict from yaml file.</p> <code>None</code> Source code in <code>biocypher/biocypher/_mapping.py</code> <pre><code>def _extend_schema(self, d: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Get leaves of the tree hierarchy from the data structure dict\n    contained in the `schema_config.yaml`. Creates virtual leaves\n    (as children) from entries that provide more than one preferred\n    id type (and corresponding inputs).\n\n    Args:\n        d:\n            Data structure dict from yaml file.\n\n    \"\"\"\n\n    d = d or self.schema\n\n    extended_schema = dict()\n\n    # first pass: get parent leaves with direct representation in ontology\n    for k, v in d.items():\n        # k is not an entity\n        if \"represented_as\" not in v:\n            continue\n\n        # preferred_id optional: if not provided, use `id`\n        if not v.get(\"preferred_id\"):\n            v[\"preferred_id\"] = \"id\"\n\n        # k is an entity that is present in the ontology\n        if \"is_a\" not in v:\n            extended_schema[k] = v\n\n    # second pass: \"vertical\" inheritance\n    d = self._vertical_property_inheritance(d)\n    for k, v in d.items():\n        if \"is_a\" in v:\n            # prevent loops\n            if k == v[\"is_a\"]:\n                logger.warning(\n                    f\"Loop detected in ontology mapping: {k} -&gt; {v}. \"\n                    \"Removing item. Please fix the inheritance if you want \"\n                    \"to use this item.\"\n                )\n                continue\n\n            extended_schema[k] = v\n\n    # \"horizontal\" inheritance: create siblings for multiple identifiers or\n    # sources -&gt; virtual leaves or implicit children\n    mi_leaves = {}\n    ms_leaves = {}\n    for k, v in d.items():\n        # k is not an entity\n        if \"represented_as\" not in v:\n            continue\n\n        if isinstance(v.get(\"preferred_id\"), list):\n            mi_leaves = self._horizontal_inheritance_pid(k, v)\n            extended_schema.update(mi_leaves)\n\n        elif isinstance(v.get(\"source\"), list):\n            ms_leaves = self._horizontal_inheritance_source(k, v)\n            extended_schema.update(ms_leaves)\n\n    return extended_schema\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._mapping.OntologyMapping._horizontal_inheritance_pid","title":"<code>_horizontal_inheritance_pid(key, value)</code>","text":"<p>Create virtual leaves for multiple preferred id types or sources.</p> <p>If we create virtual leaves, input_label/label_in_input always has to be a list.</p> Source code in <code>biocypher/biocypher/_mapping.py</code> <pre><code>def _horizontal_inheritance_pid(self, key, value):\n    \"\"\"\n    Create virtual leaves for multiple preferred id types or sources.\n\n    If we create virtual leaves, input_label/label_in_input always has to be\n    a list.\n    \"\"\"\n\n    leaves = {}\n\n    preferred_id = value[\"preferred_id\"]\n    input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n    represented_as = value[\"represented_as\"]\n\n    # adjust lengths\n    max_l = max(\n        [\n            len(_misc.to_list(preferred_id)),\n            len(_misc.to_list(input_label)),\n            len(_misc.to_list(represented_as)),\n        ],\n    )\n\n    # adjust pid length if necessary\n    if isinstance(preferred_id, str):\n        pids = [preferred_id] * max_l\n    else:\n        pids = preferred_id\n\n    # adjust rep length if necessary\n    if isinstance(represented_as, str):\n        reps = [represented_as] * max_l\n    else:\n        reps = represented_as\n\n    for pid, lab, rep in zip(pids, input_label, reps):\n        skey = pid + \".\" + key\n        svalue = {\n            \"preferred_id\": pid,\n            \"input_label\": lab,\n            \"represented_as\": rep,\n            # mark as virtual\n            \"virtual\": True,\n        }\n\n        # inherit is_a if exists\n        if \"is_a\" in value.keys():\n            # treat as multiple inheritance\n            if isinstance(value[\"is_a\"], list):\n                v = list(value[\"is_a\"])\n                v.insert(0, key)\n                svalue[\"is_a\"] = v\n\n            else:\n                svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n        else:\n            # set parent as is_a\n            svalue[\"is_a\"] = key\n\n        # inherit everything except core attributes\n        for k, v in value.items():\n            if k not in [\n                \"is_a\",\n                \"preferred_id\",\n                \"input_label\",\n                \"label_in_input\",\n                \"represented_as\",\n            ]:\n                svalue[k] = v\n\n        leaves[skey] = svalue\n\n    return leaves\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._mapping.OntologyMapping._horizontal_inheritance_source","title":"<code>_horizontal_inheritance_source(key, value)</code>","text":"<p>Create virtual leaves for multiple sources.</p> <p>If we create virtual leaves, input_label/label_in_input always has to be a list.</p> Source code in <code>biocypher/biocypher/_mapping.py</code> <pre><code>def _horizontal_inheritance_source(self, key, value):\n    \"\"\"\n    Create virtual leaves for multiple sources.\n\n    If we create virtual leaves, input_label/label_in_input always has to be\n    a list.\n    \"\"\"\n\n    leaves = {}\n\n    source = value[\"source\"]\n    input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n    represented_as = value[\"represented_as\"]\n\n    # adjust lengths\n    src_l = len(source)\n\n    # adjust label length if necessary\n    if isinstance(input_label, str):\n        labels = [input_label] * src_l\n    else:\n        labels = input_label\n\n    # adjust rep length if necessary\n    if isinstance(represented_as, str):\n        reps = [represented_as] * src_l\n    else:\n        reps = represented_as\n\n    for src, lab, rep in zip(source, labels, reps):\n        skey = src + \".\" + key\n        svalue = {\n            \"source\": src,\n            \"input_label\": lab,\n            \"represented_as\": rep,\n            # mark as virtual\n            \"virtual\": True,\n        }\n\n        # inherit is_a if exists\n        if \"is_a\" in value.keys():\n            # treat as multiple inheritance\n            if isinstance(value[\"is_a\"], list):\n                v = list(value[\"is_a\"])\n                v.insert(0, key)\n                svalue[\"is_a\"] = v\n\n            else:\n                svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n        else:\n            # set parent as is_a\n            svalue[\"is_a\"] = key\n\n        # inherit everything except core attributes\n        for k, v in value.items():\n            if k not in [\n                \"is_a\",\n                \"source\",\n                \"input_label\",\n                \"label_in_input\",\n                \"represented_as\",\n            ]:\n                svalue[k] = v\n\n        leaves[skey] = svalue\n\n    return leaves\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._mapping.OntologyMapping._read_config","title":"<code>_read_config(config_file=None)</code>","text":"<p>Read the configuration file and store the ontology mapping and extensions.</p> Source code in <code>biocypher/biocypher/_mapping.py</code> <pre><code>def _read_config(self, config_file: str = None):\n    \"\"\"\n    Read the configuration file and store the ontology mapping and extensions.\n    \"\"\"\n    if config_file is None:\n        schema_config = {}\n\n    # load yaml file from web\n    elif config_file.startswith(\"http\"):\n        with urlopen(config_file) as f:\n            schema_config = yaml.safe_load(f)\n\n    # get graph state from config (assume file is local)\n    else:\n        with open(config_file, \"r\") as f:\n            schema_config = yaml.safe_load(f)\n\n    return schema_config\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._mapping.OntologyMapping._vertical_property_inheritance","title":"<code>_vertical_property_inheritance(d)</code>","text":"<p>Inherit properties from parents to children and update <code>d</code> accordingly.</p> Source code in <code>biocypher/biocypher/_mapping.py</code> <pre><code>def _vertical_property_inheritance(self, d):\n    \"\"\"\n    Inherit properties from parents to children and update `d` accordingly.\n    \"\"\"\n    for k, v in d.items():\n        # k is not an entity\n        if \"represented_as\" not in v:\n            continue\n\n        # k is an entity that is present in the ontology\n        if \"is_a\" not in v:\n            continue\n\n        # \"vertical\" inheritance: inherit properties from parent\n        if v.get(\"inherit_properties\", False):\n            # get direct ancestor\n            if isinstance(v[\"is_a\"], list):\n                parent = v[\"is_a\"][0]\n            else:\n                parent = v[\"is_a\"]\n\n            # ensure child has properties and exclude_properties\n            if \"properties\" not in v:\n                v[\"properties\"] = {}\n            if \"exclude_properties\" not in v:\n                v[\"exclude_properties\"] = {}\n\n            # update properties of child\n            parent_props = self.schema[parent].get(\"properties\", {})\n            if parent_props:\n                v[\"properties\"].update(parent_props)\n\n            parent_excl_props = self.schema[parent].get(\"exclude_properties\", {})\n            if parent_excl_props:\n                v[\"exclude_properties\"].update(parent_excl_props)\n\n            # update schema (d)\n            d[k] = v\n\n    return d\n</code></pre>"},{"location":"BioCypher/reference/source/#_metadatapy","title":"_metadata.py","text":"<p>Package metadata (version, authors, etc).</p>"},{"location":"BioCypher/reference/source/#biocypher._metadata.get_metadata","title":"<code>get_metadata()</code>","text":"<p>Basic package metadata.</p> <p>Retrieves package metadata from the current project directory or from the installed package.</p> Source code in <code>biocypher/biocypher/_metadata.py</code> <pre><code>def get_metadata():\n    \"\"\"\n    Basic package metadata.\n\n    Retrieves package metadata from the current project directory or from\n    the installed package.\n    \"\"\"\n\n    here = pathlib.Path(__file__).parent\n    pyproj_toml = \"pyproject.toml\"\n    meta = {}\n\n    for project_dir in (here, here.parent):\n        toml_path = str(project_dir.joinpath(pyproj_toml).absolute())\n\n        if os.path.exists(toml_path):\n            pyproject = toml.load(toml_path)\n\n            meta = {\n                \"name\": pyproject[\"tool\"][\"poetry\"][\"name\"],\n                \"version\": pyproject[\"tool\"][\"poetry\"][\"version\"],\n                \"author\": pyproject[\"tool\"][\"poetry\"][\"authors\"],\n                \"license\": pyproject[\"tool\"][\"poetry\"][\"license\"],\n                \"full_metadata\": pyproject,\n            }\n\n            break\n\n    if not meta:\n        try:\n            meta = {k.lower(): v for k, v in importlib.metadata.metadata(here.name).items()}\n\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    meta[\"version\"] = meta.get(\"version\", None) or _VERSION\n\n    return meta\n</code></pre>"},{"location":"BioCypher/reference/source/#_miscpy","title":"_misc.py","text":"<p>Handy functions for use in various places.</p>"},{"location":"BioCypher/reference/source/#biocypher._misc._get_inheritance_tree","title":"<code>_get_inheritance_tree(inheritance_graph)</code>","text":"<p>Transforms an inheritance_graph into an inheritance_tree.</p> <p>Parameters:</p> Name Type Description Default <code>inheritance_graph</code> <code>Union[dict, Graph]</code> <p>A dict or nx.Graph representing the inheritance graph.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict representing the inheritance tree.</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def _get_inheritance_tree(inheritance_graph: Union[dict, nx.Graph]) -&gt; dict:\n    \"\"\"Transforms an inheritance_graph into an inheritance_tree.\n\n    Args:\n        inheritance_graph: A dict or nx.Graph representing the inheritance graph.\n\n    Returns:\n        A dict representing the inheritance tree.\n    \"\"\"\n    if isinstance(inheritance_graph, nx.Graph):\n        inheritance_tree = nx.to_dict_of_lists(inheritance_graph)\n\n        multiple_parents_present = _multiple_inheritance_present(inheritance_tree)\n        if multiple_parents_present:\n            logger.warning(\n                \"The ontology contains multiple inheritance (one child node \"\n                \"has multiple parent nodes). This is not visualized in the \"\n                \"following hierarchy tree (the child node is only added once). \"\n                \"If you wish to browse all relationships of the parsed \"\n                \"ontologies, write a graphml file to disk using \"\n                \"`to_disk = &lt;directory&gt;` and view this file.\"\n            )\n\n        # unlist values\n        inheritance_tree = {k: v[0] for k, v in inheritance_tree.items() if v}\n        return inheritance_tree\n    elif not _multiple_inheritance_present(inheritance_graph):\n        return inheritance_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc._multiple_inheritance_present","title":"<code>_multiple_inheritance_present(inheritance_tree)</code>","text":"<p>Checks if multiple inheritance is present in the inheritance_tree.</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def _multiple_inheritance_present(inheritance_tree: dict) -&gt; bool:\n    \"\"\"Checks if multiple inheritance is present in the inheritance_tree.\"\"\"\n    return any(len(value) &gt; 1 for value in inheritance_tree.values())\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.create_tree_visualisation","title":"<code>create_tree_visualisation(inheritance_graph)</code>","text":"<p>Creates a visualisation of the inheritance tree using treelib.</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def create_tree_visualisation(inheritance_graph: Union[dict, nx.Graph]) -&gt; Tree:\n    \"\"\"\n    Creates a visualisation of the inheritance tree using treelib.\n    \"\"\"\n    inheritance_tree = _get_inheritance_tree(inheritance_graph)\n    classes, root = _find_root_node(inheritance_tree)\n\n    tree = Tree()\n    tree.create_node(root, root)\n    while classes:\n        for child in classes:\n            parent = inheritance_tree[child]\n            if parent in tree.nodes.keys() or parent == root:\n                tree.create_node(child, child, parent=parent)\n\n        for node in tree.nodes.keys():\n            if node in classes:\n                classes.remove(node)\n\n    return tree\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.ensure_iterable","title":"<code>ensure_iterable(value)</code>","text":"<p>Returns iterables, except strings, wraps simple types into tuple.</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def ensure_iterable(value: Any) -&gt; Iterable:\n    \"\"\"\n    Returns iterables, except strings, wraps simple types into tuple.\n    \"\"\"\n\n    return value if isinstance(value, LIST_LIKE) else (value,)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.is_nested","title":"<code>is_nested(lst)</code>","text":"<p>Check if a list is nested.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>The list to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the list is nested, False otherwise.</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def is_nested(lst) -&gt; bool:\n    \"\"\"\n    Check if a list is nested.\n\n    Args:\n        lst (list): The list to check.\n\n    Returns:\n        bool: True if the list is nested, False otherwise.\n    \"\"\"\n    for item in lst:\n        if isinstance(item, list):\n            return True\n    return False\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.pascalcase_to_sentencecase","title":"<code>pascalcase_to_sentencecase(s)</code>","text":"<p>Convert PascalCase to sentence case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in PascalCase</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in sentence case form</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def pascalcase_to_sentencecase(s: str) -&gt; str:\n    \"\"\"\n    Convert PascalCase to sentence case.\n\n    Args:\n        s: Input string in PascalCase\n\n    Returns:\n        string in sentence case form\n    \"\"\"\n    return from_pascal(s, sep=\" \")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.sentencecase_to_pascalcase","title":"<code>sentencecase_to_pascalcase(s, sep='\\\\s')</code>","text":"<p>Convert sentence case to PascalCase.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in sentence case</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in PascalCase form</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def sentencecase_to_pascalcase(s: str, sep: str = r\"\\s\") -&gt; str:\n    \"\"\"\n    Convert sentence case to PascalCase.\n\n    Args:\n        s: Input string in sentence case\n\n    Returns:\n        string in PascalCase form\n    \"\"\"\n    return re.sub(\n        r\"(?:^|[\" + sep + \"])([a-zA-Z])\",\n        lambda match: match.group(1).upper(),\n        s,\n    )\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.sentencecase_to_snakecase","title":"<code>sentencecase_to_snakecase(s)</code>","text":"<p>Convert sentence case to snake_case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in sentence case</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in snake_case form</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def sentencecase_to_snakecase(s: str) -&gt; str:\n    \"\"\"\n    Convert sentence case to snake_case.\n\n    Args:\n        s: Input string in sentence case\n\n    Returns:\n        string in snake_case form\n    \"\"\"\n    return stringcase.snakecase(s).lower()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.snakecase_to_sentencecase","title":"<code>snakecase_to_sentencecase(s)</code>","text":"<p>Convert snake_case to sentence case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in snake_case</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in sentence case form</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def snakecase_to_sentencecase(s: str) -&gt; str:\n    \"\"\"\n    Convert snake_case to sentence case.\n\n    Args:\n        s: Input string in snake_case\n\n    Returns:\n        string in sentence case form\n    \"\"\"\n    return stringcase.sentencecase(s).lower()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.to_list","title":"<code>to_list(value)</code>","text":"<p>Ensures that <code>value</code> is a list.</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def to_list(value: Any) -&gt; list:\n    \"\"\"\n    Ensures that ``value`` is a list.\n    \"\"\"\n\n    if isinstance(value, LIST_LIKE):\n        value = list(value)\n\n    else:\n        value = [value]\n\n    return value\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._misc.to_lower_sentence_case","title":"<code>to_lower_sentence_case(s)</code>","text":"<p>Convert any string to lower sentence case. Works with snake_case, PascalCase, and sentence case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in lower sentence case form</p> Source code in <code>biocypher/biocypher/_misc.py</code> <pre><code>def to_lower_sentence_case(s: str) -&gt; str:\n    \"\"\"\n    Convert any string to lower sentence case. Works with snake_case,\n    PascalCase, and sentence case.\n\n    Args:\n        s: Input string\n\n    Returns:\n        string in lower sentence case form\n    \"\"\"\n    if \"_\" in s:\n        return snakecase_to_sentencecase(s)\n    elif \" \" in s:\n        return s.lower()\n    elif s[0].isupper():\n        return pascalcase_to_sentencecase(s)\n    else:\n        return s\n</code></pre>"},{"location":"BioCypher/reference/source/#_ontologypy","title":"_ontology.py","text":"<p>BioCypher 'ontology' module. Contains classes and functions to handle parsing and representation of single ontologies as well as their hybridisation and other advanced operations.</p>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology","title":"<code>Ontology</code>","text":"<p>A class that represents the ontological \"backbone\" of a BioCypher knowledge graph. The ontology can be built from a single resource, or hybridised from a combination of resources, with one resource being the \"head\" ontology, while an arbitrary number of other resources can become \"tail\" ontologies at arbitrary fusion points inside the \"head\" ontology.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>class Ontology:\n    \"\"\"A class that represents the ontological \"backbone\" of a BioCypher knowledge\n    graph. The ontology can be built from a single resource, or hybridised from\n    a combination of resources, with one resource being the \"head\" ontology,\n    while an arbitrary number of other resources can become \"tail\" ontologies at\n    arbitrary fusion points inside the \"head\" ontology.\n    \"\"\"\n\n    def __init__(\n        self,\n        head_ontology: dict,\n        ontology_mapping: Optional[\"OntologyMapping\"] = None,\n        tail_ontologies: dict | None = None,\n    ):\n        \"\"\"Initialize the Ontology class.\n\n        Args:\n        ----\n            head_ontology (OntologyAdapter): The head ontology.\n\n            tail_ontologies (list): A list of OntologyAdapters that will be\n                added to the head ontology. Defaults to None.\n\n        \"\"\"\n        self._head_ontology_meta = head_ontology\n        self.mapping = ontology_mapping\n        self._tail_ontology_meta = tail_ontologies\n\n        self._tail_ontologies = None\n        self._nx_graph = None\n\n        # keep track of nodes that have been extended\n        self._extended_nodes = set()\n\n        self._main()\n\n    def _main(self) -&gt; None:\n        \"\"\"Main method to be run on instantiation. Loads the ontologies, joins\n        them, and returns the hybrid ontology. Loads only the head ontology\n        if nothing else is given. Adds user extensions and properties from\n        the mapping.\n        \"\"\"\n        self._load_ontologies()\n\n        if self._tail_ontologies:\n            for adapter in self._tail_ontologies.values():\n                head_join_node = self._get_head_join_node(adapter)\n                self._join_ontologies(adapter, head_join_node)\n        else:\n            self._nx_graph = self._head_ontology.get_nx_graph()\n\n        if self.mapping:\n            self._extend_ontology()\n\n            # experimental: add connections of disjoint classes to entity\n            # self._connect_biolink_classes()\n\n            self._add_properties()\n\n    def _load_ontologies(self) -&gt; None:\n        \"\"\"For each ontology, load the OntologyAdapter object and store it as an\n        instance variable (head) or a dictionary (tail).\n        \"\"\"\n        logger.info(\"Loading ontologies...\")\n\n        self._head_ontology = OntologyAdapter(\n            ontology_file=self._head_ontology_meta[\"url\"],\n            root_label=self._head_ontology_meta[\"root_node\"],\n            ontology_file_format=self._head_ontology_meta.get(\"format\", None),\n            switch_label_and_id=self._head_ontology_meta.get(\"switch_label_and_id\", True),\n        )\n\n        if self._tail_ontology_meta:\n            self._tail_ontologies = {}\n            for key, value in self._tail_ontology_meta.items():\n                self._tail_ontologies[key] = OntologyAdapter(\n                    ontology_file=value[\"url\"],\n                    root_label=value[\"tail_join_node\"],\n                    head_join_node_label=value[\"head_join_node\"],\n                    ontology_file_format=value.get(\"format\", None),\n                    merge_nodes=value.get(\"merge_nodes\", True),\n                    switch_label_and_id=value.get(\"switch_label_and_id\", True),\n                )\n\n    def _get_head_join_node(self, adapter: OntologyAdapter) -&gt; str:\n        \"\"\"Tries to find the head join node of the given ontology adapter in the\n        head ontology. If the join node is not found, the method will raise an\n        error.\n\n        Args:\n        ----\n            adapter (OntologyAdapter): The ontology adapter of which to find the\n                join node in the head ontology.\n\n        \"\"\"\n        head_join_node = None\n        user_defined_head_join_node_label = adapter.get_head_join_node()\n        head_join_node_label_in_bc_format = to_lower_sentence_case(user_defined_head_join_node_label.replace(\"_\", \" \"))\n\n        if self._head_ontology._switch_label_and_id:\n            head_join_node = head_join_node_label_in_bc_format\n        elif not self._head_ontology._switch_label_and_id:\n            for node_id, data in self._head_ontology.get_nx_graph().nodes(data=True):\n                if \"label\" in data and data[\"label\"] == head_join_node_label_in_bc_format:\n                    head_join_node = node_id\n                    break\n\n        if head_join_node not in self._head_ontology.get_nx_graph().nodes:\n            head_ontology = self._head_ontology._rdf_to_nx(\n                self._head_ontology.get_rdf_graph(),\n                self._head_ontology._root_label,\n                self._head_ontology._switch_label_and_id,\n                rename_nodes=False,\n            )\n            raise ValueError(\n                f\"Head join node '{head_join_node}' not found in head ontology. \"\n                f\"The head ontology contains the following nodes: {head_ontology.nodes}.\",\n            )\n        return head_join_node\n\n    def _join_ontologies(self, adapter: OntologyAdapter, head_join_node) -&gt; None:\n        \"\"\"Joins the ontologies by adding the tail ontology as a subgraph to the\n        head ontology at the specified join nodes.\n\n        Args:\n        ----\n            adapter (OntologyAdapter): The ontology adapter of the tail ontology\n                to be added to the head ontology.\n\n        \"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        tail_join_node = adapter.get_root_node()\n        tail_ontology = adapter.get_nx_graph()\n\n        # subtree of tail ontology at join node\n        tail_ontology_subtree = nx.dfs_tree(tail_ontology.reverse(), tail_join_node).reverse()\n\n        # transfer node attributes from tail ontology to subtree\n        for node in tail_ontology_subtree.nodes:\n            tail_ontology_subtree.nodes[node].update(tail_ontology.nodes[node])\n\n        # if merge_nodes is False, create parent of tail join node from head\n        # join node\n        if not adapter._merge_nodes:\n            # add head join node from head ontology to tail ontology subtree\n            # as parent of tail join node\n            tail_ontology_subtree.add_node(\n                head_join_node,\n                **self._head_ontology.get_nx_graph().nodes[head_join_node],\n            )\n            tail_ontology_subtree.add_edge(tail_join_node, head_join_node)\n\n        # else rename tail join node to match head join node if necessary\n        elif tail_join_node != head_join_node:\n            tail_ontology_subtree = nx.relabel_nodes(tail_ontology_subtree, {tail_join_node: head_join_node})\n\n        # combine head ontology and tail subtree\n        self._nx_graph = nx.compose(self._nx_graph, tail_ontology_subtree)\n\n    def _extend_ontology(self) -&gt; None:\n        \"\"\"Add the user extensions to the ontology.\n\n        Tries to find the parent in the ontology, adds it if necessary, and adds\n        the child and a directed edge from child to parent. Can handle multiple\n        parents.\n        \"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        for key, value in self.mapping.extended_schema.items():\n            if not value.get(\"is_a\"):\n                if self._nx_graph.has_node(value.get(\"synonym_for\")):\n                    continue\n\n                if not self._nx_graph.has_node(key):\n                    msg = (\n                        f\"Node {key} not found in ontology, but also has no inheritance definition. Please check your \"\n                        \"schema for spelling errors, first letter not in lower case, use of underscores, a missing \"\n                        \"`is_a` definition (SubClassOf a root node), or missing labels in class or super-classes.\"\n                    )\n                    logger.error(msg)\n                    raise ValueError(msg)\n\n                continue\n\n            parents = to_list(value.get(\"is_a\"))\n            child = key\n\n            while parents:\n                parent = parents.pop(0)\n\n                if parent not in self._nx_graph.nodes:\n                    self._nx_graph.add_node(parent)\n                    self._nx_graph.nodes[parent][\"label\"] = sentencecase_to_pascalcase(parent)\n\n                    # mark parent as user extension\n                    self._nx_graph.nodes[parent][\"user_extension\"] = True\n                    self._extended_nodes.add(parent)\n\n                if child not in self._nx_graph.nodes:\n                    self._nx_graph.add_node(child)\n                    self._nx_graph.nodes[child][\"label\"] = sentencecase_to_pascalcase(child)\n\n                    # mark child as user extension\n                    self._nx_graph.nodes[child][\"user_extension\"] = True\n                    self._extended_nodes.add(child)\n\n                self._nx_graph.add_edge(child, parent)\n\n                child = parent\n\n    def _connect_biolink_classes(self) -&gt; None:\n        \"\"\"Experimental: Adds edges from disjoint classes to the entity node.\"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        if \"entity\" not in self._nx_graph.nodes:\n            return\n\n        # biolink classes that are disjoint from entity\n        disjoint_classes = [\n            \"frequency qualifier mixin\",\n            \"chemical entity to entity association mixin\",\n            \"ontology class\",\n            \"relationship quantifier\",\n            \"physical essence or occurrent\",\n            \"gene or gene product\",\n            \"subject of investigation\",\n        ]\n\n        for node in disjoint_classes:\n            if not self._nx_graph.nodes.get(node):\n                self._nx_graph.add_node(node)\n                self._nx_graph.nodes[node][\"label\"] = sentencecase_to_pascalcase(node)\n\n            self._nx_graph.add_edge(node, \"entity\")\n\n    def _add_properties(self) -&gt; None:\n        \"\"\"For each entity in the mapping, update the ontology with the properties\n        specified in the mapping. Updates synonym information in the graph,\n        setting the synonym as the primary node label.\n        \"\"\"\n        for key, value in self.mapping.extended_schema.items():\n            if key in self._nx_graph.nodes:\n                self._nx_graph.nodes[key].update(value)\n\n            if value.get(\"synonym_for\"):\n                # change node label to synonym\n                if value[\"synonym_for\"] not in self._nx_graph.nodes:\n                    raise ValueError(f\"Node {value['synonym_for']} not found in ontology.\")\n\n                self._nx_graph = nx.relabel_nodes(self._nx_graph, {value[\"synonym_for\"]: key})\n\n    def get_ancestors(self, node_label: str) -&gt; list:\n        \"\"\"Get the ancestors of a node in the ontology.\n\n        Args:\n        ----\n            node_label (str): The label of the node in the ontology.\n\n        Returns:\n        -------\n            list: A list of the ancestors of the node.\n\n        \"\"\"\n        return nx.dfs_tree(self._nx_graph, node_label)\n\n    def show_ontology_structure(self, to_disk: str = None, full: bool = False):\n        \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n        Args:\n        ----\n            to_disk (str): If specified, the ontology structure will be saved\n                to disk as a GRAPHML file at the location (directory) specified\n                by the `to_disk` string, to be opened in your favourite graph\n                visualisation tool.\n\n            full (bool): If True, the full ontology structure will be shown,\n                including all nodes and edges. If False, only the nodes and\n                edges that are relevant to the extended schema will be shown.\n\n        \"\"\"\n        if not full and not self.mapping.extended_schema:\n            raise ValueError(\n                \"You are attempting to visualise a subset of the loaded\"\n                \"ontology, but have not provided a schema configuration. \"\n                \"To display a partial ontology graph, please provide a schema \"\n                \"configuration file; to visualise the full graph, please use \"\n                \"the parameter `full = True`.\",\n            )\n\n        if not self._nx_graph:\n            raise ValueError(\"Ontology not loaded.\")\n\n        if not self._tail_ontologies:\n            msg = f\"Showing ontology structure based on {self._head_ontology._ontology_file}\"\n\n        else:\n            msg = f\"Showing ontology structure based on {len(self._tail_ontology_meta) + 1} ontologies: \"\n\n        logger.info(msg)\n\n        if not full:\n            # set of leaves and their intermediate parents up to the root\n            filter_nodes = set(self.mapping.extended_schema.keys())\n\n            for node in self.mapping.extended_schema.keys():\n                filter_nodes.update(self.get_ancestors(node).nodes)\n\n            # filter graph\n            G = self._nx_graph.subgraph(filter_nodes)\n\n        else:\n            G = self._nx_graph\n\n        if not to_disk:\n            # create tree\n            tree = create_tree_visualisation(G)\n\n            # add synonym information\n            for node in self.mapping.extended_schema:\n                if not isinstance(self.mapping.extended_schema[node], dict):\n                    continue\n                if self.mapping.extended_schema[node].get(\"synonym_for\"):\n                    tree.nodes[node].tag = f\"{node} = {self.mapping.extended_schema[node].get('synonym_for')}\"\n\n            logger.info(f\"\\n{tree}\")\n\n            return tree\n\n        else:\n            # convert lists/dicts to strings for vis only\n            for node in G.nodes:\n                # rename node and use former id as label\n                label = G.nodes[node].get(\"label\")\n\n                if not label:\n                    label = node\n\n                G = nx.relabel_nodes(G, {node: label})\n                G.nodes[label][\"label\"] = node\n\n                for attrib in G.nodes[label]:\n                    if type(G.nodes[label][attrib]) in [list, dict]:\n                        G.nodes[label][attrib] = str(G.nodes[label][attrib])\n\n            path = os.path.join(to_disk, \"ontology_structure.graphml\")\n\n            logger.info(f\"Writing ontology structure to {path}.\")\n\n            nx.write_graphml(G, path)\n\n            return True\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"Returns a dictionary compatible with a BioCypher node for compatibility\n        with the Neo4j driver.\n        \"\"\"\n        d = {\n            \"node_id\": self._get_current_id(),\n            \"node_label\": \"BioCypher\",\n            \"properties\": {\n                \"schema\": \"self.ontology_mapping.extended_schema\",\n            },\n        }\n\n        return d\n\n    def _get_current_id(self):\n        \"\"\"Instantiate a version ID for the current session. For now does simple\n        versioning using datetime.\n\n        Can later implement incremental versioning, versioning from\n        config file, or manual specification via argument.\n        \"\"\"\n        now = datetime.now()\n        return now.strftime(\"v%Y%m%d-%H%M%S\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology.__init__","title":"<code>__init__(head_ontology, ontology_mapping=None, tail_ontologies=None)</code>","text":"<p>Initialize the Ontology class.</p> <pre><code>head_ontology (OntologyAdapter): The head ontology.\n\ntail_ontologies (list): A list of OntologyAdapters that will be\n    added to the head ontology. Defaults to None.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def __init__(\n    self,\n    head_ontology: dict,\n    ontology_mapping: Optional[\"OntologyMapping\"] = None,\n    tail_ontologies: dict | None = None,\n):\n    \"\"\"Initialize the Ontology class.\n\n    Args:\n    ----\n        head_ontology (OntologyAdapter): The head ontology.\n\n        tail_ontologies (list): A list of OntologyAdapters that will be\n            added to the head ontology. Defaults to None.\n\n    \"\"\"\n    self._head_ontology_meta = head_ontology\n    self.mapping = ontology_mapping\n    self._tail_ontology_meta = tail_ontologies\n\n    self._tail_ontologies = None\n    self._nx_graph = None\n\n    # keep track of nodes that have been extended\n    self._extended_nodes = set()\n\n    self._main()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._add_properties","title":"<code>_add_properties()</code>","text":"<p>For each entity in the mapping, update the ontology with the properties specified in the mapping. Updates synonym information in the graph, setting the synonym as the primary node label.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _add_properties(self) -&gt; None:\n    \"\"\"For each entity in the mapping, update the ontology with the properties\n    specified in the mapping. Updates synonym information in the graph,\n    setting the synonym as the primary node label.\n    \"\"\"\n    for key, value in self.mapping.extended_schema.items():\n        if key in self._nx_graph.nodes:\n            self._nx_graph.nodes[key].update(value)\n\n        if value.get(\"synonym_for\"):\n            # change node label to synonym\n            if value[\"synonym_for\"] not in self._nx_graph.nodes:\n                raise ValueError(f\"Node {value['synonym_for']} not found in ontology.\")\n\n            self._nx_graph = nx.relabel_nodes(self._nx_graph, {value[\"synonym_for\"]: key})\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._connect_biolink_classes","title":"<code>_connect_biolink_classes()</code>","text":"<p>Experimental: Adds edges from disjoint classes to the entity node.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _connect_biolink_classes(self) -&gt; None:\n    \"\"\"Experimental: Adds edges from disjoint classes to the entity node.\"\"\"\n    if not self._nx_graph:\n        self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n    if \"entity\" not in self._nx_graph.nodes:\n        return\n\n    # biolink classes that are disjoint from entity\n    disjoint_classes = [\n        \"frequency qualifier mixin\",\n        \"chemical entity to entity association mixin\",\n        \"ontology class\",\n        \"relationship quantifier\",\n        \"physical essence or occurrent\",\n        \"gene or gene product\",\n        \"subject of investigation\",\n    ]\n\n    for node in disjoint_classes:\n        if not self._nx_graph.nodes.get(node):\n            self._nx_graph.add_node(node)\n            self._nx_graph.nodes[node][\"label\"] = sentencecase_to_pascalcase(node)\n\n        self._nx_graph.add_edge(node, \"entity\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._extend_ontology","title":"<code>_extend_ontology()</code>","text":"<p>Add the user extensions to the ontology.</p> <p>Tries to find the parent in the ontology, adds it if necessary, and adds the child and a directed edge from child to parent. Can handle multiple parents.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _extend_ontology(self) -&gt; None:\n    \"\"\"Add the user extensions to the ontology.\n\n    Tries to find the parent in the ontology, adds it if necessary, and adds\n    the child and a directed edge from child to parent. Can handle multiple\n    parents.\n    \"\"\"\n    if not self._nx_graph:\n        self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n    for key, value in self.mapping.extended_schema.items():\n        if not value.get(\"is_a\"):\n            if self._nx_graph.has_node(value.get(\"synonym_for\")):\n                continue\n\n            if not self._nx_graph.has_node(key):\n                msg = (\n                    f\"Node {key} not found in ontology, but also has no inheritance definition. Please check your \"\n                    \"schema for spelling errors, first letter not in lower case, use of underscores, a missing \"\n                    \"`is_a` definition (SubClassOf a root node), or missing labels in class or super-classes.\"\n                )\n                logger.error(msg)\n                raise ValueError(msg)\n\n            continue\n\n        parents = to_list(value.get(\"is_a\"))\n        child = key\n\n        while parents:\n            parent = parents.pop(0)\n\n            if parent not in self._nx_graph.nodes:\n                self._nx_graph.add_node(parent)\n                self._nx_graph.nodes[parent][\"label\"] = sentencecase_to_pascalcase(parent)\n\n                # mark parent as user extension\n                self._nx_graph.nodes[parent][\"user_extension\"] = True\n                self._extended_nodes.add(parent)\n\n            if child not in self._nx_graph.nodes:\n                self._nx_graph.add_node(child)\n                self._nx_graph.nodes[child][\"label\"] = sentencecase_to_pascalcase(child)\n\n                # mark child as user extension\n                self._nx_graph.nodes[child][\"user_extension\"] = True\n                self._extended_nodes.add(child)\n\n            self._nx_graph.add_edge(child, parent)\n\n            child = parent\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._get_current_id","title":"<code>_get_current_id()</code>","text":"<p>Instantiate a version ID for the current session. For now does simple versioning using datetime.</p> <p>Can later implement incremental versioning, versioning from config file, or manual specification via argument.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_current_id(self):\n    \"\"\"Instantiate a version ID for the current session. For now does simple\n    versioning using datetime.\n\n    Can later implement incremental versioning, versioning from\n    config file, or manual specification via argument.\n    \"\"\"\n    now = datetime.now()\n    return now.strftime(\"v%Y%m%d-%H%M%S\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._get_head_join_node","title":"<code>_get_head_join_node(adapter)</code>","text":"<p>Tries to find the head join node of the given ontology adapter in the head ontology. If the join node is not found, the method will raise an error.</p> <pre><code>adapter (OntologyAdapter): The ontology adapter of which to find the\n    join node in the head ontology.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_head_join_node(self, adapter: OntologyAdapter) -&gt; str:\n    \"\"\"Tries to find the head join node of the given ontology adapter in the\n    head ontology. If the join node is not found, the method will raise an\n    error.\n\n    Args:\n    ----\n        adapter (OntologyAdapter): The ontology adapter of which to find the\n            join node in the head ontology.\n\n    \"\"\"\n    head_join_node = None\n    user_defined_head_join_node_label = adapter.get_head_join_node()\n    head_join_node_label_in_bc_format = to_lower_sentence_case(user_defined_head_join_node_label.replace(\"_\", \" \"))\n\n    if self._head_ontology._switch_label_and_id:\n        head_join_node = head_join_node_label_in_bc_format\n    elif not self._head_ontology._switch_label_and_id:\n        for node_id, data in self._head_ontology.get_nx_graph().nodes(data=True):\n            if \"label\" in data and data[\"label\"] == head_join_node_label_in_bc_format:\n                head_join_node = node_id\n                break\n\n    if head_join_node not in self._head_ontology.get_nx_graph().nodes:\n        head_ontology = self._head_ontology._rdf_to_nx(\n            self._head_ontology.get_rdf_graph(),\n            self._head_ontology._root_label,\n            self._head_ontology._switch_label_and_id,\n            rename_nodes=False,\n        )\n        raise ValueError(\n            f\"Head join node '{head_join_node}' not found in head ontology. \"\n            f\"The head ontology contains the following nodes: {head_ontology.nodes}.\",\n        )\n    return head_join_node\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._join_ontologies","title":"<code>_join_ontologies(adapter, head_join_node)</code>","text":"<p>Joins the ontologies by adding the tail ontology as a subgraph to the head ontology at the specified join nodes.</p> <pre><code>adapter (OntologyAdapter): The ontology adapter of the tail ontology\n    to be added to the head ontology.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _join_ontologies(self, adapter: OntologyAdapter, head_join_node) -&gt; None:\n    \"\"\"Joins the ontologies by adding the tail ontology as a subgraph to the\n    head ontology at the specified join nodes.\n\n    Args:\n    ----\n        adapter (OntologyAdapter): The ontology adapter of the tail ontology\n            to be added to the head ontology.\n\n    \"\"\"\n    if not self._nx_graph:\n        self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n    tail_join_node = adapter.get_root_node()\n    tail_ontology = adapter.get_nx_graph()\n\n    # subtree of tail ontology at join node\n    tail_ontology_subtree = nx.dfs_tree(tail_ontology.reverse(), tail_join_node).reverse()\n\n    # transfer node attributes from tail ontology to subtree\n    for node in tail_ontology_subtree.nodes:\n        tail_ontology_subtree.nodes[node].update(tail_ontology.nodes[node])\n\n    # if merge_nodes is False, create parent of tail join node from head\n    # join node\n    if not adapter._merge_nodes:\n        # add head join node from head ontology to tail ontology subtree\n        # as parent of tail join node\n        tail_ontology_subtree.add_node(\n            head_join_node,\n            **self._head_ontology.get_nx_graph().nodes[head_join_node],\n        )\n        tail_ontology_subtree.add_edge(tail_join_node, head_join_node)\n\n    # else rename tail join node to match head join node if necessary\n    elif tail_join_node != head_join_node:\n        tail_ontology_subtree = nx.relabel_nodes(tail_ontology_subtree, {tail_join_node: head_join_node})\n\n    # combine head ontology and tail subtree\n    self._nx_graph = nx.compose(self._nx_graph, tail_ontology_subtree)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._load_ontologies","title":"<code>_load_ontologies()</code>","text":"<p>For each ontology, load the OntologyAdapter object and store it as an instance variable (head) or a dictionary (tail).</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _load_ontologies(self) -&gt; None:\n    \"\"\"For each ontology, load the OntologyAdapter object and store it as an\n    instance variable (head) or a dictionary (tail).\n    \"\"\"\n    logger.info(\"Loading ontologies...\")\n\n    self._head_ontology = OntologyAdapter(\n        ontology_file=self._head_ontology_meta[\"url\"],\n        root_label=self._head_ontology_meta[\"root_node\"],\n        ontology_file_format=self._head_ontology_meta.get(\"format\", None),\n        switch_label_and_id=self._head_ontology_meta.get(\"switch_label_and_id\", True),\n    )\n\n    if self._tail_ontology_meta:\n        self._tail_ontologies = {}\n        for key, value in self._tail_ontology_meta.items():\n            self._tail_ontologies[key] = OntologyAdapter(\n                ontology_file=value[\"url\"],\n                root_label=value[\"tail_join_node\"],\n                head_join_node_label=value[\"head_join_node\"],\n                ontology_file_format=value.get(\"format\", None),\n                merge_nodes=value.get(\"merge_nodes\", True),\n                switch_label_and_id=value.get(\"switch_label_and_id\", True),\n            )\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology._main","title":"<code>_main()</code>","text":"<p>Main method to be run on instantiation. Loads the ontologies, joins them, and returns the hybrid ontology. Loads only the head ontology if nothing else is given. Adds user extensions and properties from the mapping.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _main(self) -&gt; None:\n    \"\"\"Main method to be run on instantiation. Loads the ontologies, joins\n    them, and returns the hybrid ontology. Loads only the head ontology\n    if nothing else is given. Adds user extensions and properties from\n    the mapping.\n    \"\"\"\n    self._load_ontologies()\n\n    if self._tail_ontologies:\n        for adapter in self._tail_ontologies.values():\n            head_join_node = self._get_head_join_node(adapter)\n            self._join_ontologies(adapter, head_join_node)\n    else:\n        self._nx_graph = self._head_ontology.get_nx_graph()\n\n    if self.mapping:\n        self._extend_ontology()\n\n        # experimental: add connections of disjoint classes to entity\n        # self._connect_biolink_classes()\n\n        self._add_properties()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology.get_ancestors","title":"<code>get_ancestors(node_label)</code>","text":"<p>Get the ancestors of a node in the ontology.</p> <pre><code>node_label (str): The label of the node in the ontology.\n</code></pre> <pre><code>list: A list of the ancestors of the node.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_ancestors(self, node_label: str) -&gt; list:\n    \"\"\"Get the ancestors of a node in the ontology.\n\n    Args:\n    ----\n        node_label (str): The label of the node in the ontology.\n\n    Returns:\n    -------\n        list: A list of the ancestors of the node.\n\n    \"\"\"\n    return nx.dfs_tree(self._nx_graph, node_label)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology.get_dict","title":"<code>get_dict()</code>","text":"<p>Returns a dictionary compatible with a BioCypher node for compatibility with the Neo4j driver.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"Returns a dictionary compatible with a BioCypher node for compatibility\n    with the Neo4j driver.\n    \"\"\"\n    d = {\n        \"node_id\": self._get_current_id(),\n        \"node_label\": \"BioCypher\",\n        \"properties\": {\n            \"schema\": \"self.ontology_mapping.extended_schema\",\n        },\n    }\n\n    return d\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.Ontology.show_ontology_structure","title":"<code>show_ontology_structure(to_disk=None, full=False)</code>","text":"<p>Show the ontology structure using treelib or write to GRAPHML file.</p> <pre><code>to_disk (str): If specified, the ontology structure will be saved\n    to disk as a GRAPHML file at the location (directory) specified\n    by the `to_disk` string, to be opened in your favourite graph\n    visualisation tool.\n\nfull (bool): If True, the full ontology structure will be shown,\n    including all nodes and edges. If False, only the nodes and\n    edges that are relevant to the extended schema will be shown.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def show_ontology_structure(self, to_disk: str = None, full: bool = False):\n    \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n    Args:\n    ----\n        to_disk (str): If specified, the ontology structure will be saved\n            to disk as a GRAPHML file at the location (directory) specified\n            by the `to_disk` string, to be opened in your favourite graph\n            visualisation tool.\n\n        full (bool): If True, the full ontology structure will be shown,\n            including all nodes and edges. If False, only the nodes and\n            edges that are relevant to the extended schema will be shown.\n\n    \"\"\"\n    if not full and not self.mapping.extended_schema:\n        raise ValueError(\n            \"You are attempting to visualise a subset of the loaded\"\n            \"ontology, but have not provided a schema configuration. \"\n            \"To display a partial ontology graph, please provide a schema \"\n            \"configuration file; to visualise the full graph, please use \"\n            \"the parameter `full = True`.\",\n        )\n\n    if not self._nx_graph:\n        raise ValueError(\"Ontology not loaded.\")\n\n    if not self._tail_ontologies:\n        msg = f\"Showing ontology structure based on {self._head_ontology._ontology_file}\"\n\n    else:\n        msg = f\"Showing ontology structure based on {len(self._tail_ontology_meta) + 1} ontologies: \"\n\n    logger.info(msg)\n\n    if not full:\n        # set of leaves and their intermediate parents up to the root\n        filter_nodes = set(self.mapping.extended_schema.keys())\n\n        for node in self.mapping.extended_schema.keys():\n            filter_nodes.update(self.get_ancestors(node).nodes)\n\n        # filter graph\n        G = self._nx_graph.subgraph(filter_nodes)\n\n    else:\n        G = self._nx_graph\n\n    if not to_disk:\n        # create tree\n        tree = create_tree_visualisation(G)\n\n        # add synonym information\n        for node in self.mapping.extended_schema:\n            if not isinstance(self.mapping.extended_schema[node], dict):\n                continue\n            if self.mapping.extended_schema[node].get(\"synonym_for\"):\n                tree.nodes[node].tag = f\"{node} = {self.mapping.extended_schema[node].get('synonym_for')}\"\n\n        logger.info(f\"\\n{tree}\")\n\n        return tree\n\n    else:\n        # convert lists/dicts to strings for vis only\n        for node in G.nodes:\n            # rename node and use former id as label\n            label = G.nodes[node].get(\"label\")\n\n            if not label:\n                label = node\n\n            G = nx.relabel_nodes(G, {node: label})\n            G.nodes[label][\"label\"] = node\n\n            for attrib in G.nodes[label]:\n                if type(G.nodes[label][attrib]) in [list, dict]:\n                    G.nodes[label][attrib] = str(G.nodes[label][attrib])\n\n        path = os.path.join(to_disk, \"ontology_structure.graphml\")\n\n        logger.info(f\"Writing ontology structure to {path}.\")\n\n        nx.write_graphml(G, path)\n\n        return True\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter","title":"<code>OntologyAdapter</code>","text":"<p>Class that represents an ontology to be used in the Biocypher framework. Can read from a variety of formats, including OWL, OBO, and RDF/XML. The ontology is represented by a networkx.DiGraph object; an RDFlib graph is also kept. By default, the DiGraph reverses the label and identifier of the nodes, such that the node name in the graph is the human-readable label. The edges are oriented from child to parent. Labels are formatted in lower sentence case and underscores are replaced by spaces. Identifiers are taken as defined and the prefixes are removed by default.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>class OntologyAdapter:\n    \"\"\"Class that represents an ontology to be used in the Biocypher framework. Can\n    read from a variety of formats, including OWL, OBO, and RDF/XML. The\n    ontology is represented by a networkx.DiGraph object; an RDFlib graph is\n    also kept. By default, the DiGraph reverses the label and identifier of the\n    nodes, such that the node name in the graph is the human-readable label. The\n    edges are oriented from child to parent.\n    Labels are formatted in lower sentence case and underscores are replaced by spaces.\n    Identifiers are taken as defined and the prefixes are removed by default.\n    \"\"\"\n\n    def __init__(\n        self,\n        ontology_file: str,\n        root_label: str,\n        ontology_file_format: str | None = None,\n        head_join_node_label: str | None = None,\n        merge_nodes: bool | None = True,\n        switch_label_and_id: bool = True,\n        remove_prefixes: bool = True,\n    ):\n        \"\"\"Initialize the OntologyAdapter class.\n\n        Args:\n        ----\n            ontology_file (str): Path to the ontology file. Can be local or\n                remote.\n\n            root_label (str): The label of the root node in the ontology. In\n                case of a tail ontology, this is the tail join node.\n\n            ontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n                If format is not passed, it is determined automatically.\n\n            head_join_node_label (str): Optional variable to store the label of the\n                node in the head ontology that should be used to join to the\n                root node of the tail ontology. Defaults to None.\n\n            merge_nodes (bool): If True, head and tail join nodes will be\n                merged, using the label of the head join node. If False, the\n                tail join node will be attached as a child of the head join\n                node.\n\n            switch_label_and_id (bool): If True, the node names in the graph will be\n                the human-readable labels. If False, the node names will be the\n                identifiers. Defaults to True.\n\n            remove_prefixes (bool): If True, the prefixes of the identifiers will\n                be removed. Defaults to True.\n\n        \"\"\"\n        logger.info(f\"Instantiating OntologyAdapter class for {ontology_file}.\")\n\n        self._ontology_file = ontology_file\n        self._root_label = root_label\n        self._format = ontology_file_format\n        self._merge_nodes = merge_nodes\n        self._head_join_node = head_join_node_label\n        self._switch_label_and_id = switch_label_and_id\n        self._remove_prefixes = remove_prefixes\n\n        self._rdf_graph = self._load_rdf_graph(ontology_file)\n\n        self._nx_graph = self._rdf_to_nx(self._rdf_graph, root_label, switch_label_and_id)\n\n    def _rdf_to_nx(\n        self,\n        _rdf_graph: rdflib.Graph,\n        root_label: str,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        one_to_one_triples, one_to_many_dict = self._get_relevant_rdf_triples(_rdf_graph)\n        nx_graph = self._convert_to_nx(one_to_one_triples, one_to_many_dict)\n        nx_graph = self._add_labels_to_nodes(nx_graph, switch_label_and_id)\n        nx_graph = self._change_nodes_to_biocypher_format(nx_graph, switch_label_and_id, rename_nodes)\n        nx_graph = self._get_all_ancestors(nx_graph, root_label, switch_label_and_id, rename_nodes)\n        return nx.DiGraph(nx_graph)\n\n    def _get_relevant_rdf_triples(self, g: rdflib.Graph) -&gt; tuple:\n        one_to_one_inheritance_graph = self._get_one_to_one_inheritance_triples(g)\n        intersection = self._get_multiple_inheritance_dict(g)\n        return one_to_one_inheritance_graph, intersection\n\n    def _get_one_to_one_inheritance_triples(self, g: rdflib.Graph) -&gt; rdflib.Graph:\n        \"\"\"Get the one to one inheritance triples from the RDF graph.\n\n        Args:\n        ----\n            g (rdflib.Graph): The RDF graph\n\n        Returns:\n        -------\n            rdflib.Graph: The one to one inheritance graph\n\n        \"\"\"\n        one_to_one_inheritance_graph = Graph()\n        for s, p, o in g.triples((None, rdflib.RDFS.subClassOf, None)):\n            if self.has_label(s, g):\n                one_to_one_inheritance_graph.add((s, p, o))\n        return one_to_one_inheritance_graph\n\n    def _get_multiple_inheritance_dict(self, g: rdflib.Graph) -&gt; dict:\n        \"\"\"Get the multiple inheritance dictionary from the RDF graph.\n\n        Args:\n        ----\n            g (rdflib.Graph): The RDF graph\n\n        Returns:\n        -------\n            dict: The multiple inheritance dictionary\n\n        \"\"\"\n        multiple_inheritance = g.triples((None, rdflib.OWL.intersectionOf, None))\n        intersection = {}\n        for (\n            node,\n            has_multiple_parents,\n            first_node_of_intersection_list,\n        ) in multiple_inheritance:\n            parents = self._retrieve_rdf_linked_list(first_node_of_intersection_list)\n            child_name = None\n            for s_, _, _ in g.triples((None, rdflib.RDFS.subClassOf, node)):\n                child_name = s_\n\n            # Handle Snomed CT post coordinated expressions\n            if not child_name:\n                for s_, _, _ in g.triples((None, rdflib.OWL.equivalentClass, node)):\n                    child_name = s_\n\n            if child_name:\n                intersection[node] = {\n                    \"child_name\": child_name,\n                    \"parent_node_names\": parents,\n                }\n        return intersection\n\n    def has_label(self, node: rdflib.URIRef, g: rdflib.Graph) -&gt; bool:\n        \"\"\"Does the node have a label in g?\n\n        Args:\n        ----\n            node (rdflib.URIRef): The node to check\n            g (rdflib.Graph): The graph to check in\n        Returns:\n            bool: True if the node has a label, False otherwise\n\n        \"\"\"\n        return (node, rdflib.RDFS.label, None) in g\n\n    def _retrieve_rdf_linked_list(self, subject: rdflib.URIRef) -&gt; list:\n        \"\"\"Recursively retrieves a linked list from RDF.\n        Example RDF list with the items [item1, item2]:\n        list_node - first -&gt; item1\n        list_node - rest -&gt; list_node2\n        list_node2 - first -&gt; item2\n        list_node2 - rest -&gt; nil\n        Args:\n            subject (rdflib.URIRef): One list_node of the RDF list\n        Returns:\n            list: The items of the RDF list\n        \"\"\"\n        g = self._rdf_graph\n        rdf_list = []\n        for s, p, o in g.triples((subject, rdflib.RDF.first, None)):\n            rdf_list.append(o)\n        for s, p, o in g.triples((subject, rdflib.RDF.rest, None)):\n            if o != rdflib.RDF.nil:\n                rdf_list.extend(self._retrieve_rdf_linked_list(o))\n        return rdf_list\n\n    def _convert_to_nx(self, one_to_one: rdflib.Graph, one_to_many: dict) -&gt; nx.DiGraph:\n        \"\"\"Convert the one to one and one to many inheritance graphs to networkx.\n\n        Args:\n        ----\n            one_to_one (rdflib.Graph): The one to one inheritance graph\n            one_to_many (dict): The one to many inheritance dictionary\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx graph\n\n        \"\"\"\n        nx_graph = rdflib_to_networkx_digraph(one_to_one, edge_attrs=lambda s, p, o: {}, calc_weights=False)\n        for key, value in one_to_many.items():\n            nx_graph.add_edges_from([(value[\"child_name\"], parent) for parent in value[\"parent_node_names\"]])\n            if key in nx_graph.nodes:\n                nx_graph.remove_node(key)\n        return nx_graph\n\n    def _add_labels_to_nodes(self, nx_graph: nx.DiGraph, switch_label_and_id: bool) -&gt; nx.DiGraph:\n        \"\"\"Add labels to the nodes in the networkx graph.\n\n        Args:\n        ----\n            nx_graph (nx.DiGraph): The networkx graph\n            switch_label_and_id (bool): If True, id and label are switched\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx graph with labels\n\n        \"\"\"\n        for node in list(nx_graph.nodes):\n            nx_id, nx_label = self._get_nx_id_and_label(node, switch_label_and_id)\n            if nx_id == \"none\":\n                # remove node if it has no id\n                nx_graph.remove_node(node)\n                continue\n\n            nx_graph.nodes[node][\"label\"] = nx_label\n        return nx_graph\n\n    def _change_nodes_to_biocypher_format(\n        self,\n        nx_graph: nx.DiGraph,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        \"\"\"Change the nodes in the networkx graph to BioCypher format:\n            - remove the prefix of the identifier\n            - switch id and label\n            - adapt the labels (replace _ with space and convert to lower sentence case)\n\n        Args:\n        ----\n            nx_graph (nx.DiGraph): The networkx graph\n            switch_label_and_id (bool): If True, id and label are switched\n            rename_nodes (bool): If True, the nodes are renamed\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx ontology graph in BioCypher format\n\n        \"\"\"\n        mapping = {\n            node: self._get_nx_id_and_label(node, switch_label_and_id, rename_nodes)[0] for node in nx_graph.nodes\n        }\n        renamed = nx.relabel_nodes(nx_graph, mapping, copy=False)\n        return renamed\n\n    def _get_all_ancestors(\n        self,\n        renamed: nx.DiGraph,\n        root_label: str,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        \"\"\"Get all ancestors of the root node in the networkx graph.\n\n        Args:\n        ----\n            renamed (nx.DiGraph): The renamed networkx graph\n            root_label (str): The label of the root node in the ontology\n            switch_label_and_id (bool): If True, id and label are switched\n            rename_nodes (bool): If True, the nodes are renamed\n\n        Returns:\n        -------\n            nx.DiGraph: The filtered networkx graph\n\n        \"\"\"\n        root = self._get_nx_id_and_label(\n            self._find_root_label(self._rdf_graph, root_label),\n            switch_label_and_id,\n            rename_nodes,\n        )[0]\n        ancestors = nx.ancestors(renamed, root)\n        ancestors.add(root)\n        filtered_graph = renamed.subgraph(ancestors)\n        return filtered_graph\n\n    def _get_nx_id_and_label(self, node, switch_id_and_label: bool, rename_nodes: bool = True) -&gt; tuple[str, str]:\n        \"\"\"Rename node id and label for nx graph.\n\n        Args:\n        ----\n            node (str): The node to rename\n            switch_id_and_label (bool): If True, switch id and label\n\n        Returns:\n        -------\n            tuple[str, str]: The renamed node id and label\n\n        \"\"\"\n        node_id_str = self._remove_prefix(str(node))\n        node_label_str = str(self._rdf_graph.value(node, rdflib.RDFS.label))\n        if rename_nodes:\n            node_label_str = node_label_str.replace(\"_\", \" \")\n            node_label_str = to_lower_sentence_case(node_label_str)\n        nx_id = node_label_str if switch_id_and_label else node_id_str\n        nx_label = node_id_str if switch_id_and_label else node_label_str\n        return nx_id, nx_label\n\n    def _find_root_label(self, g, root_label):\n        # Loop through all labels in the ontology\n        for label_subject, _, label_in_ontology in g.triples((None, rdflib.RDFS.label, None)):\n            # If the label is the root label, set the root node to the label's subject\n            if str(label_in_ontology) == root_label:\n                root = label_subject\n                break\n        else:\n            labels_in_ontology = []\n            for label_subject, _, label_in_ontology in g.triples((None, rdflib.RDFS.label, None)):\n                labels_in_ontology.append(str(label_in_ontology))\n            raise ValueError(\n                f\"Could not find root node with label '{root_label}'. \"\n                f\"The ontology contains the following labels: {labels_in_ontology}\",\n            )\n        return root\n\n    def _remove_prefix(self, uri: str) -&gt; str:\n        \"\"\"Remove the prefix of a URI. URIs can contain either \"#\" or \"/\" as a\n        separator between the prefix and the local name. The prefix is\n        everything before the last separator.\n        \"\"\"\n        if self._remove_prefixes:\n            return uri.rsplit(\"#\", 1)[-1].rsplit(\"/\", 1)[-1]\n        else:\n            return uri\n\n    def _load_rdf_graph(self, ontology_file):\n        \"\"\"Load the ontology into an RDFlib graph. The ontology file can be in\n        OWL, OBO, or RDF/XML format.\n        \"\"\"\n        g = rdflib.Graph()\n        g.parse(ontology_file, format=self._get_format(ontology_file))\n        return g\n\n    def _get_format(self, ontology_file):\n        \"\"\"Get the format of the ontology file.\"\"\"\n        if self._format:\n            if self._format == \"owl\":\n                return \"application/rdf+xml\"\n            elif self._format == \"obo\":\n                raise NotImplementedError(\"OBO format not yet supported\")\n            elif self._format == \"rdf\":\n                return \"application/rdf+xml\"\n            elif self._format == \"ttl\":\n                return self._format\n            else:\n                raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n\n        if ontology_file.endswith(\".owl\"):\n            return \"application/rdf+xml\"\n        elif ontology_file.endswith(\".obo\"):\n            raise NotImplementedError(\"OBO format not yet supported\")\n        elif ontology_file.endswith(\".rdf\"):\n            return \"application/rdf+xml\"\n        elif ontology_file.endswith(\".ttl\"):\n            return \"ttl\"\n        else:\n            raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n\n    def get_nx_graph(self):\n        \"\"\"Get the networkx graph representing the ontology.\"\"\"\n        return self._nx_graph\n\n    def get_rdf_graph(self):\n        \"\"\"Get the RDFlib graph representing the ontology.\"\"\"\n        return self._rdf_graph\n\n    def get_root_node(self):\n        \"\"\"Get root node in the ontology.\n\n        Returns\n        -------\n            root_node: If _switch_label_and_id is True, the root node label is returned,\n                otherwise the root node id is returned.\n\n        \"\"\"\n        root_node = None\n        root_label = self._root_label.replace(\"_\", \" \")\n\n        if self._switch_label_and_id:\n            root_node = to_lower_sentence_case(root_label)\n        elif not self._switch_label_and_id:\n            for node, data in self.get_nx_graph().nodes(data=True):\n                if \"label\" in data and data[\"label\"] == to_lower_sentence_case(root_label):\n                    root_node = node\n                    break\n\n        return root_node\n\n    def get_ancestors(self, node_label):\n        \"\"\"Get the ancestors of a node in the ontology.\"\"\"\n        return nx.dfs_preorder_nodes(self._nx_graph, node_label)\n\n    def get_head_join_node(self):\n        \"\"\"Get the head join node of the ontology.\"\"\"\n        return self._head_join_node\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.__init__","title":"<code>__init__(ontology_file, root_label, ontology_file_format=None, head_join_node_label=None, merge_nodes=True, switch_label_and_id=True, remove_prefixes=True)</code>","text":"<p>Initialize the OntologyAdapter class.</p> <pre><code>ontology_file (str): Path to the ontology file. Can be local or\n    remote.\n\nroot_label (str): The label of the root node in the ontology. In\n    case of a tail ontology, this is the tail join node.\n\nontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n    If format is not passed, it is determined automatically.\n\nhead_join_node_label (str): Optional variable to store the label of the\n    node in the head ontology that should be used to join to the\n    root node of the tail ontology. Defaults to None.\n\nmerge_nodes (bool): If True, head and tail join nodes will be\n    merged, using the label of the head join node. If False, the\n    tail join node will be attached as a child of the head join\n    node.\n\nswitch_label_and_id (bool): If True, the node names in the graph will be\n    the human-readable labels. If False, the node names will be the\n    identifiers. Defaults to True.\n\nremove_prefixes (bool): If True, the prefixes of the identifiers will\n    be removed. Defaults to True.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def __init__(\n    self,\n    ontology_file: str,\n    root_label: str,\n    ontology_file_format: str | None = None,\n    head_join_node_label: str | None = None,\n    merge_nodes: bool | None = True,\n    switch_label_and_id: bool = True,\n    remove_prefixes: bool = True,\n):\n    \"\"\"Initialize the OntologyAdapter class.\n\n    Args:\n    ----\n        ontology_file (str): Path to the ontology file. Can be local or\n            remote.\n\n        root_label (str): The label of the root node in the ontology. In\n            case of a tail ontology, this is the tail join node.\n\n        ontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n            If format is not passed, it is determined automatically.\n\n        head_join_node_label (str): Optional variable to store the label of the\n            node in the head ontology that should be used to join to the\n            root node of the tail ontology. Defaults to None.\n\n        merge_nodes (bool): If True, head and tail join nodes will be\n            merged, using the label of the head join node. If False, the\n            tail join node will be attached as a child of the head join\n            node.\n\n        switch_label_and_id (bool): If True, the node names in the graph will be\n            the human-readable labels. If False, the node names will be the\n            identifiers. Defaults to True.\n\n        remove_prefixes (bool): If True, the prefixes of the identifiers will\n            be removed. Defaults to True.\n\n    \"\"\"\n    logger.info(f\"Instantiating OntologyAdapter class for {ontology_file}.\")\n\n    self._ontology_file = ontology_file\n    self._root_label = root_label\n    self._format = ontology_file_format\n    self._merge_nodes = merge_nodes\n    self._head_join_node = head_join_node_label\n    self._switch_label_and_id = switch_label_and_id\n    self._remove_prefixes = remove_prefixes\n\n    self._rdf_graph = self._load_rdf_graph(ontology_file)\n\n    self._nx_graph = self._rdf_to_nx(self._rdf_graph, root_label, switch_label_and_id)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._add_labels_to_nodes","title":"<code>_add_labels_to_nodes(nx_graph, switch_label_and_id)</code>","text":"<p>Add labels to the nodes in the networkx graph.</p> <pre><code>nx_graph (nx.DiGraph): The networkx graph\nswitch_label_and_id (bool): If True, id and label are switched\n</code></pre> <pre><code>nx.DiGraph: The networkx graph with labels\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _add_labels_to_nodes(self, nx_graph: nx.DiGraph, switch_label_and_id: bool) -&gt; nx.DiGraph:\n    \"\"\"Add labels to the nodes in the networkx graph.\n\n    Args:\n    ----\n        nx_graph (nx.DiGraph): The networkx graph\n        switch_label_and_id (bool): If True, id and label are switched\n\n    Returns:\n    -------\n        nx.DiGraph: The networkx graph with labels\n\n    \"\"\"\n    for node in list(nx_graph.nodes):\n        nx_id, nx_label = self._get_nx_id_and_label(node, switch_label_and_id)\n        if nx_id == \"none\":\n            # remove node if it has no id\n            nx_graph.remove_node(node)\n            continue\n\n        nx_graph.nodes[node][\"label\"] = nx_label\n    return nx_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._change_nodes_to_biocypher_format","title":"<code>_change_nodes_to_biocypher_format(nx_graph, switch_label_and_id, rename_nodes=True)</code>","text":"Change the nodes in the networkx graph to BioCypher format <ul> <li>remove the prefix of the identifier</li> <li>switch id and label</li> <li>adapt the labels (replace _ with space and convert to lower sentence case)</li> </ul> <pre><code>nx_graph (nx.DiGraph): The networkx graph\nswitch_label_and_id (bool): If True, id and label are switched\nrename_nodes (bool): If True, the nodes are renamed\n</code></pre> <pre><code>nx.DiGraph: The networkx ontology graph in BioCypher format\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _change_nodes_to_biocypher_format(\n    self,\n    nx_graph: nx.DiGraph,\n    switch_label_and_id: bool,\n    rename_nodes: bool = True,\n) -&gt; nx.DiGraph:\n    \"\"\"Change the nodes in the networkx graph to BioCypher format:\n        - remove the prefix of the identifier\n        - switch id and label\n        - adapt the labels (replace _ with space and convert to lower sentence case)\n\n    Args:\n    ----\n        nx_graph (nx.DiGraph): The networkx graph\n        switch_label_and_id (bool): If True, id and label are switched\n        rename_nodes (bool): If True, the nodes are renamed\n\n    Returns:\n    -------\n        nx.DiGraph: The networkx ontology graph in BioCypher format\n\n    \"\"\"\n    mapping = {\n        node: self._get_nx_id_and_label(node, switch_label_and_id, rename_nodes)[0] for node in nx_graph.nodes\n    }\n    renamed = nx.relabel_nodes(nx_graph, mapping, copy=False)\n    return renamed\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._convert_to_nx","title":"<code>_convert_to_nx(one_to_one, one_to_many)</code>","text":"<p>Convert the one to one and one to many inheritance graphs to networkx.</p> <pre><code>one_to_one (rdflib.Graph): The one to one inheritance graph\none_to_many (dict): The one to many inheritance dictionary\n</code></pre> <pre><code>nx.DiGraph: The networkx graph\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _convert_to_nx(self, one_to_one: rdflib.Graph, one_to_many: dict) -&gt; nx.DiGraph:\n    \"\"\"Convert the one to one and one to many inheritance graphs to networkx.\n\n    Args:\n    ----\n        one_to_one (rdflib.Graph): The one to one inheritance graph\n        one_to_many (dict): The one to many inheritance dictionary\n\n    Returns:\n    -------\n        nx.DiGraph: The networkx graph\n\n    \"\"\"\n    nx_graph = rdflib_to_networkx_digraph(one_to_one, edge_attrs=lambda s, p, o: {}, calc_weights=False)\n    for key, value in one_to_many.items():\n        nx_graph.add_edges_from([(value[\"child_name\"], parent) for parent in value[\"parent_node_names\"]])\n        if key in nx_graph.nodes:\n            nx_graph.remove_node(key)\n    return nx_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._get_all_ancestors","title":"<code>_get_all_ancestors(renamed, root_label, switch_label_and_id, rename_nodes=True)</code>","text":"<p>Get all ancestors of the root node in the networkx graph.</p> <pre><code>renamed (nx.DiGraph): The renamed networkx graph\nroot_label (str): The label of the root node in the ontology\nswitch_label_and_id (bool): If True, id and label are switched\nrename_nodes (bool): If True, the nodes are renamed\n</code></pre> <pre><code>nx.DiGraph: The filtered networkx graph\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_all_ancestors(\n    self,\n    renamed: nx.DiGraph,\n    root_label: str,\n    switch_label_and_id: bool,\n    rename_nodes: bool = True,\n) -&gt; nx.DiGraph:\n    \"\"\"Get all ancestors of the root node in the networkx graph.\n\n    Args:\n    ----\n        renamed (nx.DiGraph): The renamed networkx graph\n        root_label (str): The label of the root node in the ontology\n        switch_label_and_id (bool): If True, id and label are switched\n        rename_nodes (bool): If True, the nodes are renamed\n\n    Returns:\n    -------\n        nx.DiGraph: The filtered networkx graph\n\n    \"\"\"\n    root = self._get_nx_id_and_label(\n        self._find_root_label(self._rdf_graph, root_label),\n        switch_label_and_id,\n        rename_nodes,\n    )[0]\n    ancestors = nx.ancestors(renamed, root)\n    ancestors.add(root)\n    filtered_graph = renamed.subgraph(ancestors)\n    return filtered_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._get_format","title":"<code>_get_format(ontology_file)</code>","text":"<p>Get the format of the ontology file.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_format(self, ontology_file):\n    \"\"\"Get the format of the ontology file.\"\"\"\n    if self._format:\n        if self._format == \"owl\":\n            return \"application/rdf+xml\"\n        elif self._format == \"obo\":\n            raise NotImplementedError(\"OBO format not yet supported\")\n        elif self._format == \"rdf\":\n            return \"application/rdf+xml\"\n        elif self._format == \"ttl\":\n            return self._format\n        else:\n            raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n\n    if ontology_file.endswith(\".owl\"):\n        return \"application/rdf+xml\"\n    elif ontology_file.endswith(\".obo\"):\n        raise NotImplementedError(\"OBO format not yet supported\")\n    elif ontology_file.endswith(\".rdf\"):\n        return \"application/rdf+xml\"\n    elif ontology_file.endswith(\".ttl\"):\n        return \"ttl\"\n    else:\n        raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._get_multiple_inheritance_dict","title":"<code>_get_multiple_inheritance_dict(g)</code>","text":"<p>Get the multiple inheritance dictionary from the RDF graph.</p> <pre><code>g (rdflib.Graph): The RDF graph\n</code></pre> <pre><code>dict: The multiple inheritance dictionary\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_multiple_inheritance_dict(self, g: rdflib.Graph) -&gt; dict:\n    \"\"\"Get the multiple inheritance dictionary from the RDF graph.\n\n    Args:\n    ----\n        g (rdflib.Graph): The RDF graph\n\n    Returns:\n    -------\n        dict: The multiple inheritance dictionary\n\n    \"\"\"\n    multiple_inheritance = g.triples((None, rdflib.OWL.intersectionOf, None))\n    intersection = {}\n    for (\n        node,\n        has_multiple_parents,\n        first_node_of_intersection_list,\n    ) in multiple_inheritance:\n        parents = self._retrieve_rdf_linked_list(first_node_of_intersection_list)\n        child_name = None\n        for s_, _, _ in g.triples((None, rdflib.RDFS.subClassOf, node)):\n            child_name = s_\n\n        # Handle Snomed CT post coordinated expressions\n        if not child_name:\n            for s_, _, _ in g.triples((None, rdflib.OWL.equivalentClass, node)):\n                child_name = s_\n\n        if child_name:\n            intersection[node] = {\n                \"child_name\": child_name,\n                \"parent_node_names\": parents,\n            }\n    return intersection\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._get_nx_id_and_label","title":"<code>_get_nx_id_and_label(node, switch_id_and_label, rename_nodes=True)</code>","text":"<p>Rename node id and label for nx graph.</p> <pre><code>node (str): The node to rename\nswitch_id_and_label (bool): If True, switch id and label\n</code></pre> <pre><code>tuple[str, str]: The renamed node id and label\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_nx_id_and_label(self, node, switch_id_and_label: bool, rename_nodes: bool = True) -&gt; tuple[str, str]:\n    \"\"\"Rename node id and label for nx graph.\n\n    Args:\n    ----\n        node (str): The node to rename\n        switch_id_and_label (bool): If True, switch id and label\n\n    Returns:\n    -------\n        tuple[str, str]: The renamed node id and label\n\n    \"\"\"\n    node_id_str = self._remove_prefix(str(node))\n    node_label_str = str(self._rdf_graph.value(node, rdflib.RDFS.label))\n    if rename_nodes:\n        node_label_str = node_label_str.replace(\"_\", \" \")\n        node_label_str = to_lower_sentence_case(node_label_str)\n    nx_id = node_label_str if switch_id_and_label else node_id_str\n    nx_label = node_id_str if switch_id_and_label else node_label_str\n    return nx_id, nx_label\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._get_one_to_one_inheritance_triples","title":"<code>_get_one_to_one_inheritance_triples(g)</code>","text":"<p>Get the one to one inheritance triples from the RDF graph.</p> <pre><code>g (rdflib.Graph): The RDF graph\n</code></pre> <pre><code>rdflib.Graph: The one to one inheritance graph\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _get_one_to_one_inheritance_triples(self, g: rdflib.Graph) -&gt; rdflib.Graph:\n    \"\"\"Get the one to one inheritance triples from the RDF graph.\n\n    Args:\n    ----\n        g (rdflib.Graph): The RDF graph\n\n    Returns:\n    -------\n        rdflib.Graph: The one to one inheritance graph\n\n    \"\"\"\n    one_to_one_inheritance_graph = Graph()\n    for s, p, o in g.triples((None, rdflib.RDFS.subClassOf, None)):\n        if self.has_label(s, g):\n            one_to_one_inheritance_graph.add((s, p, o))\n    return one_to_one_inheritance_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._load_rdf_graph","title":"<code>_load_rdf_graph(ontology_file)</code>","text":"<p>Load the ontology into an RDFlib graph. The ontology file can be in OWL, OBO, or RDF/XML format.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _load_rdf_graph(self, ontology_file):\n    \"\"\"Load the ontology into an RDFlib graph. The ontology file can be in\n    OWL, OBO, or RDF/XML format.\n    \"\"\"\n    g = rdflib.Graph()\n    g.parse(ontology_file, format=self._get_format(ontology_file))\n    return g\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._remove_prefix","title":"<code>_remove_prefix(uri)</code>","text":"<p>Remove the prefix of a URI. URIs can contain either \"#\" or \"/\" as a separator between the prefix and the local name. The prefix is everything before the last separator.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _remove_prefix(self, uri: str) -&gt; str:\n    \"\"\"Remove the prefix of a URI. URIs can contain either \"#\" or \"/\" as a\n    separator between the prefix and the local name. The prefix is\n    everything before the last separator.\n    \"\"\"\n    if self._remove_prefixes:\n        return uri.rsplit(\"#\", 1)[-1].rsplit(\"/\", 1)[-1]\n    else:\n        return uri\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter._retrieve_rdf_linked_list","title":"<code>_retrieve_rdf_linked_list(subject)</code>","text":"<p>Recursively retrieves a linked list from RDF. Example RDF list with the items [item1, item2]: list_node - first -&gt; item1 list_node - rest -&gt; list_node2 list_node2 - first -&gt; item2 list_node2 - rest -&gt; nil Args:     subject (rdflib.URIRef): One list_node of the RDF list Returns:     list: The items of the RDF list</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def _retrieve_rdf_linked_list(self, subject: rdflib.URIRef) -&gt; list:\n    \"\"\"Recursively retrieves a linked list from RDF.\n    Example RDF list with the items [item1, item2]:\n    list_node - first -&gt; item1\n    list_node - rest -&gt; list_node2\n    list_node2 - first -&gt; item2\n    list_node2 - rest -&gt; nil\n    Args:\n        subject (rdflib.URIRef): One list_node of the RDF list\n    Returns:\n        list: The items of the RDF list\n    \"\"\"\n    g = self._rdf_graph\n    rdf_list = []\n    for s, p, o in g.triples((subject, rdflib.RDF.first, None)):\n        rdf_list.append(o)\n    for s, p, o in g.triples((subject, rdflib.RDF.rest, None)):\n        if o != rdflib.RDF.nil:\n            rdf_list.extend(self._retrieve_rdf_linked_list(o))\n    return rdf_list\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.get_ancestors","title":"<code>get_ancestors(node_label)</code>","text":"<p>Get the ancestors of a node in the ontology.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_ancestors(self, node_label):\n    \"\"\"Get the ancestors of a node in the ontology.\"\"\"\n    return nx.dfs_preorder_nodes(self._nx_graph, node_label)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.get_head_join_node","title":"<code>get_head_join_node()</code>","text":"<p>Get the head join node of the ontology.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_head_join_node(self):\n    \"\"\"Get the head join node of the ontology.\"\"\"\n    return self._head_join_node\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.get_nx_graph","title":"<code>get_nx_graph()</code>","text":"<p>Get the networkx graph representing the ontology.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_nx_graph(self):\n    \"\"\"Get the networkx graph representing the ontology.\"\"\"\n    return self._nx_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.get_rdf_graph","title":"<code>get_rdf_graph()</code>","text":"<p>Get the RDFlib graph representing the ontology.</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_rdf_graph(self):\n    \"\"\"Get the RDFlib graph representing the ontology.\"\"\"\n    return self._rdf_graph\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.get_root_node","title":"<code>get_root_node()</code>","text":"<p>Get root node in the ontology.</p>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.get_root_node--returns","title":"Returns","text":"<pre><code>root_node: If _switch_label_and_id is True, the root node label is returned,\n    otherwise the root node id is returned.\n</code></pre> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def get_root_node(self):\n    \"\"\"Get root node in the ontology.\n\n    Returns\n    -------\n        root_node: If _switch_label_and_id is True, the root node label is returned,\n            otherwise the root node id is returned.\n\n    \"\"\"\n    root_node = None\n    root_label = self._root_label.replace(\"_\", \" \")\n\n    if self._switch_label_and_id:\n        root_node = to_lower_sentence_case(root_label)\n    elif not self._switch_label_and_id:\n        for node, data in self.get_nx_graph().nodes(data=True):\n            if \"label\" in data and data[\"label\"] == to_lower_sentence_case(root_label):\n                root_node = node\n                break\n\n    return root_node\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._ontology.OntologyAdapter.has_label","title":"<code>has_label(node, g)</code>","text":"<p>Does the node have a label in g?</p> <pre><code>node (rdflib.URIRef): The node to check\ng (rdflib.Graph): The graph to check in\n</code></pre> <p>Returns:     bool: True if the node has a label, False otherwise</p> Source code in <code>biocypher/biocypher/_ontology.py</code> <pre><code>def has_label(self, node: rdflib.URIRef, g: rdflib.Graph) -&gt; bool:\n    \"\"\"Does the node have a label in g?\n\n    Args:\n    ----\n        node (rdflib.URIRef): The node to check\n        g (rdflib.Graph): The graph to check in\n    Returns:\n        bool: True if the node has a label, False otherwise\n\n    \"\"\"\n    return (node, rdflib.RDFS.label, None) in g\n</code></pre>"},{"location":"BioCypher/reference/source/#_translatepy","title":"_translate.py","text":"<p>BioCypher 'translation' module. Responsible for translating between the raw input data and the BioCypherNode and BioCypherEdge objects.</p>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator","title":"<code>Translator</code>","text":"<p>Class responsible for exacting the translation process that is configured in the schema_config.yaml file. Creates a mapping dictionary from that file, and, given nodes and edges, translates them into BioCypherNodes and BioCypherEdges. During this process, can also filter the properties of the entities if the schema_config.yaml file specifies a property whitelist or blacklist.</p> <p>Provides utility functions for translating between input and output labels and cypher queries.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>class Translator:\n    \"\"\"\n    Class responsible for exacting the translation process that is configured in\n    the schema_config.yaml file. Creates a mapping dictionary from that file,\n    and, given nodes and edges, translates them into BioCypherNodes and\n    BioCypherEdges. During this process, can also filter the properties of the\n    entities if the schema_config.yaml file specifies a property whitelist or\n    blacklist.\n\n    Provides utility functions for translating between input and output labels\n    and cypher queries.\n    \"\"\"\n\n    def __init__(self, ontology: \"Ontology\", strict_mode: bool = False):\n        \"\"\"\n        Args:\n            leaves:\n                Dictionary detailing the leaves of the hierarchy\n                tree representing the structure of the graph; the leaves are\n                the entities that will be direct components of the graph,\n                while the intermediary nodes are additional labels for\n                filtering purposes.\n            strict_mode:\n                If True, the translator will raise an error if input data do not\n                carry source, licence, and version information.\n        \"\"\"\n\n        self.ontology = ontology\n        self.strict_mode = strict_mode\n\n        # record nodes without biolink type configured in schema_config.yaml\n        self.notype = {}\n\n        # mapping functionality for translating terms and queries\n        self.mappings = {}\n        self.reverse_mappings = {}\n\n        self._update_ontology_types()\n\n    def translate_entities(self, entities):\n        entities = peekable(entities)\n        if (\n            isinstance(entities.peek(), BioCypherNode)\n            or isinstance(entities.peek(), BioCypherEdge)\n            or isinstance(entities.peek(), BioCypherRelAsNode)\n        ):\n            translated_entities = entities\n        elif len(entities.peek()) &lt; 4:\n            translated_entities = self.translate_nodes(entities)\n        else:\n            translated_entities = self.translate_edges(entities)\n        return translated_entities\n\n    def translate_nodes(\n        self,\n        node_tuples: Iterable,\n    ) -&gt; Generator[BioCypherNode, None, None]:\n        \"\"\"\n        Translates input node representation to a representation that\n        conforms to the schema of the given BioCypher graph. For now\n        requires explicit statement of node type on pass.\n\n        Args:\n            node_tuples (list of tuples): collection of tuples\n                representing individual nodes by their unique id and a type\n                that is translated from the original database notation to\n                the corresponding BioCypher notation.\n\n        \"\"\"\n\n        self._log_begin_translate(node_tuples, \"nodes\")\n\n        for _id, _type, _props in node_tuples:\n            # check for strict mode requirements\n            required_props = [\"source\", \"licence\", \"version\"]\n\n            if self.strict_mode:\n                # rename 'license' to 'licence' in _props\n                if _props.get(\"license\"):\n                    _props[\"licence\"] = _props.pop(\"license\")\n\n                for prop in required_props:\n                    if prop not in _props:\n                        raise ValueError(\n                            f\"Property `{prop}` missing from node {_id}. \"\n                            \"Strict mode is enabled, so this is not allowed.\"\n                        )\n\n            # find the node in leaves that represents ontology node type\n            _ontology_class = self._get_ontology_mapping(_type)\n\n            if _ontology_class:\n                # filter properties for those specified in schema_config if any\n                _filtered_props = self._filter_props(_ontology_class, _props)\n\n                # preferred id\n                _preferred_id = self._get_preferred_id(_ontology_class)\n\n                yield BioCypherNode(\n                    node_id=_id,\n                    node_label=_ontology_class,\n                    preferred_id=_preferred_id,\n                    properties=_filtered_props,\n                )\n\n            else:\n                self._record_no_type(_type, _id)\n\n        self._log_finish_translate(\"nodes\")\n\n    def _get_preferred_id(self, _bl_type: str) -&gt; str:\n        \"\"\"\n        Returns the preferred id for the given Biolink type.\n        \"\"\"\n\n        return (\n            self.ontology.mapping.extended_schema[_bl_type][\"preferred_id\"]\n            if \"preferred_id\" in self.ontology.mapping.extended_schema.get(_bl_type, {})\n            else \"id\"\n        )\n\n    def _filter_props(self, bl_type: str, props: dict) -&gt; dict:\n        \"\"\"\n        Filters properties for those specified in schema_config if any.\n        \"\"\"\n\n        filter_props = self.ontology.mapping.extended_schema[bl_type].get(\"properties\", {})\n\n        # strict mode: add required properties (only if there is a whitelist)\n        if self.strict_mode and filter_props:\n            filter_props.update(\n                {\"source\": \"str\", \"licence\": \"str\", \"version\": \"str\"},\n            )\n\n        exclude_props = self.ontology.mapping.extended_schema[bl_type].get(\"exclude_properties\", [])\n\n        if isinstance(exclude_props, str):\n            exclude_props = [exclude_props]\n\n        if filter_props and exclude_props:\n            filtered_props = {k: v for k, v in props.items() if (k in filter_props.keys() and k not in exclude_props)}\n\n        elif filter_props:\n            filtered_props = {k: v for k, v in props.items() if k in filter_props.keys()}\n\n        elif exclude_props:\n            filtered_props = {k: v for k, v in props.items() if k not in exclude_props}\n\n        else:\n            return props\n\n        missing_props = [k for k in filter_props.keys() if k not in filtered_props.keys()]\n        # add missing properties with default values\n        for k in missing_props:\n            filtered_props[k] = None\n\n        return filtered_props\n\n    def translate_edges(\n        self,\n        edge_tuples: Iterable,\n    ) -&gt; Generator[Union[BioCypherEdge, BioCypherRelAsNode], None, None]:\n        \"\"\"\n        Translates input edge representation to a representation that\n        conforms to the schema of the given BioCypher graph. For now\n        requires explicit statement of edge type on pass.\n\n        Args:\n\n            edge_tuples (list of tuples):\n\n                collection of tuples representing source and target of\n                an interaction via their unique ids as well as the type\n                of interaction in the original database notation, which\n                is translated to BioCypher notation using the `leaves`.\n                Can optionally possess its own ID.\n        \"\"\"\n\n        self._log_begin_translate(edge_tuples, \"edges\")\n\n        # legacy: deal with 4-tuples (no edge id)\n        # TODO remove for performance reasons once safe\n        edge_tuples = peekable(edge_tuples)\n        if len(edge_tuples.peek()) == 4:\n            edge_tuples = [(None, src, tar, typ, props) for src, tar, typ, props in edge_tuples]\n\n        for _id, _src, _tar, _type, _props in edge_tuples:\n            # check for strict mode requirements\n            if self.strict_mode:\n                if \"source\" not in _props:\n                    raise ValueError(\n                        f\"Edge {_id if _id else (_src, _tar)} does not have a `source` property.\",\n                        \" This is required in strict mode.\",\n                    )\n                if \"licence\" not in _props:\n                    raise ValueError(\n                        f\"Edge {_id if _id else (_src, _tar)} does not have a `licence` property.\",\n                        \" This is required in strict mode.\",\n                    )\n\n            # match the input label (_type) to\n            # a Biolink label from schema_config\n            bl_type = self._get_ontology_mapping(_type)\n\n            if bl_type:\n                # filter properties for those specified in schema_config if any\n                _filtered_props = self._filter_props(bl_type, _props)\n\n                rep = self.ontology.mapping.extended_schema[bl_type][\"represented_as\"]\n\n                if rep == \"node\":\n                    if _id:\n                        # if it brings its own ID, use it\n                        node_id = _id\n\n                    else:\n                        # source target concat\n                        node_id = str(_src) + \"_\" + str(_tar) + \"_\" + \"_\".join(str(v) for v in _filtered_props.values())\n\n                    n = BioCypherNode(\n                        node_id=node_id,\n                        node_label=bl_type,\n                        properties=_filtered_props,\n                    )\n\n                    # directionality check TODO generalise to account for\n                    # different descriptions of directionality or find a\n                    # more consistent solution for indicating directionality\n                    if _filtered_props.get(\"directed\") == True:  # noqa: E712 (seems to not work without '== True')\n                        l1 = \"IS_SOURCE_OF\"\n                        l2 = \"IS_TARGET_OF\"\n\n                    elif _filtered_props.get(\n                        \"src_role\",\n                    ) and _filtered_props.get(\"tar_role\"):\n                        l1 = _filtered_props.get(\"src_role\")\n                        l2 = _filtered_props.get(\"tar_role\")\n\n                    else:\n                        l1 = l2 = \"IS_PART_OF\"\n\n                    e_s = BioCypherEdge(\n                        source_id=_src,\n                        target_id=node_id,\n                        relationship_label=l1,\n                        # additional here\n                    )\n\n                    e_t = BioCypherEdge(\n                        source_id=_tar,\n                        target_id=node_id,\n                        relationship_label=l2,\n                        # additional here\n                    )\n\n                    yield BioCypherRelAsNode(n, e_s, e_t)\n\n                else:\n                    edge_label = self.ontology.mapping.extended_schema[bl_type].get(\"label_as_edge\")\n\n                    if edge_label is None:\n                        edge_label = bl_type\n\n                    yield BioCypherEdge(\n                        relationship_id=_id,\n                        source_id=_src,\n                        target_id=_tar,\n                        relationship_label=edge_label,\n                        properties=_filtered_props,\n                    )\n\n            else:\n                self._record_no_type(_type, (_src, _tar))\n\n        self._log_finish_translate(\"edges\")\n\n    def _record_no_type(self, _type: Any, what: Any) -&gt; None:\n        \"\"\"\n        Records the type of a node or edge that is not represented in the\n        schema_config.\n        \"\"\"\n\n        logger.debug(f\"No ontology type defined for `{_type}`: {what}\")\n\n        if self.notype.get(_type, None):\n            self.notype[_type] += 1\n\n        else:\n            self.notype[_type] = 1\n\n    def get_missing_biolink_types(self) -&gt; dict:\n        \"\"\"\n        Returns a dictionary of types that were not represented in the\n        schema_config.\n        \"\"\"\n\n        return self.notype\n\n    @staticmethod\n    def _log_begin_translate(_input: Iterable, what: str):\n        n = f\"{len(_input)} \" if hasattr(_input, \"__len__\") else \"\"\n\n        logger.debug(f\"Translating {n}{what} to BioCypher\")\n\n    @staticmethod\n    def _log_finish_translate(what: str):\n        logger.debug(f\"Finished translating {what} to BioCypher.\")\n\n    def _update_ontology_types(self):\n        \"\"\"\n        Creates a dictionary to translate from input labels to ontology labels.\n\n        If multiple input labels, creates mapping for each.\n        \"\"\"\n\n        self._ontology_mapping = {}\n\n        for key, value in self.ontology.mapping.extended_schema.items():\n            labels = value.get(\"input_label\") or value.get(\"label_in_input\")\n\n            if isinstance(labels, str):\n                self._ontology_mapping[labels] = key\n\n            elif isinstance(labels, list):\n                for label in labels:\n                    self._ontology_mapping[label] = key\n\n            if value.get(\"label_as_edge\"):\n                self._add_translation_mappings(labels, value[\"label_as_edge\"])\n\n            else:\n                self._add_translation_mappings(labels, key)\n\n    def _get_ontology_mapping(self, label: str) -&gt; Optional[str]:\n        \"\"\"\n        For each given input type (\"input_label\" or \"label_in_input\"), find the\n        corresponding ontology class in the leaves dictionary (from the\n        `schema_config.yam`).\n\n        Args:\n            label:\n                The input type to find (`input_label` or `label_in_input` in\n                `schema_config.yaml`).\n        \"\"\"\n\n        # commented out until behaviour of _update_bl_types is fixed\n        return self._ontology_mapping.get(label, None)\n\n    def translate_term(self, term):\n        \"\"\"\n        Translate a single term.\n        \"\"\"\n\n        return self.mappings.get(term, None)\n\n    def reverse_translate_term(self, term):\n        \"\"\"\n        Reverse translate a single term.\n        \"\"\"\n\n        return self.reverse_mappings.get(term, None)\n\n    def translate(self, query):\n        \"\"\"\n        Translate a cypher query. Only translates labels as of now.\n        \"\"\"\n        for key in self.mappings:\n            query = query.replace(\":\" + key, \":\" + self.mappings[key])\n        return query\n\n    def reverse_translate(self, query):\n        \"\"\"\n        Reverse translate a cypher query. Only translates labels as of\n        now.\n        \"\"\"\n        for key in self.reverse_mappings:\n            a = \":\" + key + \")\"\n            b = \":\" + key + \"]\"\n            # TODO this conditional probably does not cover all cases\n            if a in query or b in query:\n                if isinstance(self.reverse_mappings[key], list):\n                    raise NotImplementedError(\n                        \"Reverse translation of multiple inputs not \"\n                        \"implemented yet. Many-to-one mappings are \"\n                        \"not reversible. \"\n                        f\"({key} -&gt; {self.reverse_mappings[key]})\",\n                    )\n                else:\n                    query = query.replace(\n                        a,\n                        \":\" + self.reverse_mappings[key] + \")\",\n                    ).replace(b, \":\" + self.reverse_mappings[key] + \"]\")\n        return query\n\n    def _add_translation_mappings(self, original_name, biocypher_name):\n        \"\"\"\n        Add translation mappings for a label and name. We use here the\n        PascalCase version of the BioCypher name, since sentence case is\n        not useful for Cypher queries.\n        \"\"\"\n        if isinstance(original_name, list):\n            for on in original_name:\n                self.mappings[on] = self.name_sentence_to_pascal(\n                    biocypher_name,\n                )\n        else:\n            self.mappings[original_name] = self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n\n        if isinstance(biocypher_name, list):\n            for bn in biocypher_name:\n                self.reverse_mappings[\n                    self.name_sentence_to_pascal(\n                        bn,\n                    )\n                ] = original_name\n        else:\n            self.reverse_mappings[\n                self.name_sentence_to_pascal(\n                    biocypher_name,\n                )\n            ] = original_name\n\n    @staticmethod\n    def name_sentence_to_pascal(name: str) -&gt; str:\n        \"\"\"\n        Converts a name in sentence case to pascal case.\n        \"\"\"\n        # split on dots if dot is present\n        if \".\" in name:\n            return \".\".join(\n                [_misc.sentencecase_to_pascalcase(n) for n in name.split(\".\")],\n            )\n        else:\n            return _misc.sentencecase_to_pascalcase(name)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.__init__","title":"<code>__init__(ontology, strict_mode=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>leaves</code> <p>Dictionary detailing the leaves of the hierarchy tree representing the structure of the graph; the leaves are the entities that will be direct components of the graph, while the intermediary nodes are additional labels for filtering purposes.</p> required <code>strict_mode</code> <code>bool</code> <p>If True, the translator will raise an error if input data do not carry source, licence, and version information.</p> <code>False</code> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def __init__(self, ontology: \"Ontology\", strict_mode: bool = False):\n    \"\"\"\n    Args:\n        leaves:\n            Dictionary detailing the leaves of the hierarchy\n            tree representing the structure of the graph; the leaves are\n            the entities that will be direct components of the graph,\n            while the intermediary nodes are additional labels for\n            filtering purposes.\n        strict_mode:\n            If True, the translator will raise an error if input data do not\n            carry source, licence, and version information.\n    \"\"\"\n\n    self.ontology = ontology\n    self.strict_mode = strict_mode\n\n    # record nodes without biolink type configured in schema_config.yaml\n    self.notype = {}\n\n    # mapping functionality for translating terms and queries\n    self.mappings = {}\n    self.reverse_mappings = {}\n\n    self._update_ontology_types()\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator._add_translation_mappings","title":"<code>_add_translation_mappings(original_name, biocypher_name)</code>","text":"<p>Add translation mappings for a label and name. We use here the PascalCase version of the BioCypher name, since sentence case is not useful for Cypher queries.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def _add_translation_mappings(self, original_name, biocypher_name):\n    \"\"\"\n    Add translation mappings for a label and name. We use here the\n    PascalCase version of the BioCypher name, since sentence case is\n    not useful for Cypher queries.\n    \"\"\"\n    if isinstance(original_name, list):\n        for on in original_name:\n            self.mappings[on] = self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n    else:\n        self.mappings[original_name] = self.name_sentence_to_pascal(\n            biocypher_name,\n        )\n\n    if isinstance(biocypher_name, list):\n        for bn in biocypher_name:\n            self.reverse_mappings[\n                self.name_sentence_to_pascal(\n                    bn,\n                )\n            ] = original_name\n    else:\n        self.reverse_mappings[\n            self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n        ] = original_name\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator._filter_props","title":"<code>_filter_props(bl_type, props)</code>","text":"<p>Filters properties for those specified in schema_config if any.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def _filter_props(self, bl_type: str, props: dict) -&gt; dict:\n    \"\"\"\n    Filters properties for those specified in schema_config if any.\n    \"\"\"\n\n    filter_props = self.ontology.mapping.extended_schema[bl_type].get(\"properties\", {})\n\n    # strict mode: add required properties (only if there is a whitelist)\n    if self.strict_mode and filter_props:\n        filter_props.update(\n            {\"source\": \"str\", \"licence\": \"str\", \"version\": \"str\"},\n        )\n\n    exclude_props = self.ontology.mapping.extended_schema[bl_type].get(\"exclude_properties\", [])\n\n    if isinstance(exclude_props, str):\n        exclude_props = [exclude_props]\n\n    if filter_props and exclude_props:\n        filtered_props = {k: v for k, v in props.items() if (k in filter_props.keys() and k not in exclude_props)}\n\n    elif filter_props:\n        filtered_props = {k: v for k, v in props.items() if k in filter_props.keys()}\n\n    elif exclude_props:\n        filtered_props = {k: v for k, v in props.items() if k not in exclude_props}\n\n    else:\n        return props\n\n    missing_props = [k for k in filter_props.keys() if k not in filtered_props.keys()]\n    # add missing properties with default values\n    for k in missing_props:\n        filtered_props[k] = None\n\n    return filtered_props\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator._get_ontology_mapping","title":"<code>_get_ontology_mapping(label)</code>","text":"<p>For each given input type (\"input_label\" or \"label_in_input\"), find the corresponding ontology class in the leaves dictionary (from the <code>schema_config.yam</code>).</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The input type to find (<code>input_label</code> or <code>label_in_input</code> in <code>schema_config.yaml</code>).</p> required Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def _get_ontology_mapping(self, label: str) -&gt; Optional[str]:\n    \"\"\"\n    For each given input type (\"input_label\" or \"label_in_input\"), find the\n    corresponding ontology class in the leaves dictionary (from the\n    `schema_config.yam`).\n\n    Args:\n        label:\n            The input type to find (`input_label` or `label_in_input` in\n            `schema_config.yaml`).\n    \"\"\"\n\n    # commented out until behaviour of _update_bl_types is fixed\n    return self._ontology_mapping.get(label, None)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator._get_preferred_id","title":"<code>_get_preferred_id(_bl_type)</code>","text":"<p>Returns the preferred id for the given Biolink type.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def _get_preferred_id(self, _bl_type: str) -&gt; str:\n    \"\"\"\n    Returns the preferred id for the given Biolink type.\n    \"\"\"\n\n    return (\n        self.ontology.mapping.extended_schema[_bl_type][\"preferred_id\"]\n        if \"preferred_id\" in self.ontology.mapping.extended_schema.get(_bl_type, {})\n        else \"id\"\n    )\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator._record_no_type","title":"<code>_record_no_type(_type, what)</code>","text":"<p>Records the type of a node or edge that is not represented in the schema_config.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def _record_no_type(self, _type: Any, what: Any) -&gt; None:\n    \"\"\"\n    Records the type of a node or edge that is not represented in the\n    schema_config.\n    \"\"\"\n\n    logger.debug(f\"No ontology type defined for `{_type}`: {what}\")\n\n    if self.notype.get(_type, None):\n        self.notype[_type] += 1\n\n    else:\n        self.notype[_type] = 1\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator._update_ontology_types","title":"<code>_update_ontology_types()</code>","text":"<p>Creates a dictionary to translate from input labels to ontology labels.</p> <p>If multiple input labels, creates mapping for each.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def _update_ontology_types(self):\n    \"\"\"\n    Creates a dictionary to translate from input labels to ontology labels.\n\n    If multiple input labels, creates mapping for each.\n    \"\"\"\n\n    self._ontology_mapping = {}\n\n    for key, value in self.ontology.mapping.extended_schema.items():\n        labels = value.get(\"input_label\") or value.get(\"label_in_input\")\n\n        if isinstance(labels, str):\n            self._ontology_mapping[labels] = key\n\n        elif isinstance(labels, list):\n            for label in labels:\n                self._ontology_mapping[label] = key\n\n        if value.get(\"label_as_edge\"):\n            self._add_translation_mappings(labels, value[\"label_as_edge\"])\n\n        else:\n            self._add_translation_mappings(labels, key)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.get_missing_biolink_types","title":"<code>get_missing_biolink_types()</code>","text":"<p>Returns a dictionary of types that were not represented in the schema_config.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def get_missing_biolink_types(self) -&gt; dict:\n    \"\"\"\n    Returns a dictionary of types that were not represented in the\n    schema_config.\n    \"\"\"\n\n    return self.notype\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.name_sentence_to_pascal","title":"<code>name_sentence_to_pascal(name)</code>  <code>staticmethod</code>","text":"<p>Converts a name in sentence case to pascal case.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>@staticmethod\ndef name_sentence_to_pascal(name: str) -&gt; str:\n    \"\"\"\n    Converts a name in sentence case to pascal case.\n    \"\"\"\n    # split on dots if dot is present\n    if \".\" in name:\n        return \".\".join(\n            [_misc.sentencecase_to_pascalcase(n) for n in name.split(\".\")],\n        )\n    else:\n        return _misc.sentencecase_to_pascalcase(name)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.reverse_translate","title":"<code>reverse_translate(query)</code>","text":"<p>Reverse translate a cypher query. Only translates labels as of now.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def reverse_translate(self, query):\n    \"\"\"\n    Reverse translate a cypher query. Only translates labels as of\n    now.\n    \"\"\"\n    for key in self.reverse_mappings:\n        a = \":\" + key + \")\"\n        b = \":\" + key + \"]\"\n        # TODO this conditional probably does not cover all cases\n        if a in query or b in query:\n            if isinstance(self.reverse_mappings[key], list):\n                raise NotImplementedError(\n                    \"Reverse translation of multiple inputs not \"\n                    \"implemented yet. Many-to-one mappings are \"\n                    \"not reversible. \"\n                    f\"({key} -&gt; {self.reverse_mappings[key]})\",\n                )\n            else:\n                query = query.replace(\n                    a,\n                    \":\" + self.reverse_mappings[key] + \")\",\n                ).replace(b, \":\" + self.reverse_mappings[key] + \"]\")\n    return query\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.reverse_translate_term","title":"<code>reverse_translate_term(term)</code>","text":"<p>Reverse translate a single term.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def reverse_translate_term(self, term):\n    \"\"\"\n    Reverse translate a single term.\n    \"\"\"\n\n    return self.reverse_mappings.get(term, None)\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.translate","title":"<code>translate(query)</code>","text":"<p>Translate a cypher query. Only translates labels as of now.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def translate(self, query):\n    \"\"\"\n    Translate a cypher query. Only translates labels as of now.\n    \"\"\"\n    for key in self.mappings:\n        query = query.replace(\":\" + key, \":\" + self.mappings[key])\n    return query\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.translate_edges","title":"<code>translate_edges(edge_tuples)</code>","text":"<p>Translates input edge representation to a representation that conforms to the schema of the given BioCypher graph. For now requires explicit statement of edge type on pass.</p> <p>Args:</p> <pre><code>edge_tuples (list of tuples):\n\n    collection of tuples representing source and target of\n    an interaction via their unique ids as well as the type\n    of interaction in the original database notation, which\n    is translated to BioCypher notation using the `leaves`.\n    Can optionally possess its own ID.\n</code></pre> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def translate_edges(\n    self,\n    edge_tuples: Iterable,\n) -&gt; Generator[Union[BioCypherEdge, BioCypherRelAsNode], None, None]:\n    \"\"\"\n    Translates input edge representation to a representation that\n    conforms to the schema of the given BioCypher graph. For now\n    requires explicit statement of edge type on pass.\n\n    Args:\n\n        edge_tuples (list of tuples):\n\n            collection of tuples representing source and target of\n            an interaction via their unique ids as well as the type\n            of interaction in the original database notation, which\n            is translated to BioCypher notation using the `leaves`.\n            Can optionally possess its own ID.\n    \"\"\"\n\n    self._log_begin_translate(edge_tuples, \"edges\")\n\n    # legacy: deal with 4-tuples (no edge id)\n    # TODO remove for performance reasons once safe\n    edge_tuples = peekable(edge_tuples)\n    if len(edge_tuples.peek()) == 4:\n        edge_tuples = [(None, src, tar, typ, props) for src, tar, typ, props in edge_tuples]\n\n    for _id, _src, _tar, _type, _props in edge_tuples:\n        # check for strict mode requirements\n        if self.strict_mode:\n            if \"source\" not in _props:\n                raise ValueError(\n                    f\"Edge {_id if _id else (_src, _tar)} does not have a `source` property.\",\n                    \" This is required in strict mode.\",\n                )\n            if \"licence\" not in _props:\n                raise ValueError(\n                    f\"Edge {_id if _id else (_src, _tar)} does not have a `licence` property.\",\n                    \" This is required in strict mode.\",\n                )\n\n        # match the input label (_type) to\n        # a Biolink label from schema_config\n        bl_type = self._get_ontology_mapping(_type)\n\n        if bl_type:\n            # filter properties for those specified in schema_config if any\n            _filtered_props = self._filter_props(bl_type, _props)\n\n            rep = self.ontology.mapping.extended_schema[bl_type][\"represented_as\"]\n\n            if rep == \"node\":\n                if _id:\n                    # if it brings its own ID, use it\n                    node_id = _id\n\n                else:\n                    # source target concat\n                    node_id = str(_src) + \"_\" + str(_tar) + \"_\" + \"_\".join(str(v) for v in _filtered_props.values())\n\n                n = BioCypherNode(\n                    node_id=node_id,\n                    node_label=bl_type,\n                    properties=_filtered_props,\n                )\n\n                # directionality check TODO generalise to account for\n                # different descriptions of directionality or find a\n                # more consistent solution for indicating directionality\n                if _filtered_props.get(\"directed\") == True:  # noqa: E712 (seems to not work without '== True')\n                    l1 = \"IS_SOURCE_OF\"\n                    l2 = \"IS_TARGET_OF\"\n\n                elif _filtered_props.get(\n                    \"src_role\",\n                ) and _filtered_props.get(\"tar_role\"):\n                    l1 = _filtered_props.get(\"src_role\")\n                    l2 = _filtered_props.get(\"tar_role\")\n\n                else:\n                    l1 = l2 = \"IS_PART_OF\"\n\n                e_s = BioCypherEdge(\n                    source_id=_src,\n                    target_id=node_id,\n                    relationship_label=l1,\n                    # additional here\n                )\n\n                e_t = BioCypherEdge(\n                    source_id=_tar,\n                    target_id=node_id,\n                    relationship_label=l2,\n                    # additional here\n                )\n\n                yield BioCypherRelAsNode(n, e_s, e_t)\n\n            else:\n                edge_label = self.ontology.mapping.extended_schema[bl_type].get(\"label_as_edge\")\n\n                if edge_label is None:\n                    edge_label = bl_type\n\n                yield BioCypherEdge(\n                    relationship_id=_id,\n                    source_id=_src,\n                    target_id=_tar,\n                    relationship_label=edge_label,\n                    properties=_filtered_props,\n                )\n\n        else:\n            self._record_no_type(_type, (_src, _tar))\n\n    self._log_finish_translate(\"edges\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.translate_nodes","title":"<code>translate_nodes(node_tuples)</code>","text":"<p>Translates input node representation to a representation that conforms to the schema of the given BioCypher graph. For now requires explicit statement of node type on pass.</p> <p>Parameters:</p> Name Type Description Default <code>node_tuples</code> <code>list of tuples</code> <p>collection of tuples representing individual nodes by their unique id and a type that is translated from the original database notation to the corresponding BioCypher notation.</p> required Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def translate_nodes(\n    self,\n    node_tuples: Iterable,\n) -&gt; Generator[BioCypherNode, None, None]:\n    \"\"\"\n    Translates input node representation to a representation that\n    conforms to the schema of the given BioCypher graph. For now\n    requires explicit statement of node type on pass.\n\n    Args:\n        node_tuples (list of tuples): collection of tuples\n            representing individual nodes by their unique id and a type\n            that is translated from the original database notation to\n            the corresponding BioCypher notation.\n\n    \"\"\"\n\n    self._log_begin_translate(node_tuples, \"nodes\")\n\n    for _id, _type, _props in node_tuples:\n        # check for strict mode requirements\n        required_props = [\"source\", \"licence\", \"version\"]\n\n        if self.strict_mode:\n            # rename 'license' to 'licence' in _props\n            if _props.get(\"license\"):\n                _props[\"licence\"] = _props.pop(\"license\")\n\n            for prop in required_props:\n                if prop not in _props:\n                    raise ValueError(\n                        f\"Property `{prop}` missing from node {_id}. \"\n                        \"Strict mode is enabled, so this is not allowed.\"\n                    )\n\n        # find the node in leaves that represents ontology node type\n        _ontology_class = self._get_ontology_mapping(_type)\n\n        if _ontology_class:\n            # filter properties for those specified in schema_config if any\n            _filtered_props = self._filter_props(_ontology_class, _props)\n\n            # preferred id\n            _preferred_id = self._get_preferred_id(_ontology_class)\n\n            yield BioCypherNode(\n                node_id=_id,\n                node_label=_ontology_class,\n                preferred_id=_preferred_id,\n                properties=_filtered_props,\n            )\n\n        else:\n            self._record_no_type(_type, _id)\n\n    self._log_finish_translate(\"nodes\")\n</code></pre>"},{"location":"BioCypher/reference/source/#biocypher._translate.Translator.translate_term","title":"<code>translate_term(term)</code>","text":"<p>Translate a single term.</p> Source code in <code>biocypher/biocypher/_translate.py</code> <pre><code>def translate_term(self, term):\n    \"\"\"\n    Translate a single term.\n    \"\"\"\n\n    return self.mappings.get(term, None)\n</code></pre>"},{"location":"BioCypher/reference/source/#classes","title":"Classes","text":""},{"location":"BioCypher/reference/source/#functions","title":"Functions","text":""},{"location":"BioCypher/reference/source/#other-members","title":"Other Members","text":""}]}